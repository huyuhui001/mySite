<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href=https://huyuhui001.github.io/mySite/linux/SES/linux_ses_demo/ rel=canonical><link rel=icon href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.3.0, mkdocs-material-7.3.6"><title>SUSE Enterprise Storage Basic Operation - UPSkilling</title><link rel=stylesheet href=../../../assets/stylesheets/main.a57b2b03.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.3f5d1f46.min.css><meta name=theme-color content=#009485><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback"><style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style><link rel=stylesheet href=../../../extra.css></head> <body dir=ltr data-md-color-scheme data-md-color-primary=teal data-md-color-accent=green> <script>function __prefix(e){return new URL("../../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#suse-enterprise-storage-6-installation-and-basic-operation class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title=UPSkilling class="md-header__button md-logo" aria-label=UPSkilling data-md-component=logo> <img src=../../../assets/logo.jpg alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> UPSkilling </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> SUSE Enterprise Storage Basic Operation </span> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/huyuhui001/mySite title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> huyuhui001/mySite </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> UPSkilling </a> </li> <li class=md-tabs__item> <a href=../../ class="md-tabs__link md-tabs__link--active"> Linux </a> </li> <li class=md-tabs__item> <a href=../../../k8s/ class=md-tabs__link> Kubernetes </a> </li> <li class=md-tabs__item> <a href=../../../python/ class=md-tabs__link> Python </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title=UPSkilling class="md-nav__button md-logo" aria-label=UPSkilling data-md-component=logo> <img src=../../../assets/logo.jpg alt=logo> </a> UPSkilling </label> <div class=md-nav__source> <a href=https://github.com/huyuhui001/mySite title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> huyuhui001/mySite </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_1 type=checkbox id=__nav_1> <label class=md-nav__link for=__nav_1> UPSkilling <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=UPSkilling data-md-level=1> <label class=md-nav__title for=__nav_1> <span class="md-nav__icon md-icon"></span> UPSkilling </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> Welcome </a> </li> <li class=md-nav__item> <a href=../../../about/ class=md-nav__link> About </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2 type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2> Linux <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Linux data-md-level=1> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Linux </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../ class=md-nav__link> Index </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_2 type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2> Linux SRE <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Linux SRE" data-md-level=2> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> Linux SRE </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../SRE/01-fundamentals/ class=md-nav__link> Linux Fundamentals </a> </li> <li class=md-nav__item> <a href=../../SRE/02-filesystem/ class=md-nav__link> File System </a> </li> <li class=md-nav__item> <a href=../../SRE/03-identity-security/ class=md-nav__link> Identity & Security </a> </li> <li class=md-nav__item> <a href=../../SRE/04-RegExpress/ class=md-nav__link> VIM & Regular Expression </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_3 type=checkbox id=__nav_2_3> <label class=md-nav__link for=__nav_2_3> SUSE Linux Administration <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="SUSE Linux Administration" data-md-level=2> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> SUSE Linux Administration </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Administration/01/ class=md-nav__link> Linux File System Overview </a> </li> <li class=md-nav__item> <a href=../../Administration/02/ class=md-nav__link> Useful Commands </a> </li> <li class=md-nav__item> <a href=../../Administration/03/ class=md-nav__link> Shell </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_2_4 type=checkbox id=__nav_2_4 checked> <label class=md-nav__link for=__nav_2_4> SUSE Enterprise Storage Foundation <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="SUSE Enterprise Storage Foundation" data-md-level=2> <label class=md-nav__title for=__nav_2_4> <span class="md-nav__icon md-icon"></span> SUSE Enterprise Storage Foundation </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../linux_ses_memo/ class=md-nav__link> SUSE Enterprise Storage Foundation </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> SUSE Enterprise Storage Basic Operation <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> SUSE Enterprise Storage Basic Operation </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#1-installation class=md-nav__link> 1. Installation </a> <nav class=md-nav aria-label="1. Installation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#11-environment-setup class=md-nav__link> 1.1. Environment Setup </a> </li> <li class=md-nav__item> <a href=#12-install-packages class=md-nav__link> 1.2. Install Packages </a> </li> <li class=md-nav__item> <a href=#13-stage-0-the-preparation class=md-nav__link> 1.3. Stage 0 — the preparation </a> </li> <li class=md-nav__item> <a href=#14-stage-2-the-configuration class=md-nav__link> 1.4. Stage 2 — the configuration </a> </li> <li class=md-nav__item> <a href=#15-stage-3-the-deployment class=md-nav__link> 1.5. Stage 3 — the deployment </a> </li> <li class=md-nav__item> <a href=#16-stage-4-the-services class=md-nav__link> 1.6. Stage 4 — the services </a> </li> <li class=md-nav__item> <a href=#17-stage-5-the-removal-stage class=md-nav__link> 1.7. Stage 5 — the removal stage </a> </li> <li class=md-nav__item> <a href=#18-installation-guide class=md-nav__link> 1.8. Installation Guide </a> </li> <li class=md-nav__item> <a href=#19-issues-during-installation class=md-nav__link> 1.9. Issues during installation </a> </li> <li class=md-nav__item> <a href=#110-shutting-down-the-whole-ceph-cluster class=md-nav__link> 1.10. Shutting Down the Whole Ceph Cluster </a> </li> <li class=md-nav__item> <a href=#111-starting-stopping-and-restarting-services-using-targets class=md-nav__link> 1.11. Starting, Stopping, and Restarting Services Using Targets </a> </li> <li class=md-nav__item> <a href=#112-restarting-all-services class=md-nav__link> 1.12. Restarting All Services </a> </li> <li class=md-nav__item> <a href=#113-restarting-specific-services class=md-nav__link> 1.13. Restarting Specific Services </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#2-basic-operation class=md-nav__link> 2. Basic Operation </a> <nav class=md-nav aria-label="2. Basic Operation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#21-pools-and-data-placement class=md-nav__link> 2.1. Pools and Data Placement </a> <nav class=md-nav aria-label="2.1. Pools and Data Placement"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#211-enable-the-pg-autoscaler-and-balancer-modules class=md-nav__link> 2.1.1. Enable the PG Autoscaler and Balancer Modules </a> <nav class=md-nav aria-label="2.1.1. Enable the PG Autoscaler and Balancer Modules"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-view-the-state-of-all-the-manager-modules class=md-nav__link> Task 1: View the state of all the Manager Modules </a> </li> <li class=md-nav__item> <a href=#task-2-list-the-existing-pools class=md-nav__link> Task 2: List the Existing Pools </a> </li> <li class=md-nav__item> <a href=#task-3-enable-the-pg_autoscaler-module class=md-nav__link> Task 3: Enable the pg_autoscaler module </a> </li> <li class=md-nav__item> <a href=#task-4-turn-on-the-placement-group-balancer-feature class=md-nav__link> Task 4: Turn on the Placement Group balancer feature </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#212-manipulate-erasure-code-profiles class=md-nav__link> 2.1.2. Manipulate Erasure Code Profiles </a> <nav class=md-nav aria-label="2.1.2. Manipulate Erasure Code Profiles"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-display-a-list-of-the-current-erasure-code-profiles class=md-nav__link> Task 1: Display a list of the current Erasure Code profiles </a> </li> <li class=md-nav__item> <a href=#task-2-examine-the-details-of-the-default-ec-profile class=md-nav__link> Task 2: Examine the details of the default EC profile </a> </li> <li class=md-nav__item> <a href=#task-3-create-and-remove-a-new-ec-profile class=md-nav__link> Task 3: Create and remove a new EC profile </a> </li> <li class=md-nav__item> <a href=#task-4-create-a-better-ec-profile class=md-nav__link> Task 4: Create a better EC profile </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#213-manipulate-crush-map-rulesets class=md-nav__link> 2.1.3. Manipulate CRUSH Map Rulesets </a> <nav class=md-nav aria-label="2.1.3. Manipulate CRUSH Map Rulesets"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-display-a-list-of-the-current-crush-map-rules class=md-nav__link> Task 1: Display a list of the current CRUSH Map rules </a> </li> <li class=md-nav__item> <a href=#task-2-examine-the-details-of-the-default-crush-map-rule class=md-nav__link> Task 2: Examine the details of the default CRUSH Map rule </a> </li> <li class=md-nav__item> <a href=#task-3-create-and-remove-a-new-crush-map-rule class=md-nav__link> Task 3: Create and remove a new CRUSH Map rule </a> </li> <li class=md-nav__item> <a href=#task-4-create-a-better-crush-map-rule class=md-nav__link> Task 4: Create a better CRUSH Map rule </a> </li> <li class=md-nav__item> <a href=#task-5-create-crush-map-rules-for-different-classes-of-devices class=md-nav__link> Task 5: Create CRUSH Map rules for different classes of devices </a> </li> <li class=md-nav__item> <a href=#task-6-change-the-ruleset-used-by-a-pool class=md-nav__link> Task 6: Change the ruleset used by a pool </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#214-investigate-bluestore class=md-nav__link> 2.1.4. Investigate BlueStore </a> <nav class=md-nav aria-label="2.1.4. Investigate BlueStore"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-explore-the-drive_groupsyml-configuration class=md-nav__link> Task 1: Explore the drive_groups.yml configuration </a> </li> <li class=md-nav__item> <a href=#task-2-examine-a-storage-hosts-storage-devices class=md-nav__link> Task 2: Examine a storage host’s storage devices </a> </li> <li class=md-nav__item> <a href=#task-3-examine-a-storage-hosts-osd-details class=md-nav__link> Task 3: Examine a storage host’s OSD details </a> </li> <li class=md-nav__item> <a href=#task-4-display-bluestore-information-using-ceph-bluestore-tool class=md-nav__link> Task 4: Display BlueStore information using ceph-bluestore-tool </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#22-common-day-1-tasks-using-the-cli class=md-nav__link> 2.2. Common Day 1 Tasks Using the CLI </a> <nav class=md-nav aria-label="2.2. Common Day 1 Tasks Using the CLI"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#221-ceph-users-and-configuration class=md-nav__link> 2.2.1. Ceph Users and Configuration </a> <nav class=md-nav aria-label="2.2.1. Ceph Users and Configuration"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-view-the-current-user-keyrings class=md-nav__link> Task 1: View the current user keyrings </a> </li> <li class=md-nav__item> <a href=#task-2-create-a-new-keyring-and-associated-user class=md-nav__link> Task 2: Create a new keyring and associated user </a> </li> <li class=md-nav__item> <a href=#task-3-create-a-client-key-for-rbd class=md-nav__link> Task 3: Create a client key for RBD </a> </li> <li class=md-nav__item> <a href=#task-4-view-the-ceph-master-configuration-file class=md-nav__link> Task 4: View the Ceph master configuration file </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#222-run-the-ceph-health-commands class=md-nav__link> 2.2.2. Run the Ceph Health Commands </a> </li> <li class=md-nav__item> <a href=#223-manipulate-pools class=md-nav__link> 2.2.3. Manipulate Pools </a> <nav class=md-nav aria-label="2.2.3. Manipulate Pools"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-display-a-list-of-the-current-pools class=md-nav__link> Task 1: Display a list of the current pools </a> </li> <li class=md-nav__item> <a href=#task-2-display-the-usage-data-and-stats-of-the-current-pools class=md-nav__link> Task 2: Display the usage data and stats of the current pools </a> </li> <li class=md-nav__item> <a href=#task-3-create-two-new-pools-one-replicated-one-ec class=md-nav__link> Task 3: Create two new pools, one replicated, one EC </a> </li> <li class=md-nav__item> <a href=#task-4-assign-an-application-to-the-two-new-pools class=md-nav__link> Task 4: Assign an application to the two new pools </a> </li> <li class=md-nav__item> <a href=#task-5-manage-snapshots-of-the-new-rgw-bucket-pool class=md-nav__link> Task 5: Manage snapshots of the new RGW bucket pool </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#224-maintain-consistency-of-data-with-scrub-and-repair class=md-nav__link> 2.2.4. Maintain consistency of data with Scrub and Repair </a> <nav class=md-nav aria-label="2.2.4. Maintain consistency of data with Scrub and Repair"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-display-a-few-of-the-scrub-settings class=md-nav__link> Task 1: Display a few of the Scrub settings </a> </li> <li class=md-nav__item> <a href=#task-2-change-the-scrub-settings-in-cephconf class=md-nav__link> Task 2: Change the Scrub settings in ceph.conf </a> </li> <li class=md-nav__item> <a href=#task-3-change-the-scrub-settings-directly-in-the-configuration-db class=md-nav__link> Task 3: Change the Scrub settings directly in the Configuration DB </a> </li> <li class=md-nav__item> <a href=#task-4-manually-scrub-and-repair-an-osd-and-a-pg class=md-nav__link> Task 4: Manually scrub and repair an OSD and a PG </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#225-manipulate-manager-modules class=md-nav__link> 2.2.5. Manipulate Manager Modules </a> <nav class=md-nav aria-label="2.2.5. Manipulate Manager Modules"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-display-the-list-of-enabled-manager-modules class=md-nav__link> Task 1: Display the list of enabled Manager Modules </a> </li> <li class=md-nav__item> <a href=#task-4-briefly-attempt-to-use-the-crash-manager-module class=md-nav__link> Task 4: Briefly attempt to use the crash manager module </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#226-introduction-to-the-tell-command class=md-nav__link> 2.2.6. Introduction to the Tell command </a> <nav class=md-nav aria-label="2.2.6. Introduction to the Tell command"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-run-a-benchmark-test-on-an-osd class=md-nav__link> Task 1: Run a benchmark test on an OSD </a> </li> <li class=md-nav__item> <a href=#task-2-change-the-protection-setting-regarding-the-deletion-of-pools class=md-nav__link> Task 2: Change the protection setting regarding the deletion of pools </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#23-ceph-dashboard class=md-nav__link> 2.3. Ceph Dashboard </a> <nav class=md-nav aria-label="2.3. Ceph Dashboard"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#231-access-dashboard class=md-nav__link> 2.3.1. Access Dashboard </a> <nav class=md-nav aria-label="2.3.1. Access Dashboard"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-set-the-password-for-the-admin-user-of-the-ceph-dashboard class=md-nav__link> Task 1: Set the password for the admin user of the Ceph Dashboard </a> </li> <li class=md-nav__item> <a href=#task-3-visit-the-ceph-dashboard-url class=md-nav__link> Task 3: Visit the Ceph Dashboard URL </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#232-explore-the-dashboard-health-performance-status class=md-nav__link> 2.3.2. Explore the Dashboard Health, Performance, Status </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#24-storage-data-access class=md-nav__link> 2.4. Storage Data Access </a> <nav class=md-nav aria-label="2.4. Storage Data Access"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#241-ensure-the-ses-cluster-is-healthy class=md-nav__link> 2.4.1. Ensure the SES Cluster is Healthy </a> <nav class=md-nav aria-label="2.4.1. Ensure the SES Cluster is Healthy"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-check-the-clusters-health class=md-nav__link> Task 1: Check the Cluster’s health </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#242-use-the-s3-api-to-interact-with-the-rados-gateway class=md-nav__link> 2.4.2. Use the S3 API to Interact with the RADOS Gateway </a> <nav class=md-nav aria-label="2.4.2. Use the S3 API to Interact with the RADOS Gateway"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-using-the-s3cmd-tool-and-create-an-s3-user class=md-nav__link> Task 1: Using the s3cmd tool and create an S3 user </a> </li> <li class=md-nav__item> <a href=#task-2-create-a-new-s3cmd-configuration-file-and-a-new-s3-bucket class=md-nav__link> Task 2: Create a new s3cmd configuration file and a new S3 bucket </a> </li> <li class=md-nav__item> <a href=#task3-create-and-upload-a-file-to-a-bucket-using-the-s3-api class=md-nav__link> Task3: Create and upload a file to a bucket using the S3 API </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#243-use-the-swift-api-to-interact-with-the-rados-gateway class=md-nav__link> 2.4.3. Use the swift API to Interact with the RADOS Gateway </a> <nav class=md-nav aria-label="2.4.3. Use the swift API to Interact with the RADOS Gateway"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-create-a-swift-subuser class=md-nav__link> Task 1: Create a swift subuser </a> </li> <li class=md-nav__item> <a href=#task-2-use-the-swift-command-to-access-a-file-created-with-the-s3cmd-tool class=md-nav__link> Task 2: Use the swift command to access a file created with the S3cmd tool </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#244-create-snapshots-on-ses-using-rbd class=md-nav__link> 2.4.4. Create Snapshots on SES using RBD </a> <nav class=md-nav aria-label="2.4.4. Create Snapshots on SES using RBD"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-create-a-new-pool-for-rbd-images class=md-nav__link> Task 1: Create a new pool for RBD images </a> </li> <li class=md-nav__item> <a href=#task-2-create-a-new-rbd-image-in-the-rbd-images-pool class=md-nav__link> Task 2: Create a new RBD image in the rbd-images pool </a> </li> <li class=md-nav__item> <a href=#task-3-mount-the-new-image-on-the-admin-node-and-create-a-filesystem class=md-nav__link> Task 3: Mount the new image on the admin node and create a filesystem </a> </li> <li class=md-nav__item> <a href=#task-4-create-a-file-on-the-new-filesystem-and-snapshot-the-rbd-image-and-make-some-additional-changes class=md-nav__link> Task 4: Create a file on the new filesystem and snapshot the rbd image and make some additional changes </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#245-create-and-manage-cow-clones-with-rbd class=md-nav__link> 2.4.5. Create and manage COW Clones with rbd </a> <nav class=md-nav aria-label="2.4.5. Create and manage COW Clones with rbd"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-create-a-new-pool class=md-nav__link> Task 1: Create a new pool </a> </li> <li class=md-nav__item> <a href=#task-4-snapshot-the-rbd-image-and-protect-the-snapshot class=md-nav__link> Task 4: Snapshot the rbd image and protect the snapshot </a> </li> <li class=md-nav__item> <a href=#task-7-flatten-a-cow-clone-and-remove-the-parent-image class=md-nav__link> Task 7: Flatten a COW Clone and remove the parent image </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#246-configure-iscsi-on-ses class=md-nav__link> 2.4.6. Configure iSCSI on SES </a> <nav class=md-nav aria-label="2.4.6. Configure iSCSI on SES"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-create-a-new-rbd-image-in-the-iscsi-images-pool class=md-nav__link> Task 1: Create a new RBD image in the iscsi-images pool </a> </li> <li class=md-nav__item> <a href=#task-2-define-a-new-iscsi-target-with-the-ceph-dashboard class=md-nav__link> Task 2: Define a new iSCSI target with the Ceph Dashboard </a> </li> <li class=md-nav__item> <a href=#task-3-access-the-new-iscsi-target-from-the-admin-node class=md-nav__link> Task 3: Access the new iSCSI target from the admin node </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#247-mount-cephfs-provided-by-suse-enterprise-storage class=md-nav__link> 2.4.7. Mount CephFS Provided by SUSE Enterprise Storage </a> <nav class=md-nav aria-label="2.4.7. Mount CephFS Provided by SUSE Enterprise Storage"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-verify-cephfs-configuration-of-the-ses-cluster class=md-nav__link> Task 1: Verify cephfs configuration of the SES cluster </a> </li> <li class=md-nav__item> <a href=#task-3-mount-the-ceph-filesystem-on-the-admin-node class=md-nav__link> Task 3: Mount the ceph filesystem on the admin node </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#248-export-an-nfs-share-from-ses-with-nfs-ganesha class=md-nav__link> 2.4.8. Export an NFS Share from SES with NFS Ganesha </a> <nav class=md-nav aria-label="2.4.8. Export an NFS Share from SES with NFS Ganesha"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-0-install-and-configure-ganesha-ganesha-config-location-is-not-configured-please-set-the-ganesha_rados_pool_namespace-setting class=md-nav__link> Task 0: Install and configure Ganesha (Ganesha config location is not configured. Please set the GANESHA_RADOS_POOL_NAMESPACE setting.) </a> </li> <li class=md-nav__item> <a href=#task-1-create-an-nfs-export-using-the-ceph-dashboard class=md-nav__link> Task 1: Create an NFS export using the Ceph Dashboard </a> </li> <li class=md-nav__item> <a href=#task-2-mount-the-nfs-export-on-the-admin-node class=md-nav__link> Task 2: Mount the NFS export on the admin node </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#249-configure-and-mount-cifs class=md-nav__link> 2.4.9. Configure and Mount CIFS </a> <nav class=md-nav aria-label="2.4.9. Configure and Mount CIFS"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-prepare-the-cephfs-share-for-cifs class=md-nav__link> Task 1: Prepare the CephFS share for CIFS </a> </li> <li class=md-nav__item> <a href=#task-2-create-a-samba-gateway-specific-keyring-on-the-ceph-admin-node-and-copy-it-to-the-samba-gateway-node class=md-nav__link> Task 2: Create a Samba gateway specific keyring on the Ceph admin node and copy it to the Samba gateway node </a> </li> <li class=md-nav__item> <a href=#task-3-configure-samba-on-the-samba-gateway-node class=md-nav__link> Task 3: Configure Samba on the Samba gateway node </a> </li> <li class=md-nav__item> <a href=#task-4-connect-a-client-to-the-samba-gateway class=md-nav__link> Task 4: Connect a client to the Samba gateway </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3 type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3> Kubernetes <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Kubernetes data-md-level=1> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Kubernetes </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../k8s/ class=md-nav__link> Index </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_2 type=checkbox id=__nav_3_2> <label class=md-nav__link for=__nav_3_2> Installation <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Installation data-md-level=2> <label class=md-nav__title for=__nav_3_2> <span class="md-nav__icon md-icon"></span> Installation </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../k8s/installation/single-local/ class=md-nav__link> Single Node Installation </a> </li> <li class=md-nav__item> <a href=../../../k8s/installation/multiple-local/ class=md-nav__link> Multiple Nodes Installation </a> </li> <li class=md-nav__item> <a href=../../../k8s/installation/aliyun-ubuntu/ class=md-nav__link> Installation on Aliyun ECS </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_3 type=checkbox id=__nav_3_3> <label class=md-nav__link for=__nav_3_3> Docker <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Docker data-md-level=2> <label class=md-nav__title for=__nav_3_3> <span class="md-nav__icon md-icon"></span> Docker </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../k8s/foundamentals/docker/ class=md-nav__link> Fundamentals </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_4 type=checkbox id=__nav_3_4> <label class=md-nav__link for=__nav_3_4> Foundamentals <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Foundamentals data-md-level=2> <label class=md-nav__title for=__nav_3_4> <span class="md-nav__icon md-icon"></span> Foundamentals </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../k8s/foundamentals/memo/ class=md-nav__link> Memo </a> </li> <li class=md-nav__item> <a href=../../../k8s/foundamentals/overview/ class=md-nav__link> Overview </a> </li> <li class=md-nav__item> <a href=../../../k8s/foundamentals/basics/ class=md-nav__link> kubectl basics </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_5 type=checkbox id=__nav_3_5> <label class=md-nav__link for=__nav_3_5> Core Kubernetes <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Core Kubernetes" data-md-level=2> <label class=md-nav__title for=__nav_3_5> <span class="md-nav__icon md-icon"></span> Core Kubernetes </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../k8s/foundamentals/pod/ class=md-nav__link> Pod </a> </li> <li class=md-nav__item> <a href=../../../k8s/foundamentals/deployment/ class=md-nav__link> Deployment </a> </li> <li class=md-nav__item> <a href=../../../k8s/foundamentals/service/ class=md-nav__link> Service </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_6 type=checkbox id=__nav_3_6> <label class=md-nav__link for=__nav_3_6> Application Modeling <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Application Modeling" data-md-level=2> <label class=md-nav__title for=__nav_3_6> <span class="md-nav__icon md-icon"></span> Application Modeling </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../k8s/foundamentals/namespace/ class=md-nav__link> Namespace </a> </li> <li class=md-nav__item> <a href=../../../k8s/foundamentals/statefulset/ class=md-nav__link> StatefulSet </a> </li> <li class=md-nav__item> <a href=../../../k8s/foundamentals/daemonset/ class=md-nav__link> DaemonSet </a> </li> <li class=md-nav__item> <a href=../../../k8s/foundamentals/job/ class=md-nav__link> Job and Cronjob </a> </li> <li class=md-nav__item> <a href=../../../k8s/foundamentals/configuration/ class=md-nav__link> Configuration </a> </li> <li class=md-nav__item> <a href=../../../k8s/foundamentals/secrets/ class=md-nav__link> Secrets </a> </li> <li class=md-nav__item> <a href=../../../k8s/foundamentals/persistence/ class=md-nav__link> Persistence </a> </li> <li class=md-nav__item> <a href=../../../k8s/foundamentals/rbac/ class=md-nav__link> Role Based Access Control (RBAC) </a> </li> <li class=md-nav__item> <a href=../../../k8s/foundamentals/ingress/ class=md-nav__link> Ingress </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_7 type=checkbox id=__nav_3_7> <label class=md-nav__link for=__nav_3_7> Advanced Kubernetes <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Advanced Kubernetes" data-md-level=2> <label class=md-nav__title for=__nav_3_7> <span class="md-nav__icon md-icon"></span> Advanced Kubernetes </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../k8s/foundamentals/scheduling/ class=md-nav__link> Scheduling </a> </li> <li class=md-nav__item> <a href=../../../k8s/foundamentals/hpa/ class=md-nav__link> Horizontal Pod Autoscaling </a> </li> <li class=md-nav__item> <a href=../../../k8s/foundamentals/policy/ class=md-nav__link> Policy </a> </li> <li class=md-nav__item> <a href=../../../k8s/foundamentals/networkpolicy/ class=md-nav__link> Network Policy </a> </li> <li class=md-nav__item> <a href=../../../k8s/foundamentals/clustermgt/ class=md-nav__link> Cluster Management </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_8 type=checkbox id=__nav_3_8> <label class=md-nav__link for=__nav_3_8> Operating Kubernetes <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Operating Kubernetes" data-md-level=2> <label class=md-nav__title for=__nav_3_8> <span class="md-nav__icon md-icon"></span> Operating Kubernetes </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../k8s/foundamentals/troubleshooting/ class=md-nav__link> Troubleshooting </a> </li> <li class=md-nav__item> <a href=../../../k8s/foundamentals/healthcheck/ class=md-nav__link> Health Check </a> </li> <li class=md-nav__item> <a href=../../../k8s/foundamentals/helming/ class=md-nav__link> Helming </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_9 type=checkbox id=__nav_3_9> <label class=md-nav__link for=__nav_3_9> Case Study <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Case Study" data-md-level=2> <label class=md-nav__title for=__nav_3_9> <span class="md-nav__icon md-icon"></span> Case Study </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../k8s/foundamentals/casestudy-operation-resources/ class=md-nav__link> Operations on Resources </a> </li> <li class=md-nav__item> <a href=../../../k8s/foundamentals/casestudy-health-check/ class=md-nav__link> Health Check </a> </li> <li class=md-nav__item> <a href=../../../k8s/foundamentals/casestudy-calico/ class=md-nav__link> Calico Installation </a> </li> <li class=md-nav__item> <a href=../../../k8s/foundamentals/kyma/ class=md-nav__link> Kyma </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_3_10 type=checkbox id=__nav_3_10> <label class=md-nav__link for=__nav_3_10> Demos <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Demos data-md-level=2> <label class=md-nav__title for=__nav_3_10> <span class="md-nav__icon md-icon"></span> Demos </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../k8s/demo/cap_on_kyma/ class=md-nav__link> Build CAP application on Kyma </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_4 type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4> Python <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Python data-md-level=1> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Python </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../python/ class=md-nav__link> Index </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_4_2 type=checkbox id=__nav_4_2> <label class=md-nav__link for=__nav_4_2> Python基础 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Python基础 data-md-level=2> <label class=md-nav__title for=__nav_4_2> <span class="md-nav__icon md-icon"></span> Python基础 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../python/Foundation/ch00/ class=md-nav__link> 安装 </a> </li> <li class=md-nav__item> <a href=../../../python/Foundation/ch01/ class=md-nav__link> 语言基础 </a> </li> <li class=md-nav__item> <a href=../../../python/Foundation/ch02/ class=md-nav__link> 打包和拆包 </a> </li> <li class=md-nav__item> <a href=../../../python/Foundation/ch03/ class=md-nav__link> 函数及文件 </a> </li> <li class=md-nav__item> <a href=../../../python/Foundation/ch04/ class=md-nav__link> 面向对象概念 </a> </li> <li class=md-nav__item> <a href=../../../python/Foundation/ch05/ class=md-nav__link> 面向对象特性 </a> </li> <li class=md-nav__item> <a href=../../../python/Foundation/Algorithms/ class=md-nav__link> 数据结构和算法 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_4_3 type=checkbox id=__nav_4_3> <label class=md-nav__link for=__nav_4_3> Python数据分析基础 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Python数据分析基础 data-md-level=2> <label class=md-nav__title for=__nav_4_3> <span class="md-nav__icon md-icon"></span> Python数据分析基础 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../python/DataAnalysis/ch01/ class=md-nav__link> NumPy基础 </a> </li> <li class=md-nav__item> <a href=../../../python/DataAnalysis/ch10/ class=md-nav__link> NumPy进阶 </a> </li> <li class=md-nav__item> <a href=../../../python/DataAnalysis/ch02/ class=md-nav__link> Pandas入门 </a> </li> <li class=md-nav__item> <a href=../../../python/DataAnalysis/ch03/ class=md-nav__link> 数据载入、存储及文件格式 </a> </li> <li class=md-nav__item> <a href=../../../python/DataAnalysis/ch04/ class=md-nav__link> 数据清洗与准备 </a> </li> <li class=md-nav__item> <a href=../../../python/DataAnalysis/ch05/ class=md-nav__link> 数据规整：连接、联合与重塑 </a> </li> <li class=md-nav__item> <a href=../../../python/DataAnalysis/ch06/ class=md-nav__link> 绘图与可视化 </a> </li> <li class=md-nav__item> <a href=../../../python/DataAnalysis/ch07/ class=md-nav__link> 数据聚合与分组操作 </a> </li> <li class=md-nav__item> <a href=../../../python/DataAnalysis/ch08/ class=md-nav__link> 时间序列 </a> </li> <li class=md-nav__item> <a href=../../../python/DataAnalysis/ch09/ class=md-nav__link> 高阶pandas </a> </li> <li class=md-nav__item> <a href=../../../python/DataAnalysis/ch11/ class=md-nav__link> Python建模库介绍 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" data-md-toggle=__nav_4_4 type=checkbox id=__nav_4_4> <label class=md-nav__link for=__nav_4_4> Demos <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Demos data-md-level=2> <label class=md-nav__title for=__nav_4_4> <span class="md-nav__icon md-icon"></span> Demos </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../python/Demo/CourseSystem/ class=md-nav__link> 选课系统 </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#1-installation class=md-nav__link> 1. Installation </a> <nav class=md-nav aria-label="1. Installation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#11-environment-setup class=md-nav__link> 1.1. Environment Setup </a> </li> <li class=md-nav__item> <a href=#12-install-packages class=md-nav__link> 1.2. Install Packages </a> </li> <li class=md-nav__item> <a href=#13-stage-0-the-preparation class=md-nav__link> 1.3. Stage 0 — the preparation </a> </li> <li class=md-nav__item> <a href=#14-stage-2-the-configuration class=md-nav__link> 1.4. Stage 2 — the configuration </a> </li> <li class=md-nav__item> <a href=#15-stage-3-the-deployment class=md-nav__link> 1.5. Stage 3 — the deployment </a> </li> <li class=md-nav__item> <a href=#16-stage-4-the-services class=md-nav__link> 1.6. Stage 4 — the services </a> </li> <li class=md-nav__item> <a href=#17-stage-5-the-removal-stage class=md-nav__link> 1.7. Stage 5 — the removal stage </a> </li> <li class=md-nav__item> <a href=#18-installation-guide class=md-nav__link> 1.8. Installation Guide </a> </li> <li class=md-nav__item> <a href=#19-issues-during-installation class=md-nav__link> 1.9. Issues during installation </a> </li> <li class=md-nav__item> <a href=#110-shutting-down-the-whole-ceph-cluster class=md-nav__link> 1.10. Shutting Down the Whole Ceph Cluster </a> </li> <li class=md-nav__item> <a href=#111-starting-stopping-and-restarting-services-using-targets class=md-nav__link> 1.11. Starting, Stopping, and Restarting Services Using Targets </a> </li> <li class=md-nav__item> <a href=#112-restarting-all-services class=md-nav__link> 1.12. Restarting All Services </a> </li> <li class=md-nav__item> <a href=#113-restarting-specific-services class=md-nav__link> 1.13. Restarting Specific Services </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#2-basic-operation class=md-nav__link> 2. Basic Operation </a> <nav class=md-nav aria-label="2. Basic Operation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#21-pools-and-data-placement class=md-nav__link> 2.1. Pools and Data Placement </a> <nav class=md-nav aria-label="2.1. Pools and Data Placement"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#211-enable-the-pg-autoscaler-and-balancer-modules class=md-nav__link> 2.1.1. Enable the PG Autoscaler and Balancer Modules </a> <nav class=md-nav aria-label="2.1.1. Enable the PG Autoscaler and Balancer Modules"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-view-the-state-of-all-the-manager-modules class=md-nav__link> Task 1: View the state of all the Manager Modules </a> </li> <li class=md-nav__item> <a href=#task-2-list-the-existing-pools class=md-nav__link> Task 2: List the Existing Pools </a> </li> <li class=md-nav__item> <a href=#task-3-enable-the-pg_autoscaler-module class=md-nav__link> Task 3: Enable the pg_autoscaler module </a> </li> <li class=md-nav__item> <a href=#task-4-turn-on-the-placement-group-balancer-feature class=md-nav__link> Task 4: Turn on the Placement Group balancer feature </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#212-manipulate-erasure-code-profiles class=md-nav__link> 2.1.2. Manipulate Erasure Code Profiles </a> <nav class=md-nav aria-label="2.1.2. Manipulate Erasure Code Profiles"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-display-a-list-of-the-current-erasure-code-profiles class=md-nav__link> Task 1: Display a list of the current Erasure Code profiles </a> </li> <li class=md-nav__item> <a href=#task-2-examine-the-details-of-the-default-ec-profile class=md-nav__link> Task 2: Examine the details of the default EC profile </a> </li> <li class=md-nav__item> <a href=#task-3-create-and-remove-a-new-ec-profile class=md-nav__link> Task 3: Create and remove a new EC profile </a> </li> <li class=md-nav__item> <a href=#task-4-create-a-better-ec-profile class=md-nav__link> Task 4: Create a better EC profile </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#213-manipulate-crush-map-rulesets class=md-nav__link> 2.1.3. Manipulate CRUSH Map Rulesets </a> <nav class=md-nav aria-label="2.1.3. Manipulate CRUSH Map Rulesets"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-display-a-list-of-the-current-crush-map-rules class=md-nav__link> Task 1: Display a list of the current CRUSH Map rules </a> </li> <li class=md-nav__item> <a href=#task-2-examine-the-details-of-the-default-crush-map-rule class=md-nav__link> Task 2: Examine the details of the default CRUSH Map rule </a> </li> <li class=md-nav__item> <a href=#task-3-create-and-remove-a-new-crush-map-rule class=md-nav__link> Task 3: Create and remove a new CRUSH Map rule </a> </li> <li class=md-nav__item> <a href=#task-4-create-a-better-crush-map-rule class=md-nav__link> Task 4: Create a better CRUSH Map rule </a> </li> <li class=md-nav__item> <a href=#task-5-create-crush-map-rules-for-different-classes-of-devices class=md-nav__link> Task 5: Create CRUSH Map rules for different classes of devices </a> </li> <li class=md-nav__item> <a href=#task-6-change-the-ruleset-used-by-a-pool class=md-nav__link> Task 6: Change the ruleset used by a pool </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#214-investigate-bluestore class=md-nav__link> 2.1.4. Investigate BlueStore </a> <nav class=md-nav aria-label="2.1.4. Investigate BlueStore"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-explore-the-drive_groupsyml-configuration class=md-nav__link> Task 1: Explore the drive_groups.yml configuration </a> </li> <li class=md-nav__item> <a href=#task-2-examine-a-storage-hosts-storage-devices class=md-nav__link> Task 2: Examine a storage host’s storage devices </a> </li> <li class=md-nav__item> <a href=#task-3-examine-a-storage-hosts-osd-details class=md-nav__link> Task 3: Examine a storage host’s OSD details </a> </li> <li class=md-nav__item> <a href=#task-4-display-bluestore-information-using-ceph-bluestore-tool class=md-nav__link> Task 4: Display BlueStore information using ceph-bluestore-tool </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#22-common-day-1-tasks-using-the-cli class=md-nav__link> 2.2. Common Day 1 Tasks Using the CLI </a> <nav class=md-nav aria-label="2.2. Common Day 1 Tasks Using the CLI"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#221-ceph-users-and-configuration class=md-nav__link> 2.2.1. Ceph Users and Configuration </a> <nav class=md-nav aria-label="2.2.1. Ceph Users and Configuration"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-view-the-current-user-keyrings class=md-nav__link> Task 1: View the current user keyrings </a> </li> <li class=md-nav__item> <a href=#task-2-create-a-new-keyring-and-associated-user class=md-nav__link> Task 2: Create a new keyring and associated user </a> </li> <li class=md-nav__item> <a href=#task-3-create-a-client-key-for-rbd class=md-nav__link> Task 3: Create a client key for RBD </a> </li> <li class=md-nav__item> <a href=#task-4-view-the-ceph-master-configuration-file class=md-nav__link> Task 4: View the Ceph master configuration file </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#222-run-the-ceph-health-commands class=md-nav__link> 2.2.2. Run the Ceph Health Commands </a> </li> <li class=md-nav__item> <a href=#223-manipulate-pools class=md-nav__link> 2.2.3. Manipulate Pools </a> <nav class=md-nav aria-label="2.2.3. Manipulate Pools"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-display-a-list-of-the-current-pools class=md-nav__link> Task 1: Display a list of the current pools </a> </li> <li class=md-nav__item> <a href=#task-2-display-the-usage-data-and-stats-of-the-current-pools class=md-nav__link> Task 2: Display the usage data and stats of the current pools </a> </li> <li class=md-nav__item> <a href=#task-3-create-two-new-pools-one-replicated-one-ec class=md-nav__link> Task 3: Create two new pools, one replicated, one EC </a> </li> <li class=md-nav__item> <a href=#task-4-assign-an-application-to-the-two-new-pools class=md-nav__link> Task 4: Assign an application to the two new pools </a> </li> <li class=md-nav__item> <a href=#task-5-manage-snapshots-of-the-new-rgw-bucket-pool class=md-nav__link> Task 5: Manage snapshots of the new RGW bucket pool </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#224-maintain-consistency-of-data-with-scrub-and-repair class=md-nav__link> 2.2.4. Maintain consistency of data with Scrub and Repair </a> <nav class=md-nav aria-label="2.2.4. Maintain consistency of data with Scrub and Repair"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-display-a-few-of-the-scrub-settings class=md-nav__link> Task 1: Display a few of the Scrub settings </a> </li> <li class=md-nav__item> <a href=#task-2-change-the-scrub-settings-in-cephconf class=md-nav__link> Task 2: Change the Scrub settings in ceph.conf </a> </li> <li class=md-nav__item> <a href=#task-3-change-the-scrub-settings-directly-in-the-configuration-db class=md-nav__link> Task 3: Change the Scrub settings directly in the Configuration DB </a> </li> <li class=md-nav__item> <a href=#task-4-manually-scrub-and-repair-an-osd-and-a-pg class=md-nav__link> Task 4: Manually scrub and repair an OSD and a PG </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#225-manipulate-manager-modules class=md-nav__link> 2.2.5. Manipulate Manager Modules </a> <nav class=md-nav aria-label="2.2.5. Manipulate Manager Modules"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-display-the-list-of-enabled-manager-modules class=md-nav__link> Task 1: Display the list of enabled Manager Modules </a> </li> <li class=md-nav__item> <a href=#task-4-briefly-attempt-to-use-the-crash-manager-module class=md-nav__link> Task 4: Briefly attempt to use the crash manager module </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#226-introduction-to-the-tell-command class=md-nav__link> 2.2.6. Introduction to the Tell command </a> <nav class=md-nav aria-label="2.2.6. Introduction to the Tell command"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-run-a-benchmark-test-on-an-osd class=md-nav__link> Task 1: Run a benchmark test on an OSD </a> </li> <li class=md-nav__item> <a href=#task-2-change-the-protection-setting-regarding-the-deletion-of-pools class=md-nav__link> Task 2: Change the protection setting regarding the deletion of pools </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#23-ceph-dashboard class=md-nav__link> 2.3. Ceph Dashboard </a> <nav class=md-nav aria-label="2.3. Ceph Dashboard"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#231-access-dashboard class=md-nav__link> 2.3.1. Access Dashboard </a> <nav class=md-nav aria-label="2.3.1. Access Dashboard"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-set-the-password-for-the-admin-user-of-the-ceph-dashboard class=md-nav__link> Task 1: Set the password for the admin user of the Ceph Dashboard </a> </li> <li class=md-nav__item> <a href=#task-3-visit-the-ceph-dashboard-url class=md-nav__link> Task 3: Visit the Ceph Dashboard URL </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#232-explore-the-dashboard-health-performance-status class=md-nav__link> 2.3.2. Explore the Dashboard Health, Performance, Status </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#24-storage-data-access class=md-nav__link> 2.4. Storage Data Access </a> <nav class=md-nav aria-label="2.4. Storage Data Access"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#241-ensure-the-ses-cluster-is-healthy class=md-nav__link> 2.4.1. Ensure the SES Cluster is Healthy </a> <nav class=md-nav aria-label="2.4.1. Ensure the SES Cluster is Healthy"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-check-the-clusters-health class=md-nav__link> Task 1: Check the Cluster’s health </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#242-use-the-s3-api-to-interact-with-the-rados-gateway class=md-nav__link> 2.4.2. Use the S3 API to Interact with the RADOS Gateway </a> <nav class=md-nav aria-label="2.4.2. Use the S3 API to Interact with the RADOS Gateway"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-using-the-s3cmd-tool-and-create-an-s3-user class=md-nav__link> Task 1: Using the s3cmd tool and create an S3 user </a> </li> <li class=md-nav__item> <a href=#task-2-create-a-new-s3cmd-configuration-file-and-a-new-s3-bucket class=md-nav__link> Task 2: Create a new s3cmd configuration file and a new S3 bucket </a> </li> <li class=md-nav__item> <a href=#task3-create-and-upload-a-file-to-a-bucket-using-the-s3-api class=md-nav__link> Task3: Create and upload a file to a bucket using the S3 API </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#243-use-the-swift-api-to-interact-with-the-rados-gateway class=md-nav__link> 2.4.3. Use the swift API to Interact with the RADOS Gateway </a> <nav class=md-nav aria-label="2.4.3. Use the swift API to Interact with the RADOS Gateway"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-create-a-swift-subuser class=md-nav__link> Task 1: Create a swift subuser </a> </li> <li class=md-nav__item> <a href=#task-2-use-the-swift-command-to-access-a-file-created-with-the-s3cmd-tool class=md-nav__link> Task 2: Use the swift command to access a file created with the S3cmd tool </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#244-create-snapshots-on-ses-using-rbd class=md-nav__link> 2.4.4. Create Snapshots on SES using RBD </a> <nav class=md-nav aria-label="2.4.4. Create Snapshots on SES using RBD"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-create-a-new-pool-for-rbd-images class=md-nav__link> Task 1: Create a new pool for RBD images </a> </li> <li class=md-nav__item> <a href=#task-2-create-a-new-rbd-image-in-the-rbd-images-pool class=md-nav__link> Task 2: Create a new RBD image in the rbd-images pool </a> </li> <li class=md-nav__item> <a href=#task-3-mount-the-new-image-on-the-admin-node-and-create-a-filesystem class=md-nav__link> Task 3: Mount the new image on the admin node and create a filesystem </a> </li> <li class=md-nav__item> <a href=#task-4-create-a-file-on-the-new-filesystem-and-snapshot-the-rbd-image-and-make-some-additional-changes class=md-nav__link> Task 4: Create a file on the new filesystem and snapshot the rbd image and make some additional changes </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#245-create-and-manage-cow-clones-with-rbd class=md-nav__link> 2.4.5. Create and manage COW Clones with rbd </a> <nav class=md-nav aria-label="2.4.5. Create and manage COW Clones with rbd"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-create-a-new-pool class=md-nav__link> Task 1: Create a new pool </a> </li> <li class=md-nav__item> <a href=#task-4-snapshot-the-rbd-image-and-protect-the-snapshot class=md-nav__link> Task 4: Snapshot the rbd image and protect the snapshot </a> </li> <li class=md-nav__item> <a href=#task-7-flatten-a-cow-clone-and-remove-the-parent-image class=md-nav__link> Task 7: Flatten a COW Clone and remove the parent image </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#246-configure-iscsi-on-ses class=md-nav__link> 2.4.6. Configure iSCSI on SES </a> <nav class=md-nav aria-label="2.4.6. Configure iSCSI on SES"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-create-a-new-rbd-image-in-the-iscsi-images-pool class=md-nav__link> Task 1: Create a new RBD image in the iscsi-images pool </a> </li> <li class=md-nav__item> <a href=#task-2-define-a-new-iscsi-target-with-the-ceph-dashboard class=md-nav__link> Task 2: Define a new iSCSI target with the Ceph Dashboard </a> </li> <li class=md-nav__item> <a href=#task-3-access-the-new-iscsi-target-from-the-admin-node class=md-nav__link> Task 3: Access the new iSCSI target from the admin node </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#247-mount-cephfs-provided-by-suse-enterprise-storage class=md-nav__link> 2.4.7. Mount CephFS Provided by SUSE Enterprise Storage </a> <nav class=md-nav aria-label="2.4.7. Mount CephFS Provided by SUSE Enterprise Storage"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-verify-cephfs-configuration-of-the-ses-cluster class=md-nav__link> Task 1: Verify cephfs configuration of the SES cluster </a> </li> <li class=md-nav__item> <a href=#task-3-mount-the-ceph-filesystem-on-the-admin-node class=md-nav__link> Task 3: Mount the ceph filesystem on the admin node </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#248-export-an-nfs-share-from-ses-with-nfs-ganesha class=md-nav__link> 2.4.8. Export an NFS Share from SES with NFS Ganesha </a> <nav class=md-nav aria-label="2.4.8. Export an NFS Share from SES with NFS Ganesha"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-0-install-and-configure-ganesha-ganesha-config-location-is-not-configured-please-set-the-ganesha_rados_pool_namespace-setting class=md-nav__link> Task 0: Install and configure Ganesha (Ganesha config location is not configured. Please set the GANESHA_RADOS_POOL_NAMESPACE setting.) </a> </li> <li class=md-nav__item> <a href=#task-1-create-an-nfs-export-using-the-ceph-dashboard class=md-nav__link> Task 1: Create an NFS export using the Ceph Dashboard </a> </li> <li class=md-nav__item> <a href=#task-2-mount-the-nfs-export-on-the-admin-node class=md-nav__link> Task 2: Mount the NFS export on the admin node </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#249-configure-and-mount-cifs class=md-nav__link> 2.4.9. Configure and Mount CIFS </a> <nav class=md-nav aria-label="2.4.9. Configure and Mount CIFS"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#task-1-prepare-the-cephfs-share-for-cifs class=md-nav__link> Task 1: Prepare the CephFS share for CIFS </a> </li> <li class=md-nav__item> <a href=#task-2-create-a-samba-gateway-specific-keyring-on-the-ceph-admin-node-and-copy-it-to-the-samba-gateway-node class=md-nav__link> Task 2: Create a Samba gateway specific keyring on the Ceph admin node and copy it to the Samba gateway node </a> </li> <li class=md-nav__item> <a href=#task-3-configure-samba-on-the-samba-gateway-node class=md-nav__link> Task 3: Configure Samba on the Samba gateway node </a> </li> <li class=md-nav__item> <a href=#task-4-connect-a-client-to-the-samba-gateway class=md-nav__link> Task 4: Connect a client to the Samba gateway </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/huyuhui001/mySite/edit/main/docs/linux/SES/linux_ses_demo.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg> </a> <h1 id=suse-enterprise-storage-6-installation-and-basic-operation>SUSE Enterprise Storage 6 Installation and Basic Operation<a class=headerlink href=#suse-enterprise-storage-6-installation-and-basic-operation title="Permanent link"> ¶</a></h1> <h2 id=1-installation>1. Installation<a class=headerlink href=#1-installation title="Permanent link"> ¶</a></h2> <h3 id=11-environment-setup>1.1. Environment Setup<a class=headerlink href=#11-environment-setup title="Permanent link"> ¶</a></h3> <p>In this demo, I use below environment, including VM setting and software installed. </p> <p>All VMs installed here was built on a physical host <code>10.58.121.68</code>.</p> <div class=highlight><pre><span></span><code>Host Server:
    10.58.121.68  root / rootroot

Account
    root / root123

Gateway:
    10.58.120.1

Network Mask:
    255.255.254.0

Nameserver:
    10.58.32.32
    10.33.50.20

Domain Search
    sha.me.corp
    dhcp.sha.me.corp
    me.corp
    ind.me.corp
    bgr.me.corp
</code></pre></div> <p>SUSE Server 15 SP1 Extensions and Modules were installed as below.</p> <div class=highlight><pre><span></span><code>[x] SUSE Enterprise Storage 6 
[x] Basesystem Module 15 SP1 x86_64
[x] Server Applications Module 15 SP1 x86_64
</code></pre></div> <p>Disable Services is as below:</p> <div class=highlight><pre><span></span><code>AppArmor
Firewall
</code></pre></div> <p>Enable Services is as below. SSH</p> <p>Register SLES15.1 to local SMT.</p> <div class=highlight><pre><span></span><code># SUSEConnect --url https://smtproxy.ind.me.corp
</code></pre></div> <p>Demo Environment summary is below.</p> <table> <thead> <tr> <th>Alias</th> <th>Host Name</th> <th>Memory</th> <th>Disk</th> <th>eth0</th> <th>eth0 mac address</th> </tr> </thead> <tbody> <tr> <td>sles01</td> <td>admin (salt-master)</td> <td>16GB</td> <td>Disk1: 20G</td> <td>10.58.121.181/23</td> <td>52:54:00:23:7d:cd</td> </tr> <tr> <td>sles02</td> <td>data1</td> <td>16GB</td> <td>Disk1: 20G</td> <td>10.58.121.182/23</td> <td>52:54:00:5f:ce:6f</td> </tr> <tr> <td></td> <td></td> <td></td> <td>Disk2: 8G</td> <td></td> <td></td> </tr> <tr> <td></td> <td></td> <td></td> <td>Disk3: 8G</td> <td></td> <td></td> </tr> <tr> <td></td> <td></td> <td></td> <td>Disk4: 8G</td> <td></td> <td></td> </tr> <tr> <td>sles03</td> <td>data2</td> <td>16GB</td> <td>Disk1: 20G</td> <td>10.58.121.183/23</td> <td>52:54:00:6f:f2:23</td> </tr> <tr> <td></td> <td></td> <td></td> <td>Disk2: 8G</td> <td></td> <td></td> </tr> <tr> <td></td> <td></td> <td></td> <td>Disk3: 8G</td> <td></td> <td></td> </tr> <tr> <td></td> <td></td> <td></td> <td>Disk4: 8G</td> <td></td> <td></td> </tr> <tr> <td>sles04</td> <td>data3</td> <td>16GB</td> <td>Disk1: 20G</td> <td>10.58.121.184/23</td> <td>52:54:00:93:4c:67</td> </tr> <tr> <td></td> <td></td> <td></td> <td>Disk2: 8G</td> <td></td> <td></td> </tr> <tr> <td></td> <td></td> <td></td> <td>Disk3: 8G</td> <td></td> <td></td> </tr> <tr> <td></td> <td></td> <td></td> <td>Disk4: 8G</td> <td></td> <td></td> </tr> <tr> <td>sles05</td> <td>data4</td> <td>16GB</td> <td>Disk1: 20G</td> <td>10.58.121.185/23</td> <td>52:54:00:90:b0:b0</td> </tr> <tr> <td></td> <td></td> <td></td> <td>Disk2: 8G</td> <td></td> <td></td> </tr> <tr> <td></td> <td></td> <td></td> <td>Disk3: 8G</td> <td></td> <td></td> </tr> <tr> <td></td> <td></td> <td></td> <td>Disk4: 8G</td> <td></td> <td></td> </tr> <tr> <td>sles06</td> <td>mon1</td> <td>16GB</td> <td>Disk1: 20G</td> <td>10.58.121.186/23</td> <td>52:54:00:46:43:7a</td> </tr> <tr> <td>sles07</td> <td>mon2</td> <td>16GB</td> <td>Disk1: 20G</td> <td>10.58.121.187/23</td> <td>52:54:00:00:fe:6b</td> </tr> <tr> <td>sles08</td> <td>mon3</td> <td>16GB</td> <td>Disk1: 20G</td> <td>10.58.121.188/23</td> <td>52:54:00:60:a3:92</td> </tr> </tbody> </table> <p>Add hostname to file <code>/etc/hosts</code> (all nodes)</p> <ul> <li>If you do not specify a cluster network during Ceph deployment, it assumes a single public network environment.</li> <li>Make sure that the fully qualified domain name (FQDN) of each node can be resolved to the public network IP address by all other nodes. </li> </ul> <div class=highlight><pre><span></span><code># vi /etc/hosts
10.58.121.181   admin.sha.me.corp admin salt
10.58.121.182   data1.sha.me.corp data1
10.58.121.183   data2.sha.me.corp data2
10.58.121.184   data3.sha.me.corp data3
10.58.121.185   data4.sha.me.corp data4
10.58.121.186   mon1.sha.me.corp mon1
10.58.121.187   mon2.sha.me.corp mon2
10.58.121.188   mon3.sha.me.corp mon3
</code></pre></div> <p>Add all nodes as trust ssh access (root account) <div class=highlight><pre><span></span><code># cd ~
# ssh-keygen -t rsa
# ssh-copy-id -i ~/.ssh/id_rsa.pub root@admin
# ssh-copy-id -i ~/.ssh/id_rsa.pub root@data1
# ssh-copy-id -i ~/.ssh/id_rsa.pub root@data2
# ssh-copy-id -i ~/.ssh/id_rsa.pub root@data3
# ssh-copy-id -i ~/.ssh/id_rsa.pub root@data4
# ssh-copy-id -i ~/.ssh/id_rsa.pub root@mon1
# ssh-copy-id -i ~/.ssh/id_rsa.pub root@mon2
# ssh-copy-id -i ~/.ssh/id_rsa.pub root@mon3

# ssh admin.sha.me.corp
# ssh data1.sha.me.corp
# ssh data2.sha.me.corp
# ssh data3.sha.me.corp
# ssh data4.sha.me.corp
# ssh mon1.sha.me.corp
# ssh mon2.sha.me.corp
# ssh mon3.sha.me.corp
# ssh salt
# ssh admin
# ssh data1
# ssh data2
# ssh data3
# ssh data4
# ssh mon1
# ssh mon2
# ssh mon3
</code></pre></div></p> <p>Disable firewall (all nodes) <div class=highlight><pre><span></span><code># sudo /sbin/SuSEfirewall2 off
# firewall-cmd --state
    not running

# systemctl stop firewalld.service
# systemctl status firewalld.service
    firewalld.service - firewalld - dynamic firewall daemon
    Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: disabled)
    Active: inactive (dead)
  Docs: man:firewalld(1)
</code></pre></div></p> <p>Disable IPv6 (all nodes) and Set kernel pid to max value (all nodes) <div class=highlight><pre><span></span><code># vi /etc/sysctl.conf
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
kernel.pid_max = 4194303
# sysctl -p
</code></pre></div></p> <p>Set <code>DEV_ENV=true</code> in <code>/etc/profile.local</code> in all nodes</p> <p>Install basic software (all nodes) <div class=highlight><pre><span></span><code># zypper in -y -t pattern yast2_basis base
# zypper in -y net-tools vim man sudo tuned irqbalance
# zypper in -y ethtool rsyslog iputils less supportutils-plugin-ses
# zypper in -y net-tools-deprecated tree wget
</code></pre></div></p> <p>Configure NTP service (all nodes), Setting via YaST2 and add server <code>cn.pool.ntp.,org</code>. </p> <p>And <code>/etc/chrony.conf</code> file looks like below. <div class=highlight><pre><span></span><code>admin:~ # cat /etc/chrony.conf
# Use public servers from the pool.ntp.org project.
pool cn.pool.ntp.org iburst
! pool pool.ntp.org iburst

# Record the rate at which the system clock gains/losses time.
driftfile /var/lib/chrony/drift

# Allow the system clock to be stepped in the first three updates
# if its offset is larger than 1 second.
makestep 1.0 3

# Enable kernel synchronization of the real-time clock (RTC).
rtcsync

# Enable hardware timestamping on all interfaces that support it.
#hwtimestamp *

# Increase the minimum numbgr of selectable sources required to adjust
# the system clock.
#minsources 2

# Allow NTP client access from local network.
#allow 192.168.0.0/16

# Serve time even if not synchronized to a time source.
#local stratum 10

# Specify file containing keys for NTP authentication.
#keyfile /etc/chrony.keys

# Get TAI-UTC offset and leap seconds from the system tz database.
#leapsectz right/UTC

# Specify directory for log files.
logdir /var/log/chrony

# Select which information is logged.
#log measurements statistics tracking

# Also include any directives found in configuration files in /etc/chrony.d
include /etc/chrony.d/*.conf
</code></pre></div></p> <p>Make <code>/etc/chrony.conf</code> effective.</p> <div class=highlight><pre><span></span><code># systemctl enable chronyd.service
# systemctl restart chronyd.service
# systemctl status chronyd.service

# chronyc sources
</code></pre></div> <h3 id=12-install-packages>1.2. Install Packages<a class=headerlink href=#12-install-packages title="Permanent link"> ¶</a></h3> <p>Install <code>salt-minion</code> on all nodes. And start the service. </p> <div class=highlight><pre><span></span><code>Hostname is in file `/etc/salt/minion_id`
# zypper in -y salt-minion

Uncomment below to let all nodes know who is master
# vi /etc/salt/minion
master: salt 

# systemctl enable salt-minion.service
# systemctl start salt-minion.service
# systemctl status salt-minion.service
</code></pre></div> <p>Install Ceph in admin node. Check log in <code>/var/log/salt</code></p> <div class=highlight><pre><span></span><code>admin:~ # zypper in -y salt-master
admin:~ # systemctl enable salt-master.service
admin:~ # systemctl start salt-master.service
admin:~ # systemctl status salt-master.service

Note: ganesha will be installed on mon1, not admin node.
admin:~ # zypper se ganesha
admin:~ # zypper in nfs-ganesha
admin:~ # systemctl enable nfs-ganesha
admin:~ # systemctl start nfs-ganesha
admin:~ # systemctl status nfs-ganesha
</code></pre></div> <p>admin:~ # cd /var/log/salt</p> <p>List fingerprints of all unaccepted minion keys on the Salt master. <div class=highlight><pre><span></span><code>admin:~ # salt-key -F
Local Keys:
master.pem:  c0:e5:***:04:c7
master.pub:  43:73:***:6a:34
Unaccepted Keys:
admin.sha.me.corp:  fe:51:***:b9:48
mon1.sha.me.corp:  94:13:***:91:63
mon2.sha.me.corp:  c0:fd:***:39:3f
mon3.sha.me.corp:  38:fc:***:2e:05
data1.sha.me.corp:  b6:6c:***:63:4f
data2.sha.me.corp:  ab:14:***:c8:ac
data3.sha.me.corp:  90:3f:***:76:3b
data4.sha.me.corp:  d8:12:***:f1:20
</code></pre></div></p> <p>If the minions' fingerprints match, accept them <div class=highlight><pre><span></span><code>admin:~ # salt-key --accept-all
The following keys are going to be accepted:
Unaccepted Keys:
admin.sha.me.corp
mon1.sha.me.corp
mon2.sha.me.corp
mon3.sha.me.corp
data1.sha.me.corp
data2.sha.me.corp
data3.sha.me.corp
data4.sha.me.corp
Proceed? [n/Y] Y
Key for minion admin.sha.me.corp accepted.
Key for minion mon1.sha.me.corp accepted.
Key for minion mon2.sha.me.corp accepted.
Key for minion mon3.sha.me.corp accepted.
Key for minion data1.sha.me.corp accepted.
Key for minion data2.sha.me.corp accepted.
Key for minion data3.sha.me.corp accepted.
Key for minion data4.sha.me.corp accepted.
</code></pre></div></p> <p>Verify that the keys have been accepted <div class=highlight><pre><span></span><code>admin:~ # salt-key -F
admin:~ # salt-key --list-all
Accepted Keys:
admin.sha.me.corp
data1.sha.me.corp
data2.sha.me.corp
data3.sha.me.corp
data4.sha.me.corp
mon1.sha.me.corp
mon2.sha.me.corp
mon3.sha.me.corp
Denied Keys:
Unaccepted Keys:
Rejected Keys:
</code></pre></div></p> <p>Zero out all drivers which will be used as OSDs (optional) <div class=highlight><pre><span></span><code>data1:~ lsblk

data1:~ # for I in {b,c,d}; do dd if=/dev/zero of=dev/sd$i bs=512 count=40 oflag=direct; done

data2:~ # for I in {b,c,d}; do dd if=/dev/zero of=dev/sd$i bs=512 count=40 oflag=direct; done

data3:~ # for I in {b,c,d}; do dd if=/dev/zero of=dev/sd$i bs=512 count=40 oflag=direct; done
</code></pre></div></p> <p>Install DeepSea <div class=highlight><pre><span></span><code>admin:~ # zypper in -y deepsea
</code></pre></div></p> <p>Edit the <code>/srv/pillar/ceph/deepsea_minions.sls</code> file on the Salt master (admin node) and add or replace the following line: </p> <div class=highlight><pre><span></span><code>admin:~ # vi /srv/pillar/ceph/deepsea_minions.sls
# Choose minions with a deepsea grain
deepsea_minions: &#39;G@deepsea:*&#39;  #Match all Salt minions in the cluster
# Choose all minions
# deepsea_minions: &#39;*&#39;  #Match all minions with the &#39;deepsea&#39; grain
# Choose custom Salt targeting
# deepsea_minions: &#39;ses*&#39;
# deepsea_minions: &#39;ceph* or salt&#39;
</code></pre></div> <p>Target the Minions</p> <ul> <li>Affirm salt-master (admin node) can communicate with the minions. And deploy the grains from admin node to all minions. <div class=highlight><pre><span></span><code>admin:~ # salt &#39;*&#39; test.ping
mon1.sha.me.corp:
    True
data4.sha.me.corp:
    True
data3.sha.me.corp:
    True
data2.sha.me.corp:
    True
data1.sha.me.corp:
    True
mon3.sha.me.corp:
    True
admin.sha.me.corp:
    True
mon2.sha.me.corp:
    True
</code></pre></div></li> </ul> <p>Apply the 'deepsea' grain to a group of minions, and target with a DeepSea Grain <div class=highlight><pre><span></span><code>admin:~ # salt &#39;*&#39; grains.append deepsea default
data3.sha.me.corp:
    The val default was already in the list deepsea
mon2.sha.me.corp:
    The val default was already in the list deepsea
data1.sha.me.corp:
    The val default was already in the list deepsea
data4.sha.me.corp:
    The val default was already in the list deepsea
data2.sha.me.corp:
    The val default was already in the list deepsea
mon3.sha.me.corp:
    The val default was already in the list deepsea
admin.sha.me.corp:
    The val default was already in the list deepsea
mon1.sha.me.corp:
    The val default was already in the list deepsea

admin:~ # salt -G &#39;deepsea:*&#39; test.ping  (The following command is an equivalent)
admin:~ # salt -C &#39;G@deepsea:*&#39; test.ping
admin.sha.me.corp:
    True
data3.sha.me.corp:
    True
mon1.sha.me.corp:
    True
mon2.sha.me.corp:
    True
data2.sha.me.corp:
    True
data4.sha.me.corp:
    True
mon3.sha.me.corp:
    True
data1.sha.me.corp:
    True
</code></pre></div></p> <h3 id=13-stage-0-the-preparation>1.3. Stage 0 — the preparation<a class=headerlink href=#13-stage-0-the-preparation title="Permanent link"> ¶</a></h3> <p>Run Stage 0—the preparation</p> <ul> <li>During this stage, all required updates are applied and your system may be rebooted. </li> <li>If there are errors, re-run the stage. <div class=highlight><pre><span></span><code>admin:~ # deepsea stage run ceph.stage.0 (The following commands are equivalents)
admin:~ # salt-run state.orch ceph.stage.0
admin:~ # salt-run state.orch ceph.stage.prep
</code></pre></div></li> </ul> <p>Run Stage 1—the discovery</p> <ul> <li>Here all hardware in your cluster is being detected and necessary information for the Ceph configuration is being collected.</li> <li>The discovery stage collects data from all minions and creates configuration fragments that are stored in the directory <code>/srv/pillar/ceph/proposals</code>. The data are stored in the YAML format in <code>*.sls</code> or <code>*.yml</code> files</li> </ul> <div class=highlight><pre><span></span><code>admin:~ # deepsea stage run ceph.stage.1 (The following commands are equivalents)
admin:~ # salt-run state.orch ceph.stage.1
admin:~ # salt-run state.orch ceph.stage.discovery
</code></pre></div> <h3 id=14-stage-2-the-configuration>1.4. Stage 2 — the configuration<a class=headerlink href=#14-stage-2-the-configuration title="Permanent link"> ¶</a></h3> <p>Run Stage 2 — the configuration — you need to prepare configuration data in a particular format. </p> <ul> <li>The assignment follows this pattern:<ul> <li><code>role-ROLE_NAME/PATH/FILES_TO_INCLUDE</code> (NOTE, the parent directory of PATH is <code>/srv/pillar/ceph/</code>)</li> </ul> </li> <li>To avoid trouble with performance and the upgrade procedure, do not deploy the Ceph OSD, Metadata Server, or Ceph Monitor role to the Admin Node. </li> <li>Monitors, Metadata Server, and gateways can be co-located on the OSD nodes.</li> <li>If you are using CephFS, S3/Swift, iSCSI, at least two instances of the respective roles (Metadata Server, Object Gateway, iSCSI) are required for redundancy and availability. </li> </ul> <div class=highlight><pre><span></span><code>admin:~ # cp /usr/share/doc/packages/deepsea/examples/policy.cfg-rolebased /srv/pillar/ceph/proposals/policy.cfg
admin:~ # vi /srv/pillar/ceph/proposals/policy.cfg
## Cluster Assignment
# Add all nodes into Ceph cluster
cluster-ceph/cluster/*.sls

## Roles
# The Admin node fills the “master” and “admin” roles for DeepSea
# The master role is mandatory, always add a similar line to the following
role-master/cluster/admin*.sls
role-admin/cluster/admin*.sls

# Monitoring
# Cluster monitoring and data graphs, most commonly they run on Admin node
# NFS Ganesha is configured via the file /etc/ganesha/ganesha.conf
# As additional configuration is required to install NFS Ganesha, you can install NFS Ganesha later. 
# The following requirements need to be met before DeepSea stages 2 and 4 can be executed to install NFS Ganesha:
#  a)At least one node needs to be assigned the role-ganesha.
#  b)You can define only one role-ganesha per minion.
#  c)NFS Ganesha needs either an Object Gateway or CephFS to work, otherwise the validation will fail in Stage 3.
#  d)The kernel based NFS needs to be disabled on minions with the role-ganesha role. 
role-prometheus/cluster/admin*.sls
role-grafana/cluster/mon1*.sls

# MON
# The minion will provide the monitor service to the Ceph cluster
role-mon/cluster/mon*.sls

# MGR
# The Ceph manager daemon which collects all the state information from the whole cluster
# Deploy it on all minions where you plan to deploy the Ceph monitor role
role-mgr/cluster/mon1*.sls

# MDS
# The minion will provide the metadata service to support CephFS
role-mds/cluster/mon*.sls

# IGW
# The minion will act as an iSCSI Gateway
role-igw/cluster/mon2*.sls

# RGW
# The minion will act as an Object Gateway
role-rgw/cluster/mon3*.sls

# Storage
# Use this role to specify storage nodes
# It points to data1~4 nodes with a wildcard.
role-storage/cluster/data*.sls

# COMMON
# It includes configuration files generated during the discovery (Stage 1)
# Accept the default values for common configuration parameters such as fsid and public_network
config/stack/default/global.yml
config/stack/default/ceph/cluster.yml

admin:~ # deepsea stage run ceph.stage.2  (The following commands are equivalents)
admin:~ # salt-run state.orch ceph.stage.2
admin:~ # salt-run state.orch ceph.stage.configure
</code></pre></div> <p>After the command succeeds, run below command to view the pillar data for the specified minions <div class=highlight><pre><span></span><code>admin:~ # salt &#39;mon*&#39; pillar.items
admin:~ # salt &#39;*&#39; saltutil.pillar_refresh
</code></pre></div></p> <p>Check time server (admin node) (the directory /srv/pillar/ceph/stack was initialized after deepsea installed, but global.yml file was not created yet until stage 2)</p> <p>By default, DeepSea uses the Admin Node as the time server for other cluster nodes. <div class=highlight><pre><span></span><code>admin:~ # cat /srv/pillar/ceph/stack/default/global.yml  (this file will be generated after stage 2)
monitoring:
  prometheus:
    metric_relabel_config:
      ceph: []
      grafana: []
      node_exporter: []
      prometheus: []
    relabel_config:
      ceph: []
      grafana: []
      node_exporter: []
      prometheus: []
    rule_files: []
    scrape_interval:
      ceph: 10s
      grafana: 10s
      node_exporter: 10s
      prometheus: 10s
    target_partition:
      ceph: 1/1
      grafana: 1/1
      node_exporter: 1/1
      prometheus: 1/1
time_server: admin.sha.me.corp
</code></pre></div></p> <p>Verify network (the directory <code>/srv/pillar/ceph/stack</code> was initialized after deepsea installed, but cluster.yml file was not created until stage 2 )</p> <div class=highlight><pre><span></span><code>admin:~ # cat /srv/pillar/ceph/stack/ceph/cluster.yml --nothing
admin:~ # cat /srv/pillar/ceph/stack/default/ceph/cluster.yml
available_roles:
- storage
- admin
- mon
- mds
- mgr
- igw
- grafana
- prometheus
- storage
- rgw
- ganesha
- client-cephfs
- client-radosgw
- client-iscsi
- client-nfs
- benchmark-rbd
- benchmark-blockdev
- benchmark-fs
- master
cluster_network: 10.58.120.0/23
fsid: 343ee7d3-232f-4c71-8216-1edbc55ac6e0
public_network: 10.58.120.0/23
</code></pre></div> <p>Note: customized file will overwrite default one.</p> <p>Default file:<code>/srv/pillar/ceph/stack/default/ceph/cluster.yml</code></p> <p>Customized file:<code>/srv/pillar/ceph/stack/ceph/cluster.yml</code></p> <p>Check DriveGroup</p> <ul> <li>DriveGroups specify the layouts of OSDs in the Ceph cluster. </li> <li>They are defined in a single file <code>/srv/salt/ceph/configuration/files/drive_groups.yml</code></li> </ul> <div class=highlight><pre><span></span><code>admin:~ # cat /srv/salt/ceph/configuration/files/drive_groups.yml
    default_drive_group_name:
      target: &#39;data*&#39;    &lt;--original: &#39;I@role:storage&#39;
      data_devices:
        all: true

admin:~ # salt &#39;data*&#39; pillar.items | grep -B5 stroage
</code></pre></div> <h3 id=15-stage-3-the-deployment>1.5. Stage 3 — the deployment<a class=headerlink href=#15-stage-3-the-deployment title="Permanent link"> ¶</a></h3> <p>Run Stage 3 — the deployment — creates a basic Ceph cluster with mandatory Ceph services.</p> <ul> <li>This Deployment stage has more than 60 automated steps. Be patient and make sure the stage completes successfully before proceeding.</li> </ul> <p>Set dev environment and disable subvolume:</p> <div class=highlight><pre><span></span><code>admin:~ # vi /srv/pillar/ceph/stack/global.yml
admin:~ # vi /srv/pillar/ceph/stack/default/global.yml
monitoring:
  prometheus:
    metric_relabel_config:
      ceph: []
      grafana: []
      node_exporter: []
      prometheus: []
    relabel_config:
      ceph: []
      grafana: []
      node_exporter: []
      prometheus: []
    rule_files: []
    scrape_interval:
      ceph: 10s
      grafana: 10s
      node_exporter: 10s
      prometheus: 10s
    target_partition:
      ceph: 1/1
      grafana: 1/1
      node_exporter: 1/1
      prometheus: 1/1
time_server: admin.sha.me.corp
DEV_ENV: True
subvolume_init: disabled

admin:~ # salt &#39;*&#39; saltutil.pillar_refresh
</code></pre></div> <p>Note: customized file will overwrite default one.</p> <p>Default file:<code>/srv/pillar/ceph/stack/default/global.yml</code></p> <p>Customized file:<code>/srv/pillar/ceph/stack/global.yml</code></p> <div class=highlight><pre><span></span><code>admin:~ # deepsea stage run ceph.stage.3  (The following commands are equivalents)
admin:~ # salt-run state.orch ceph.stage.3
admin:~ # salt-run state.orch ceph.stage.deploy
</code></pre></div> <p>After the command succeeds, run the following to check the status: <div class=highlight><pre><span></span><code>admin:~ # ceph -s
</code></pre></div></p> <p>Below comands return you a structure of matching disks based on your DriveGroups. (will show available information after stage 3) <div class=highlight><pre><span></span><code>admin:~ # salt-run disks.Report
admin:~ # salt-run disks.list
admin:~ # salt-run disks.details
</code></pre></div></p> <h3 id=16-stage-4-the-services>1.6. Stage 4 — the services<a class=headerlink href=#16-stage-4-the-services title="Permanent link"> ¶</a></h3> <p>Run Stage 4 — the services — additional features of Ceph like iSCSI, Object Gateway and CephFS can be installed in this stage. Each is optional. </p> <div class=highlight><pre><span></span><code>admin:~ # deepsea stage run ceph.stage.4  (The following commands are equivalents)
admin:~ # salt-run state.orch ceph.stage.4
admin:~ # salt-run state.orch ceph.stage.services

admin:~ # ceph osd lspools
1 iscsi-images
2 cephfs_data
3 cephfs_metadata
4 .rgw.root
5 default.rgw.control
6 default.rgw.meta
7 default.rgw.log
</code></pre></div> <p>Before logon to dashboard via url, need get credentials first</p> <div class=highlight><pre><span></span><code>admin:~ # salt-call grains.get dashboard_creds
local:
    ----------
    admin:
        &lt;your password&gt;   --&gt; the password was changed to mypassword to log on to dashboard

admin:~ # ceph mgr services
{
    &quot;dashboard&quot;: &quot;https://mon1.sha.me.corp:8443/&quot;,
    &quot;prometheus&quot;: &quot;http://mon1.sha.me.corp:9283/&quot;
}

https://10.58.121.186:8443
http://10.58.121.186:9283



admin:~ # watch ceph -s
Every 2.0s: ceph -s                  admin: Mon Oct  5 14:41:51 2020
  cluster:
    id:     &lt;id&gt;
    health: HEALTH_OK

  services:s: ceph -s
    mon: 3 daemons, quorum mon1,mon2,mon3 (age 87m)
    mgr: mon1(active, since 82m)
    mds: cephfs:1 {0=mon3=up:active} 2 up:standby
    osd: 12 osds: 12 up (since 85m), 12 in (since 85m)
    rgw: 1 daemon active (mon3)

  task status:
    scrub status:
        mds.mon3: idle

  data:
    pools:   7 pools, 576 pgs
    objects: 213 objects, 4.2 KiB
    usage:   12 GiB used, 84 GiB / 96 GiB avail
    pgs:     576 active+clean

  io:
    client:   852 B/s rd, 0 op/s rd, 0 op/s wr
</code></pre></div> <h3 id=17-stage-5-the-removal-stage>1.7. Stage 5 — the removal stage<a class=headerlink href=#17-stage-5-the-removal-stage title="Permanent link"> ¶</a></h3> <p>Run Stage 5 — the removal stage. This stage is not mandatory and during the initial setup it is usually not needed. In this stage the roles of minions and also the cluster configuration are removed. You need to run this stage when you need to remove a storage node from your cluster. <div class=highlight><pre><span></span><code>admin:~ # deepsea stage run ceph.stage.
</code></pre></div></p> <h3 id=18-installation-guide>1.8. Installation Guide<a class=headerlink href=#18-installation-guide title="Permanent link"> ¶</a></h3> <p><a href=https://documentation.suse.com/ses/6/html/ses-all/book-storage-deployment.html>Deployment Guide (EN)</a></p> <p><a href=https://documentation.suse.com/zh-cn/ses/6/html/ses-all/book-storage-deployment.html>Deployment Guide (ZH)</a></p> <h3 id=19-issues-during-installation>1.9. Issues during installation<a class=headerlink href=#19-issues-during-installation title="Permanent link"> ¶</a></h3> <p>[ERROR]: The Salt Master has cached the public key for this node</p> <p><a href=[link](https://github.com/SUSE/DeepSea/issues/1593)>SOLUTION</a>: Restart minions service</p> <p>[ERROR]: This server_id is computed nor by Adler32 neither by CRC32</p> <p>[QUESTION]: How to change new salt key <div class=highlight><pre><span></span><code>Stop salt-minion service
# systemctl stop salt-minion

Delete salt-minion pulic key
# rm /etc/salt/pki/minion/minion.pub
# rm /etc/salt/pki/minion/minion.pem

Change new minion_id
admin:~ # echo admin.sha.me.corp &gt; /etc/salt/minion_id
data1:~ # echo data1.sha.me.corp &gt; /etc/salt/minion_id
data2:~ # echo data2.sha.me.corp &gt; /etc/salt/minion_id
data3:~ # echo data3.sha.me.corp &gt; /etc/salt/minion_id
data4:~ # echo data4.sha.me.corp &gt; /etc/salt/minion_id
mon1:~ # echo mon1.sha.me.corp &gt; /etc/salt/minion_id
mon2:~ # echo mon2.sha.me.corp &gt; /etc/salt/minion_id
mon3:~ # echo mon3.sha.me.corp &gt; /etc/salt/minion_id

Delete old ID on admin node
# salt-key -D

Restart salt-minion service
# systemctl restart salt-minion

Accept all new key on admin node
admin:~ # salt-key -L
admin:~ # salt-key -A 
or
admin:~ # salt-key -a admin.sha.me.corp
data1:~ # salt-key -a data1.sha.me.corp
data2:~ # salt-key -a data2.sha.me.corp
data3:~ # salt-key -a data3.sha.me.corp
data4:~ # salt-key -a data4.sha.me.corp
mon1:~ # salt-key -a mon1.sha.me.corp
mon2:~ # salt-key -a mon2.sha.me.corp
mon3:~ # salt-key -a mon3.sha.me.corp
</code></pre></div></p> <p>[ERROR] ['/var/lib/ceph subvolume missing on mon3.sha.me.corp', '/var/lib/ceph subvolume missing on mon1.sha.me.corp', '/var/lib/ceph subvolume missing on mon2.sha.me.corp', 'See /srv/salt/ceph/subvolume/README.md']</p> <p><a href=[link](https://github.com/SUSE/DeepSea/issues/1593)>SOLUTION</a> Edit /srv/pillar/ceph/stack/global.yml and add the following line: subvolume_init: disabled</p> <p>Then refresh the Salt pillar and re-run DeepSea stage.3: admin:~ # salt '*' saltutil.refresh_pillar admin:~ # salt-run state.orch ceph.stage.3</p> <p>After DeepSea successfully finished stage.3, the Ceph Dashboard will be running. Refer to Book “Administration Guide”, Chapter 20 “Ceph Dashboard” for a detailed overview of Ceph Dashboard features.</p> <p>To list nodes running dashboard, run: admin:~ # ceph mgr services | grep dashboard</p> <p>To list admin credentials, run: admin:~ # salt-call grains.get dashboard_creds</p> <p>[ERROR] module function <code>cephprocesses.wait</code> executed on nodes <code>mon1~3</code> and <code>data1~4</code> in Stage 0</p> <p><a href=[link](https://github.com/SUSE/DeepSea/issues/1593)>SOLUTION</a> <div class=highlight><pre><span></span><code>Check below on all nodes
# salt-call cephprocesses.check
    ERROR: process ceph-mds for role mds is not running
    ERROR: process radosgw for role rgw is not running

admin:~ # ceph -s
    Clock skew detected on mon ceph (mon.mon2, mon.mon3)
Set time server to public server (China)
# chronyc sources
</code></pre></div></p> <h3 id=110-shutting-down-the-whole-ceph-cluster>1.10. Shutting Down the Whole Ceph Cluster<a class=headerlink href=#110-shutting-down-the-whole-ceph-cluster title="Permanent link"> ¶</a></h3> <p>Shut down or disconnect any clients accessing the cluster.</p> <p>To prevent CRUSH from automatically rebalancing the cluster, set the cluster to noout: <div class=highlight><pre><span></span><code>    # ceph osd set noout
        Other flags you can set per osd:
            nodown
            noup
            noin
            noout
</code></pre></div></p> <p>Disable safety measures and run the ceph.shutdown runner: <div class=highlight><pre><span></span><code>    admin:~ # salt-run disengage.safety
        safety is now disabled for cluster ceph

    admin:~ # salt-run state.orch ceph.shutdown
        admin.sha.me.corp_master:
          Name: set noout - Function: salt.state - Result: Changed Started: - 14:32:14.398022 Duration: 2266.75 ms
          Name: Shutting down radosgw for rgw - Function: salt.state - Result: Changed Started: - 14:32:16.665452 Duration: 1461.23 ms
          Name: Shutting down cephfs - Function: salt.state - Result: Changed Started: - 14:32:18.127353 Duration: 30326.193 ms
          Name: Shutting down iscsi - Function: salt.state - Result: Clean Started: - 14:32:48.454187 Duration: 30142.468 ms
          Name: Shutting down storage - Function: salt.state - Result: Changed Started: - 14:33:18.597321 Duration: 10841.45 ms
          Name: Shutting down mgr - Function: salt.state - Result: Changed Started: - 14:33:29.439442 Duration: 29209.141 ms
          Name: Shutting down mon - Function: salt.state - Result: Changed Started: - 14:33:58.649221 Duration: 30519.97 ms

        Summary for admin.sha.me.corp_master
        ------------
        Succeeded: 7 (changed=6)
        Failed:    0
        ------------
        Total states run:     7
        Total run time: 134.767 s
</code></pre></div></p> <p>Power off all cluster nodes: <div class=highlight><pre><span></span><code>    admin:~ # salt -C &#39;G@deepsea:*&#39; cmd.run &quot;shutdown -h&quot;
        Broadcast message from root@admin (Sat 2021-03-06 14:40:37 CST):
        The system is going down for poweroff at Sat 2021-03-06 14:41:37 CST!
        admin.sha.me.corp:
            Shutdown scheduled for Sat 2021-03-06 14:41:37 CST, use &#39;shutdown -c&#39; to cancel.
        mon2.sha.me.corp:
            Shutdown scheduled for Sat 2021-03-06 14:41:37 CST, use &#39;shutdown -c&#39; to cancel.
        data2.sha.me.corp:
            Shutdown scheduled for Sat 2021-03-06 14:41:37 CST, use &#39;shutdown -c&#39; to cancel.
        mon3.sha.me.corp:
            Shutdown scheduled for Sat 2021-03-06 14:41:37 CST, use &#39;shutdown -c&#39; to cancel.
        data3.sha.me.corp:
            Shutdown scheduled for Sat 2021-03-06 14:41:37 CST, use &#39;shutdown -c&#39; to cancel.
        data4.sha.me.corp:
            Shutdown scheduled for Sat 2021-03-06 14:41:37 CST, use &#39;shutdown -c&#39; to cancel.
        mon1.sha.me.corp:
            Shutdown scheduled for Sat 2021-03-06 14:41:37 CST, use &#39;shutdown -c&#39; to cancel.
        data1.sha.me.corp:
            Shutdown scheduled for Sat 2021-03-06 14:41:37 CST, use &#39;shutdown -c&#39; to cancel.
</code></pre></div></p> <h3 id=111-starting-stopping-and-restarting-services-using-targets>1.11. Starting, Stopping, and Restarting Services Using Targets<a class=headerlink href=#111-starting-stopping-and-restarting-services-using-targets title="Permanent link"> ¶</a></h3> <div class=highlight><pre><span></span><code># ls /usr/lib/systemd/system/ceph*.target
ceph.target
ceph-osd.target
ceph-mon.target
ceph-mgr.target
ceph-mds.target
ceph-radosgw.target
ceph-rbd-mirror.target
</code></pre></div> <p>To start/stop/restart all Ceph services on the node, run: <div class=highlight><pre><span></span><code># systemctl start ceph.target
# systemctl stop ceph.target
# systemctl restart ceph.target
</code></pre></div></p> <p>To start/stop/restart all OSDs on the node, run: <div class=highlight><pre><span></span><code># systemctl start ceph-osd.target
# systemctl stop ceph-osd.target
# systemctl restart ceph-osd.target
</code></pre></div></p> <p>Starting, Stopping, and Restarting Individual Services <div class=highlight><pre><span></span><code># systemctl list-unit-files --all --type=service ceph*
    ceph-osd@.service
    ceph-mon@.service
    ceph-mds@.service
    ceph-mgr@.service
    ceph-radosgw@.service
    ceph-rbd-mirror@.service
</code></pre></div></p> <p>Example : <div class=highlight><pre><span></span><code># systemctl status ceph-mon@HOSTNAME.service (e.g., ceph-mon@mon1.service)

# systemctl start ceph-osd@1.service
# systemctl stop ceph-osd@1.service
# systemctl restart ceph-osd@1.service
# systemctl status ceph-osd@1.service
</code></pre></div></p> <h3 id=112-restarting-all-services>1.12. Restarting All Services<a class=headerlink href=#112-restarting-all-services title="Permanent link"> ¶</a></h3> <div class=highlight><pre><span></span><code># salt-run state.orch ceph.restart
</code></pre></div> <h3 id=113-restarting-specific-services>1.13. Restarting Specific Services<a class=headerlink href=#113-restarting-specific-services title="Permanent link"> ¶</a></h3> <p>Example: salt-run state.orch ceph.restart.service_name <div class=highlight><pre><span></span><code># salt-run state.orch ceph.restart.mon
# salt-run state.orch ceph.restart.mgr
# salt-run state.orch ceph.restart.osd
# salt-run state.orch ceph.restart.mds
# salt-run state.orch ceph.restart.rgw
# salt-run state.orch ceph.restart.igw
# salt-run state.orch ceph.restart.ganesha
</code></pre></div></p> <p>Default log file path of salt-run: <code>/var/log/salt/master</code></p> <h2 id=2-basic-operation>2. Basic Operation<a class=headerlink href=#2-basic-operation title="Permanent link"> ¶</a></h2> <h3 id=21-pools-and-data-placement>2.1. Pools and Data Placement<a class=headerlink href=#21-pools-and-data-placement title="Permanent link"> ¶</a></h3> <h4 id=211-enable-the-pg-autoscaler-and-balancer-modules>2.1.1. Enable the PG Autoscaler and Balancer Modules<a class=headerlink href=#211-enable-the-pg-autoscaler-and-balancer-modules title="Permanent link"> ¶</a></h4> <h5 id=task-1-view-the-state-of-all-the-manager-modules>Task 1: View the state of all the Manager Modules<a class=headerlink href=#task-1-view-the-state-of-all-the-manager-modules title="Permanent link"> ¶</a></h5> <p>List all the existing Manager Modules <div class=highlight><pre><span></span><code>admin:~ # ceph mgr module ls | less
</code></pre></div></p> <h5 id=task-2-list-the-existing-pools>Task 2: List the Existing Pools<a class=headerlink href=#task-2-list-the-existing-pools title="Permanent link"> ¶</a></h5> <p>List the pools that already exist in the cluster <div class=highlight><pre><span></span><code>admin:~ # ceph osd lspools
1 iscsi-images
2 cephfs_data
3 cephfs_metadata
4 .rgw.root
5 default.rgw.control
6 default.rgw.meta
7 default.rgw.log
</code></pre></div></p> <p>List the pools again, but this time using the rados command: <div class=highlight><pre><span></span><code>admin:~ # rados lspools
iscsi-images
cephfs_data
cephfs_metadata
.rgw.root
default.rgw.control
default.rgw.meta
default.rgw.log
</code></pre></div></p> <p>View the output of placement group autoscale-status command for the pools <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool autoscale-status
Error ENOTSUP: Module &#39;pg_autoscaler&#39; is not enabled (required by command &#39;osd pool autoscale-status&#39;): use `ceph mgr module enable pg_autoscaler` to enable it
</code></pre></div></p> <h5 id=task-3-enable-the-pg_autoscaler-module>Task 3: Enable the pg_autoscaler module<a class=headerlink href=#task-3-enable-the-pg_autoscaler-module title="Permanent link"> ¶</a></h5> <p>Enable the pg_autoscaler module <div class=highlight><pre><span></span><code>admin:~ # ceph mgr module enable pg_autoscaler

admin:~ # ceph osd pool autoscale-status
POOL                  SIZE TARGET SIZE RATE RAW CAPACITY  RATIO TARGET RATIO EFFECTIVE RATIO BIAS PG_NUM NEW PG_NUM AUTOSCALE
iscsi-images          389               3.0       98256M 0.0000                               1.0    128         32 warn
cephfs_data             0               3.0       98256M 0.0000                               1.0    256         32 warn
cephfs_metadata      7285               3.0       98256M 0.0000                               4.0     64         16 warn
.rgw.root            1245               3.0       98256M 0.0000                               1.0     32            warn
default.rgw.control     0               3.0       98256M 0.0000                               1.0     32            warn
default.rgw.meta      381               3.0       98256M 0.0000                               1.0     32            warn
default.rgw.log     18078               3.0       98256M 0.0000                               1.0     32            warn
</code></pre></div></p> <p>Note that for the iscsi-images pool the PG_NUM value is 128. And note that the NEW PG_NUM value is 32. </p> <p>The PGs won’t be adjusted automatically because the default setting for the autoscaler is “warn”. </p> <p>Note the last column (mode) that shows status “warn” for all the pools. </p> <p>Check current status. “have too many placement groups”. That’s exactly what we want the pg_autoscaler to tell us. <div class=highlight><pre><span></span><code>admin:~ # ceph health
HEALTH_WARN 3 pools have too many placement groups
</code></pre></div></p> <p>Turn off the pg_autoscaler feature for CephFS pools <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool set cephfs_data pg_autoscale_mode off
set pool 2 pg_autoscale_mode to off

admin:~ # ceph osd pool set cephfs_metadata pg_autoscale_mode off
set pool 3 pg_autoscale_mode to off

admin:~ # ceph health
HEALTH_WARN 1 pools have too many placement groups
</code></pre></div></p> <p>Set the pg_autoscaler mode to “on” for the iscs-images pool: <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool set iscsi-images pg_autoscale_mode on
set pool 1 pg_autoscale_mode to on

admin:~ # ceph osd pool autoscale-status
POOL                  SIZE TARGET SIZE RATE RAW CAPACITY  RATIO TARGET RATIO EFFECTIVE RATIO BIAS PG_NUM NEW PG_NUM AUTOSCALE
iscsi-images          389               3.0       98256M 0.0000                               1.0    128         32 on
cephfs_data             0               3.0       98256M 0.0000                               1.0    256         32 off
cephfs_metadata      7412               3.0       98256M 0.0000                               4.0     64         16 off
.rgw.root            1245               3.0       98256M 0.0000                               1.0     32            warn
default.rgw.control     0               3.0       98256M 0.0000                               1.0     32            warn
default.rgw.meta      381               3.0       98256M 0.0000                               1.0     32            warn
default.rgw.log     18078               3.0       98256M 0.0000                               1.0     32            warn
</code></pre></div></p> <p>Turn on the pg_autoscaler feature for CephFS pools <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool set cephfs_data pg_autoscale_mode on
set pool 2 pg_autoscale_mode to on

admin:~ # ceph osd pool set cephfs_metadata pg_autoscale_mode on
set pool 3 pg_autoscale_mode to on
</code></pre></div></p> <p>PG numbgrs must always be a power of 2 <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool autoscale-status
POOL                  SIZE TARGET SIZE RATE RAW CAPACITY  RATIO TARGET RATIO EFFECTIVE RATIO BIAS PG_NUM NEW PG_NUM AUTOSCALE
iscsi-images          389               3.0       98256M 0.0000                               1.0     32            on
cephfs_data             0               3.0       98256M 0.0000                               1.0     32            off
cephfs_metadata      7412               3.0       98256M 0.0000                               4.0     16            off
.rgw.root            1245               3.0       98256M 0.0000                               1.0     32            warn
default.rgw.control     0               3.0       98256M 0.0000                               1.0     32            warn
default.rgw.meta      381               3.0       98256M 0.0000                               1.0     32            warn
default.rgw.log     35900               3.0       98256M 0.0000                               1.0     32            warn
</code></pre></div></p> <p>Show the cluster health <div class=highlight><pre><span></span><code>admin:~ # ceph -s
  cluster:
    id:     &lt;id&gt;
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum mon1,mon2,mon3 (age 4w)
    mgr: mon1(active, since 46h)
    mds: cephfs:1 {0=mon3=up:active} 2 up:standby
    osd: 12 osds: 12 up (since 8w), 12 in (since 8w)
    rgw: 1 daemon active (mon3)

  task status:
    scrub status:
        mds.mon3: idle

  data:
    pools:   7 pools, 433 pgs
    objects: 246 objects, 4.7 KiB
    usage:   13 GiB used, 83 GiB / 96 GiB avail
    pgs:     0.462% pgs not active
             431 active+clean
             2   peering

  io:
    client:   45 KiB/s rd, 0 B/s wr, 44 op/s rd, 28 op/s wr
</code></pre></div></p> <h5 id=task-4-turn-on-the-placement-group-balancer-feature>Task 4: Turn on the Placement Group balancer feature<a class=headerlink href=#task-4-turn-on-the-placement-group-balancer-feature title="Permanent link"> ¶</a></h5> <p>1). Show the “status” of the balancer: <div class=highlight><pre><span></span><code>admin:~ # ceph balancer status
{
    &quot;plans&quot;: [],
    &quot;active&quot;: false,
    &quot;last_optimize_started&quot;: &quot;&quot;,
    &quot;last_optimize_duration&quot;: &quot;&quot;,
    &quot;optimize_result&quot;: &quot;&quot;,
    &quot;mode&quot;: &quot;none&quot;
}

admin:~ # ceph balancer on

admin:~ # ceph balancer status
{
    &quot;plans&quot;: [],
    &quot;active&quot;: true,
    &quot;last_optimize_started&quot;: &quot;Mon Jan  4 20:22:57 2021&quot;,
    &quot;last_optimize_duration&quot;: &quot;0:00:00.001379&quot;,
    &quot;optimize_result&quot;: &quot;Please do \&quot;ceph balancer mode\&quot; to choose a valid mode first&quot;,
    &quot;mode&quot;: &quot;none&quot;
}
</code></pre></div></p> <p>2). Set the mode for the balancer to “upmap”: <div class=highlight><pre><span></span><code>admin:~ # ceph balancer mode upmap
Error EPERM: min_compat_client &quot;jewel&quot; &lt; &quot;luminous&quot;, which is required for pg-upmap. 
Try &quot;ceph osd set-require-min-compat-client luminous&quot; before enabling this mode

admin:~ # ceph osd set-require-min-compat-client luminous --yes-i-really-mean-it
set require_min_compat_client to luminous

admin:~ # ceph balancer mode upmap

admin:~ # ceph balancer status
{
    &quot;plans&quot;: [],
    &quot;active&quot;: true,
    &quot;last_optimize_started&quot;: &quot;Mon Jan  4 20:23:57 2021&quot;,
    &quot;last_optimize_duration&quot;: &quot;0:00:00.001807&quot;,
    &quot;optimize_result&quot;: &quot;Please do \&quot;ceph balancer mode\&quot; to choose a valid mode first&quot;,
    &quot;mode&quot;: &quot;upmap&quot;
}
</code></pre></div></p> <p>3). Create a balancer optimization plan called basic-plan. Ceph won’t let you do this yet. Because you just recently enabled the pg_autoscaler, Ceph is moving objects around, and the PGs are quite busy with re-peering. <div class=highlight><pre><span></span><code>admin:~ # ceph balancer optimize basic-plan
Error EINVAL: Balancer enabled, disable to optimize manually
</code></pre></div></p> <p>4). Show the details of the plan: This shows what “execute”-ing the plan will do, itemizing which PGs will be affected. <div class=highlight><pre><span></span><code>admin:~ # ceph balancer show basic-plan
Error ENOENT: plan basic-plan not found   &lt;--- failed here
</code></pre></div></p> <p>5). Show the effectiveness of the plan by comparing the current score for the pre-planned balancing and the score for the planned balancing: <div class=highlight><pre><span></span><code>admin:~ # ceph balancer eval
current cluster score 0.118731 (lower is better)

admin:~ # ceph balancer eval basic-plan
Error EINVAL: option &quot;basic-plan&quot; not a plan or a pool
</code></pre></div></p> <p>6). Show the status of the balancer, now with all of these settings having been set, but before putting them into effect:</p> <p>The pg_autoscaler has already optimized the balance of PGs sufficiently. That’s because this cluster is very small and has no significant content stored in it yet. </p> <p>If that’s the case, you would see a message like “Error EALREADY: Unable to find further optimization, or pool(s)' pg_num is decreasing, or distribution is already perfect.” If you receive this message, then you will not be able to complete this task. At some later time in the course you may choose to revisit this task to complete it. <div class=highlight><pre><span></span><code>admin:~ # ceph balancer status
{
    &quot;plans&quot;: [],
    &quot;active&quot;: true,
    &quot;last_optimize_started&quot;: &quot;Mon Jan  4 20:32:59 2021&quot;,
    &quot;last_optimize_duration&quot;: &quot;0:00:00.004170&quot;,
    &quot;optimize_result&quot;: &quot;Unable to find further optimization, or pool(s) pg_num is decreasing, or distribution is already perfect&quot;,
    &quot;mode&quot;: &quot;upmap&quot;
}
</code></pre></div></p> <p>7). Set the basic-plan into effect: <div class=highlight><pre><span></span><code>admin:~ # ceph balancer execute basic-plan
Error EINVAL: Balancer enabled, disable to execute a plan
</code></pre></div></p> <p>8). Now re-show the current score for the balanced cluster: <div class=highlight><pre><span></span><code>admin:~ # ceph balancer eval
current cluster score 0.118731 (lower is better)
</code></pre></div></p> <h4 id=212-manipulate-erasure-code-profiles>2.1.2. Manipulate Erasure Code Profiles<a class=headerlink href=#212-manipulate-erasure-code-profiles title="Permanent link"> ¶</a></h4> <h5 id=task-1-display-a-list-of-the-current-erasure-code-profiles>Task 1: Display a list of the current Erasure Code profiles<a class=headerlink href=#task-1-display-a-list-of-the-current-erasure-code-profiles title="Permanent link"> ¶</a></h5> <div class=highlight><pre><span></span><code>admin:~ # ceph osd erasure-code-profile
no valid command found; 4 closest matches:
osd erasure-code-profile set &lt;name&gt; {&lt;profile&gt; [&lt;profile&gt;...]} {--force}
osd erasure-code-profile get &lt;name&gt;
osd erasure-code-profile rm &lt;name&gt;
osd erasure-code-profile ls
Error EINVAL: invalid command

admin:~ # ceph osd erasure-code-profile ls
default
</code></pre></div> <h5 id=task-2-examine-the-details-of-the-default-ec-profile>Task 2: Examine the details of the default EC profile<a class=headerlink href=#task-2-examine-the-details-of-the-default-ec-profile title="Permanent link"> ¶</a></h5> <div class=highlight><pre><span></span><code>admin:~ # ceph osd erasure-code-profile get default
k=2
m=1
plugin=jerasure
technique=reed_sol_van
</code></pre></div> <h5 id=task-3-create-and-remove-a-new-ec-profile>Task 3: Create and remove a new EC profile<a class=headerlink href=#task-3-create-and-remove-a-new-ec-profile title="Permanent link"> ¶</a></h5> <div class=highlight><pre><span></span><code>1. Create a new EC profile from the command line. 
This is going to be a “bad” profile that will be removed in a moment:
admin:~ # ceph osd erasure-code-profile set bad_profile k=2 m=4 plugin=jerasure

admin:~ # ceph osd erasure-code-profile ls
bad_profile
default

admin:~ # ceph osd erasure-code-profile get bad_profile
crush-device-class=
crush-failure-domain=host
crush-root=default
jerasure-per-chunk-alignment=false
k=2
m=4
plugin=jerasure
technique=reed_sol_van
w=8

admin:~ # ceph osd erasure-code-profile rm bad_profile

admin:~ # ceph osd erasure-code-profile ls
default
</code></pre></div> <h5 id=task-4-create-a-better-ec-profile>Task 4: Create a better EC profile<a class=headerlink href=#task-4-create-a-better-ec-profile title="Permanent link"> ¶</a></h5> <div class=highlight><pre><span></span><code>admin:~ # ceph osd erasure-code-profile set usable_profile k=2 m=1 plugin=jerasure technique=reed_sol_van stripe_unit=4K crush-failure-domain=host

admin:~ # ceph osd erasure-code-profile get usable_profile
crush-device-class=
crush-failure-domain=host
crush-root=default
jerasure-per-chunk-alignment=false
k=2
m=1
plugin=jerasure
stripe_unit=4K
technique=reed_sol_van
w=8
</code></pre></div> <h4 id=213-manipulate-crush-map-rulesets>2.1.3. Manipulate CRUSH Map Rulesets<a class=headerlink href=#213-manipulate-crush-map-rulesets title="Permanent link"> ¶</a></h4> <h5 id=task-1-display-a-list-of-the-current-crush-map-rules>Task 1: Display a list of the current CRUSH Map rules<a class=headerlink href=#task-1-display-a-list-of-the-current-crush-map-rules title="Permanent link"> ¶</a></h5> <div class=highlight><pre><span></span><code>admin:~ # ceph osd crush rule ls
replicated_rule

admin:~ # ceph osd crush
                osd crush rule ls
                osd crush rule ls-by-class &lt;class&gt;
                osd crush rule dump {&lt;name&gt;}
                osd crush dump
                osd crush set {&lt;int&gt;}
                osd crush add-bucket &lt;name&gt; &lt;type&gt; {&lt;args&gt; [&lt;args&gt;...]}
                osd crush rename-bucket &lt;srcname&gt; &lt;dstname&gt;
                osd crush set &lt;osdname (id|osd.id)&gt; &lt;float[0.0-]&gt; &lt;args&gt; [&lt;args&gt;...]
                osd crush add &lt;osdname (id|osd.id)&gt; &lt;float[0.0-]&gt; &lt;args&gt; [&lt;args&gt;...]
                osd crush set-all-straw-buckets-to-straw2

admin:~ # ceph osd crush rule
                osd crush rule ls
                osd crush rule ls-by-class &lt;class&gt;
                osd crush rule dump {&lt;name&gt;}
                osd crush rule create-simple &lt;name&gt; &lt;root&gt; &lt;type&gt; {firstn|indep}
                osd crush rule create-replicated &lt;name&gt; &lt;root&gt; &lt;type&gt; {&lt;class&gt;}
                osd crush rule create-erasure &lt;name&gt; {&lt;profile&gt;}
                osd crush rule rm &lt;name&gt;
                osd crush rule rename &lt;srcname&gt; &lt;dstname&gt;
</code></pre></div> <p>List the existing CRUSH Map rulesets that have been defined according to a particular device class: <div class=highlight><pre><span></span><code>admin:~ # ceph osd crush rule ls-by-class hdd

admin:~ # ceph osd crush rule ls-by-class ssd
Error ENOENT: failed to get rules by class &#39;ssd&#39;

admin:~ # ceph osd crush rule ls-by-class nvme
Error ENOENT: failed to get rules by class &#39;nvme&#39;
</code></pre></div></p> <h5 id=task-2-examine-the-details-of-the-default-crush-map-rule>Task 2: Examine the details of the default CRUSH Map rule<a class=headerlink href=#task-2-examine-the-details-of-the-default-crush-map-rule title="Permanent link"> ¶</a></h5> <ol> <li>Show the details of the default CRUSH Map rule with the dump sub-command:</li> </ol> <p>The “rule_id” and “ruleset” values just numbgrs to keep track of rules similar to a DB key id. “min_size” and “max_size” are related to how CRUSH behaves when a certain numbgr of replicas are created.</p> <p>The “steps” section is the most functional portion of the rule, providing an ordered set of rules for how CRUSH should behave. </p> <p>Note that there are three “op” parts, one each for “take”, “chooseleaf_firstn”, and “emit”. </p> <p>“take” in a replicated rule is always the first step, and “emit” is always the last step. </p> <p>The “item_type” in the “take” step is the crush_root value, and the “host” in the “chooseleaf_firstn” step is the failure_domain. </p> <div class=highlight><pre><span></span><code>admin:~ # ceph osd crush rule dump replicated_rule
{
    &quot;rule_id&quot;: 0,
    &quot;rule_name&quot;: &quot;replicated_rule&quot;,
    &quot;ruleset&quot;: 0,
    &quot;type&quot;: 1,
    &quot;min_size&quot;: 1,
    &quot;max_size&quot;: 10,
    &quot;steps&quot;: [
        {
            &quot;op&quot;: &quot;take&quot;,
            &quot;item&quot;: -1,
            &quot;item_name&quot;: &quot;default&quot;
        },
        {
            &quot;op&quot;: &quot;chooseleaf_firstn&quot;,
            &quot;num&quot;: 0,
            &quot;type&quot;: &quot;host&quot;
        },
        {
            &quot;op&quot;: &quot;emit&quot;
        }
    ]
}
</code></pre></div> <h5 id=task-3-create-and-remove-a-new-crush-map-rule>Task 3: Create and remove a new CRUSH Map rule<a class=headerlink href=#task-3-create-and-remove-a-new-crush-map-rule title="Permanent link"> ¶</a></h5> <p>1). Create a new CRUSH ruleset from the command line.We made two mistakes here: First, we named it “bud” instead of “bad”. <div class=highlight><pre><span></span><code>admin:~ # ceph osd crush rule create-replicated bud_ruleset default host

admin:~ # ceph osd crush rule ls
replicated_rule
bud_ruleset
</code></pre></div></p> <p>2). Rename the ruleset: <div class=highlight><pre><span></span><code>admin:~ # ceph osd crush rule rename bud_ruleset bad_ruleset

admin:~ # ceph osd crush rule ls
replicated_rule
bad_ruleset
</code></pre></div></p> <p>3). The second mistake was that we specified the failure-domain at the host-bucket level.</p> <p>This is technically not a bad thing to do, in fact it would be a common use case. </p> <p>But for this demo we want to set the failure domain at the rack-bucket level. We can’t change a defined CRUSH Map ruleset, so delete the bad one: <div class=highlight><pre><span></span><code>admin:~ # ceph osd crush rule rm bad_ruleset

admin:~ # ceph osd crush rule ls
replicated_rule
</code></pre></div></p> <h5 id=task-4-create-a-better-crush-map-rule>Task 4: Create a better CRUSH Map rule<a class=headerlink href=#task-4-create-a-better-crush-map-rule title="Permanent link"> ¶</a></h5> <p>Create a more appropriate CRUSH Map rule from the CLI, that will survive the failure of a rack: <div class=highlight><pre><span></span><code>admin:~ # ceph osd crush rule create-replicated better_ruleset default rack

admin:~ # ceph osd crush rule dump better_ruleset
{
    &quot;rule_id&quot;: 1,
    &quot;rule_name&quot;: &quot;better_ruleset&quot;,
    &quot;ruleset&quot;: 1,
    &quot;type&quot;: 1,
    &quot;min_size&quot;: 1,
    &quot;max_size&quot;: 10,
    &quot;steps&quot;: [
        {
            &quot;op&quot;: &quot;take&quot;,
            &quot;item&quot;: -1,
            &quot;item_name&quot;: &quot;default&quot;
        },
        {
            &quot;op&quot;: &quot;chooseleaf_firstn&quot;,
            &quot;num&quot;: 0,
            &quot;type&quot;: &quot;rack&quot;
        },
        {
            &quot;op&quot;: &quot;emit&quot;
        }
    ]
}
</code></pre></div></p> <h5 id=task-5-create-crush-map-rules-for-different-classes-of-devices>Task 5: Create CRUSH Map rules for different classes of devices<a class=headerlink href=#task-5-create-crush-map-rules-for-different-classes-of-devices title="Permanent link"> ¶</a></h5> <p>1). Create two different CRUSH Map rules from the CLI, that will accommodate a slow set of devices (HDDs) and a fast set of devices (SDDs):</p> <p>The error of 2<sup>nd</sup> is because the cluster does not have any SSD devices. </p> <div class=highlight><pre><span></span><code>admin:~ # ceph osd crush rule create-replicated slow_devices default host hdd

admin:~ # ceph osd crush rule create-replicated fast_devices default host sdd
Error EINVAL: device class sdd does not exist
</code></pre></div> <p>2). Display the details of the new “slow” rule: <div class=highlight><pre><span></span><code>admin:~ # ceph osd crush rule dump slow_devices
{
    &quot;rule_id&quot;: 2,
    &quot;rule_name&quot;: &quot;slow_devices&quot;,
    &quot;ruleset&quot;: 2,
    &quot;type&quot;: 1,
    &quot;min_size&quot;: 1,
    &quot;max_size&quot;: 10,
    &quot;steps&quot;: [
        {
            &quot;op&quot;: &quot;take&quot;,
            &quot;item&quot;: -2,
            &quot;item_name&quot;: &quot;default~hdd&quot;
        },
        {
            &quot;op&quot;: &quot;chooseleaf_firstn&quot;,
            &quot;num&quot;: 0,
            &quot;type&quot;: &quot;host&quot;
        },
        {
            &quot;op&quot;: &quot;emit&quot;
        }
    ]
}
</code></pre></div></p> <h5 id=task-6-change-the-ruleset-used-by-a-pool>Task 6: Change the ruleset used by a pool<a class=headerlink href=#task-6-change-the-ruleset-used-by-a-pool title="Permanent link"> ¶</a></h5> <p>1). Show which CRUSH Map Ruleset is being used by the cephfs_data pool: The rule should be listed as replicated_rule. <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool get cephfs_data crush_rule
crush_rule: replicated_rule
</code></pre></div></p> <p>2). Change the cephfs_data pool to use the new CRUSH Map ruleset that you created in the previous task. <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool set cephfs_data crush_rule better_ruleset
set pool 2 crush_rule to better_ruleset
</code></pre></div></p> <p>3). Verify that the rule has been changed by re-running the earlier command: <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool get cephfs_data crush_rule
crush_rule: better_ruleset
</code></pre></div></p> <p>4). In this demo cluster, making the cephfs_data pool use the “better_ruleset” will result in problems. (There’s no rack for the CRUSH Map, and not enough nodes to accommodate the requirement for a large numbgr of PGs.) </p> <p>So change the setting back to the replicated_rule.</p> <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool set cephfs_data crush_rule replicated_rule
set pool 2 crush_rule to replicated_rule

admin:~ # ceph osd pool get cephfs_data crush_rule
crush_rule: replicated_rule
</code></pre></div> <p>Task 7: Create a CRUSH Map rule enhanced with an EC profile</p> <p>1). Combine the benefits of Erasure Coding with a CRUSH Map rule: This will only work if you have already created an appropriate EC profile called usable_profile.</p> <p>In this demo you would have done in an earlier exercise.</p> <p>And in this demo you need to tie this ec_rule to the usable_profile, not the better_profile.Or else any pool that you create using the ec_rule will fail due to insufficient resources.</p> <div class=highlight><pre><span></span><code>admin:~ # ceph osd crush rule create-erasure ec_rule usable_profile  Link the CRUSH map rule (ec_rule) to EC profile (usable_profile)
created rule ec_rule at 3
</code></pre></div> <p>P.S., The useable_profile was created by : <div class=highlight><pre><span></span><code>admin:~ # ceph osd erasure-code-profile set usable_profile k=2 m=1 plugin=jerasure technique=reed_sol_van stripe_unit=4K crush-failure-domain=host
</code></pre></div></p> <p>2). Display the details of the EC-enhanced CRUSH Map rule:</p> <p>See the added, extra “op” steps. You might also notice the different values for “type,” “min_size,” and “max_size” than what you saw in the standard replicated rules. <div class=highlight><pre><span></span><code>admin:~ # ceph osd crush rule dump ec_rule
{
    &quot;rule_id&quot;: 3,
    &quot;rule_name&quot;: &quot;ec_rule&quot;,
    &quot;ruleset&quot;: 3,
    &quot;type&quot;: 3,
    &quot;min_size&quot;: 3,
    &quot;max_size&quot;: 3,
    &quot;steps&quot;: [
        {
            &quot;op&quot;: &quot;set_chooseleaf_tries&quot;,
            &quot;num&quot;: 5
        },
        {
            &quot;op&quot;: &quot;set_choose_tries&quot;,
            &quot;num&quot;: 100
        },
        {
            &quot;op&quot;: &quot;take&quot;,
            &quot;item&quot;: -1,
            &quot;item_name&quot;: &quot;default&quot;
        },
        {
            &quot;op&quot;: &quot;chooseleaf_indep&quot;,
            &quot;num&quot;: 0,
            &quot;type&quot;: &quot;host&quot;
        },
        {
            &quot;op&quot;: &quot;emit&quot;
        }
    ]
}



admin:~ # ceph osd crush rule ls
replicated_rule
better_ruleset
slow_devices
ec_rule

admin:~ # ceph osd crush rule create-replicated better_ruleset default rack
admin:~ # ceph osd crush rule create-replicated slow_devices default host hdd
admin:~ # ceph osd crush rule create-erasure ec_rule usable_profile

admin:~ # ceph osd crush rule dump replicated_rule
{
    &quot;rule_id&quot;: 0,
    &quot;rule_name&quot;: &quot;replicated_rule&quot;,
    &quot;ruleset&quot;: 0,
    &quot;type&quot;: 1,
    &quot;min_size&quot;: 1,
    &quot;max_size&quot;: 10,
    &quot;steps&quot;: [
        {
            &quot;op&quot;: &quot;take&quot;,
            &quot;item&quot;: -1,
            &quot;item_name&quot;: &quot;default&quot;
        },
        {
            &quot;op&quot;: &quot;chooseleaf_firstn&quot;,
            &quot;num&quot;: 0,
            &quot;type&quot;: &quot;host&quot;
        },
        {
            &quot;op&quot;: &quot;emit&quot;
        }
    ]
}

admin:~ # ceph osd crush rule dump better_ruleset
{
    &quot;rule_id&quot;: 1,
    &quot;rule_name&quot;: &quot;better_ruleset&quot;,
    &quot;ruleset&quot;: 1,
    &quot;type&quot;: 1,
    &quot;min_size&quot;: 1,
    &quot;max_size&quot;: 10,
    &quot;steps&quot;: [
        {
            &quot;op&quot;: &quot;take&quot;,
            &quot;item&quot;: -1,
            &quot;item_name&quot;: &quot;default&quot;
        },
        {
            &quot;op&quot;: &quot;chooseleaf_firstn&quot;,
            &quot;num&quot;: 0,
            &quot;type&quot;: &quot;rack&quot;
        },
        {
            &quot;op&quot;: &quot;emit&quot;
        }
    ]
}

admin:~ # ceph osd crush rule dump slow_devices
{
    &quot;rule_id&quot;: 2,
    &quot;rule_name&quot;: &quot;slow_devices&quot;,
    &quot;ruleset&quot;: 2,
    &quot;type&quot;: 1,
    &quot;min_size&quot;: 1,
    &quot;max_size&quot;: 10,
    &quot;steps&quot;: [
        {
            &quot;op&quot;: &quot;take&quot;,
            &quot;item&quot;: -2,
            &quot;item_name&quot;: &quot;default~hdd&quot;
        },
        {
            &quot;op&quot;: &quot;chooseleaf_firstn&quot;,
            &quot;num&quot;: 0,
            &quot;type&quot;: &quot;host&quot;
        },
        {
            &quot;op&quot;: &quot;emit&quot;
        }
    ]
}


admin:~ # ceph osd pool
                osd pool stats {&lt;poolname&gt;}
                osd pool scrub &lt;poolname&gt; [&lt;poolname&gt;...]
                osd pool deep-scrub &lt;poolname&gt; [&lt;poolname&gt;...]
                osd pool repair &lt;poolname&gt; [&lt;poolname&gt;...]
                osd pool force-recovery &lt;poolname&gt; [&lt;poolname&gt;...]
                osd pool force-backfill &lt;poolname&gt; [&lt;poolname&gt;...]
                osd pool cancel-force-recovery &lt;poolname&gt; [&lt;poolname&gt;...]
                osd pool cancel-force-backfill &lt;poolname&gt; [&lt;poolname&gt;...]
                osd pool autoscale-status
                osd pool mksnap &lt;poolname&gt; &lt;snap&gt;

admin:~ # ceph osd pool get &lt;poolname&gt; size
                                        min_size
                                        pg_num
                                        pgp_num
                                        crush_rule
                                        Hashpspool
                                        Nodelete
                                        Nopgchange
                                        Nosizechange
                                        write_fadvise_dontneed
                                        Noscrub
                                        nodeep-scrub
                                        hit_set_type
                                        hit_set_period
                                        hit_set_count
                                        hit_set_fpp
                                        use_gmt_hitset
                                        target_max_objects
                                        target_max_bytes
                                        cache_target_dirty_ratio
                                        cache_target_dirty_high_ratio
                                        cache_target_full_ratio
                                        cache_min_flush_age
                                        cache_min_evict_age
                                        erasure_code_profile
                                        min_read_recency_for_promote
                                        All
                                        min_write_recency_for_promote
                                        fast_read
                                        hit_set_grade_decay_rate
                                        hit_set_search_last_n
                                        scrub_min_interval
                                        scrub_max_interval
                                        deep_scrub_interval
                                        recovery_priority
                                        recovery_op_priority
                                        scrub_priority
                                        compression_mode
                                        compression_algorithm
                                        compression_required_ratio
                                        compression_max_blob_size
                                        compression_min_blob_size
                                        csum_type
                                        csum_min_block
                                        csum_max_block
                                        allow_ec_overwrites
                                        fingerprint_algorithm
                                        pg_autoscale_mode
                                        pg_autoscale_bias
                                        pg_num_min
                                        target_size_bytes
                                        target_size_ratio
</code></pre></div></p> <h4 id=214-investigate-bluestore>2.1.4. Investigate BlueStore<a class=headerlink href=#214-investigate-bluestore title="Permanent link"> ¶</a></h4> <h5 id=task-1-explore-the-drive_groupsyml-configuration>Task 1: Explore the drive_groups.yml configuration<a class=headerlink href=#task-1-explore-the-drive_groupsyml-configuration title="Permanent link"> ¶</a></h5> <p>After deployment, the drive_groups.yml file is where the storage administrator defines the configuration of the cluster’s storage devices.</p> <p>Note the “data_devices” parameter. In this demo, “all” storage devices are data devices for BlueStore.</p> <p>Note that there are no definitions for “wal_devices” or “db_devices.” That’s because in this demo environment we don’t have any other “fast” devices that would be appropriate for these roles.</p> <p>Since BlueStore is the default, there is no definition of a “format” for the devices. Otherwise, a “Format: bluestore” key-value pair might exist to ensure that BlueStore is used. <div class=highlight><pre><span></span><code>admin:~ # cd /srv/salt/ceph/configuration/files

admin:/srv/salt/ceph/configuration/files # cat drive_groups.yml
# default:  &lt;- just a name - can be anything
#   target: &#39;data*&#39; &lt;- must be resolvable by salt&#39;s targeting processor
#   data_devices:
#     size: 20G
#   db_devices:
#     size: 10G
#     rotational: 1
# allflash:
#   target: &#39;fast_nodes*&#39;
#   data_devices:
#     size: 100G
#   db_devices:
#     size: 50G
#     rotational: 0
# This is the default configuration and
# will create an OSD on all available drives
default:
  target: &#39;data*&#39;
  data_devices:
    all: true
</code></pre></div></p> <h5 id=task-2-examine-a-storage-hosts-storage-devices>Task 2: Examine a storage host’s storage devices<a class=headerlink href=#task-2-examine-a-storage-hosts-storage-devices title="Permanent link"> ¶</a></h5> <div class=highlight><pre><span></span><code>admin:~ # ssh data1
Last login: Tue Jan  5 18:06:40 2021 from 10.58.121.181
</code></pre></div> <p>Should see 3 devices, which are named ceph LVM-type devices <div class=highlight><pre><span></span><code>data1:~ # lsblk
NAME                                                                                                 MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                                                                                                    8:0    0    8G  0 disk
└─ceph--14c886af--269d--475f--8ee3--f5e4abbb222d-osd--data--38911b2d--f30a--4b09--9010--8dd6fad2fcc6 254:0    0    8G  0 lvm
sdb                                                                                                    8:16   0    8G  0 disk
└─ceph--9ec4a77a--5d67--4b21--be53--d7e9221082de-osd--data--00cb3dc6--c28b--41ae--95de--efb86da254da 254:1    0    8G  0 lvm
sdc                                                                                                    8:32   0    8G  0 disk
└─ceph--5eaea8a8--bb68--49dd--a1e3--b82c5464ab1f-osd--data--a4a05f70--53d9--41d4--a273--4f47a088968a 254:2    0    8G  0 lvm
sr0                                                                                                   11:0    1  672M  0 rom
vda                                                                                                  253:0    0   20G  0 disk
├─vda1                                                                                               253:1    0    8M  0 part
├─vda2                                                                                               253:2    0 18.4G  0 part /
└─vda3                                                                                               253:3    0  1.7G  0 part [SWAP]
</code></pre></div></p> <p>See the raw ceph devices <div class=highlight><pre><span></span><code>data1:~ # ls -lad /dev/ceph*
drwxr-xr-x 2 root root 60 Oct  5 13:15 /dev/ceph-14c886af-269d-475f-8ee3-f5e4abbb222d
drwxr-xr-x 2 root root 60 Oct  5 13:16 /dev/ceph-5eaea8a8-bb68-49dd-a1e3-b82c5464ab1f
drwxr-xr-x 2 root root 60 Oct  5 13:15 /dev/ceph-9ec4a77a-5d67-4b21-be53-d7e9221082de
</code></pre></div></p> <p>Dig down even farther by examining the content of one of the directories, see a symlink to an LVM device-mapper device.</p> <p>All the devices are tied together with LVM. Note that the name of the symlink is named osd-data-<uuid>. <div class=highlight><pre><span></span><code>data1:~ # ls -l /dev/ceph-14c886af-269d-475f-8ee3-f5e4abbb222d
lrwxrwxrwx 1 ceph ceph 7 Oct  5 13:15 osd-data-38911b2d-f30a-4b09-9010-8dd6fad2fcc6 -&gt; ../dm-0

data1:~ # l /dev/dm*
brw-rw---- 1 ceph ceph 254, 0 Jan  5 18:10 /dev/dm-0
brw-rw---- 1 ceph ceph 254, 1 Jan  5 18:10 /dev/dm-1
brw-rw---- 1 ceph ceph 254, 2 Jan  5 18:10 /dev/dm-2
</code></pre></div></p> <h5 id=task-3-examine-a-storage-hosts-osd-details>Task 3: Examine a storage host’s OSD details<a class=headerlink href=#task-3-examine-a-storage-hosts-osd-details title="Permanent link"> ¶</a></h5> <div class=highlight><pre><span></span><code>data1:~ # cd /var/lib/ceph/
data1:/var/lib/ceph # ls -l
drwxr-x--- 1 ceph ceph  0 Aug 24 22:03 bootstrap-mds
drwxr-x--- 1 ceph ceph  0 Aug 24 22:03 bootstrap-mgr
drwxr-x--- 1 ceph ceph 24 Oct  5 13:15 bootstrap-osd
drwxr-x--- 1 ceph ceph  0 Aug 24 22:03 bootstrap-rbd
drwxr-x--- 1 ceph ceph  0 Aug 24 22:03 bootstrap-rbd-mirror
drwxr-x--- 1 ceph ceph  0 Aug 24 22:03 bootstrap-rgw
drwxr-x--- 1 ceph ceph 12 Oct  5 09:04 crash
drwxr-x--- 1 ceph ceph  0 Aug 24 22:03 mds
drwxr-x--- 1 ceph ceph  0 Aug 24 22:03 mgr
drwxr-x--- 1 ceph ceph  0 Aug 24 22:03 mon
drwxr-x--- 1 ceph ceph 38 Oct  5 13:16 osd
drwxr-x--- 1 ceph ceph  0 Aug 24 22:03 tmp
</code></pre></div> <p>See 3 different sub-directories, each representing the 3 different OSDs (ceph-2, ceph-6, ceph-10) that are running on this storage server <div class=highlight><pre><span></span><code>data1:/var/lib/ceph # cd osd/

data1:/var/lib/ceph/osd # ls -l  
drwxrwxrwt 2 ceph ceph 320 Oct  5 13:16 ceph-10
drwxrwxrwt 2 ceph ceph 320 Oct  5 13:15 ceph-2
drwxrwxrwt 2 ceph ceph 320 Oct  5 13:16 ceph-6
</code></pre></div></p> <p>See some functional files associated with the OSD and BlueStore. See a block file, which is a symlink to one of the ceph devices, which stores the raw objects for the OSD. <div class=highlight><pre><span></span><code>data1:/var/lib/ceph/osd # cd ceph-2

data1:/var/lib/ceph/osd/ceph-2 # ls -l  
-rw-r--r-- 1 ceph ceph 400 Oct  5 13:15 activate.monmap
lrwxrwxrwx 1 ceph ceph  92 Oct  5 13:15 block -&gt; /dev/ceph-14c886af-269d-475f-8ee3-f5e4abbb222d/osd-data-38911b2d-f30a-4b09-9010-8dd6fad2fcc6
-rw------- 1 ceph ceph   2 Oct  5 13:15 bluefs
-rw------- 1 ceph ceph  37 Oct  5 13:15 ceph_fsid
-rw-r--r-- 1 ceph ceph  37 Oct  5 13:15 fsid
-rw------- 1 ceph ceph  55 Oct  5 13:15 keyring
-rw------- 1 ceph ceph   8 Oct  5 13:15 kv_backend
-rw------- 1 ceph ceph  21 Oct  5 13:15 magic
-rw------- 1 ceph ceph   4 Oct  5 13:15 mkfs_done
-rw------- 1 ceph ceph  41 Oct  5 13:15 osd_key
-rw------- 1 ceph ceph   6 Oct  5 13:15 ready
-rw------- 1 ceph ceph   3 Oct  5 13:15 require_osd_release
-rw------- 1 ceph ceph  10 Oct  5 13:15 type
-rw------- 1 ceph ceph   2 Oct  5 13:15 whoami


data1:/var/lib/ceph/osd/ceph-2 # cat ceph_fsid  # The unique ID of this Ceph cluster
343ee7d3-232f-4c71-8216-1edbc55ac6e0  

data1:/var/lib/ceph/osd/ceph-2 # cat fsid  # The unique ID of this OSD
6df58ebc-dbfe-4822-9714-90212c06ea05

data1:/var/lib/ceph/osd/ceph-2 # cat keyring  # The Ceph key for this OSD
[osd.2]
key = &lt;your key&gt;

data1:/var/lib/ceph/osd/ceph-2 # cat ready  # Indication of the readiness of this OSD
ready

data1:/var/lib/ceph/osd/ceph-2 # cat type  # filestore or bluestore (in this case: bluestore)
bluestore

data1:/var/lib/ceph/osd/ceph-2 # cat whoami  # The integer id of this OSD (in this case: 2)
2
</code></pre></div></p> <h5 id=task-4-display-bluestore-information-using-ceph-bluestore-tool>Task 4: Display BlueStore information using ceph-bluestore-tool<a class=headerlink href=#task-4-display-bluestore-information-using-ceph-bluestore-tool title="Permanent link"> ¶</a></h5> <p>Show BlueStore metadata for osd.2: <div class=highlight><pre><span></span><code>data1:/var/lib/ceph/osd/ceph-2 # ceph-bluestore-tool show-label --path /var/lib/ceph/osd/ceph-2
inferring bluefs devices from bluestore path
{
    &quot;/var/lib/ceph/osd/ceph-2/block&quot;: {
        &quot;osd_uuid&quot;: &quot;6df58ebc-dbfe-4822-9714-90212c06ea05&quot;,
        &quot;size&quot;: 8585740288,
        &quot;btime&quot;: &quot;2020-10-05 13:15:51.227799&quot;,
        &quot;description&quot;: &quot;main&quot;,
        &quot;bluefs&quot;: &quot;1&quot;,
        &quot;ceph_fsid&quot;: &quot;343ee7d3-232f-4c71-8216-1edbc55ac6e0&quot;,
        &quot;kv_backend&quot;: &quot;rocksdb&quot;,
        &quot;magic&quot;: &quot;ceph osd volume v026&quot;,
        &quot;mkfs_done&quot;: &quot;yes&quot;,
        &quot;osd_key&quot;: &lt;your key&gt;,
        &quot;ready&quot;: &quot;ready&quot;,
        &quot;require_osd_release&quot;: &quot;14&quot;,
        &quot;whoami&quot;: &quot;2&quot;
    }
</code></pre></div></p> <p>Run a manual “scrub” on osd.7 using ceph-blestore-tool. (Received error, the tool won’t allow you to do this while the OSD is running.) <div class=highlight><pre><span></span><code>data1:/var/lib/ceph/osd/ceph-2 # ceph-bluestore-tool fsck --path /var/lib/ceph/osd/ceph-2
error from fsck: (11) Resource temporarily unavailable
2021-01-05 18:32:25.528 7f4abad6e180 -1 bluestore(/var/lib/ceph/osd/ceph-2) _lock_fsid failed to lock /var/lib/ceph/osd/ceph-2/fsid (is another ceph-osd still running?)(11) Resource temporarily unavailable
</code></pre></div></p> <p>Simulate that the OSD is down, shutdown the OSD: <div class=highlight><pre><span></span><code>data1:/var/lib/ceph/osd/ceph-2 # systemctl stop ceph-osd@2.service
</code></pre></div></p> <p>Now run the “fsck” command again. This time the “fsck” has worked, with the output showing: “fsck success” <div class=highlight><pre><span></span><code>data1:/var/lib/ceph/osd/ceph-2 # ceph-bluestore-tool fsck --path /var/lib/ceph/osd/ceph-2
fsck success
</code></pre></div></p> <p>Restart the OSD: <div class=highlight><pre><span></span><code>data1:/var/lib/ceph/osd/ceph-2 # systemctl start ceph-osd@2.service

data1:/var/lib/ceph/osd/ceph-2 # ceph-bluestore-tool show-label --path /var/lib/ceph/osd/ceph-2
inferring bluefs devices from bluestore path
{
    &quot;/var/lib/ceph/osd/ceph-2/block&quot;: {
        &quot;osd_uuid&quot;: &quot;6df58ebc-dbfe-4822-9714-90212c06ea05&quot;,
        &quot;size&quot;: 8585740288,
        &quot;btime&quot;: &quot;2020-10-05 13:15:51.227799&quot;,
        &quot;description&quot;: &quot;main&quot;,
        &quot;bluefs&quot;: &quot;1&quot;,
        &quot;ceph_fsid&quot;: &quot;343ee7d3-232f-4c71-8216-1edbc55ac6e0&quot;,
        &quot;kv_backend&quot;: &quot;rocksdb&quot;,
        &quot;magic&quot;: &quot;ceph osd volume v026&quot;,
        &quot;mkfs_done&quot;: &quot;yes&quot;,
        &quot;osd_key&quot;: &lt;your key&gt;,
        &quot;ready&quot;: &quot;ready&quot;,
        &quot;require_osd_release&quot;: &quot;14&quot;,
        &quot;whoami&quot;: &quot;2&quot;
    }
}
</code></pre></div></p> <h3 id=22-common-day-1-tasks-using-the-cli>2.2. Common Day 1 Tasks Using the CLI<a class=headerlink href=#22-common-day-1-tasks-using-the-cli title="Permanent link"> ¶</a></h3> <p>Including ollowing topics in relation to the commandline:</p> <ul> <li>Users and Ceph Configuration</li> <li>Health commands</li> <li>Erasure Code Profiles</li> <li>CRUSH Map rules</li> <li>Pools</li> <li>Scrubbing OSDs and Placement Groups</li> <li>Manager modules</li> <li>The tell commands</li> </ul> <h4 id=221-ceph-users-and-configuration>2.2.1. Ceph Users and Configuration<a class=headerlink href=#221-ceph-users-and-configuration title="Permanent link"> ¶</a></h4> <h5 id=task-1-view-the-current-user-keyrings>Task 1: View the current user keyrings<a class=headerlink href=#task-1-view-the-current-user-keyrings title="Permanent link"> ¶</a></h5> <p>Ceph keyrings are stored in below directory <div class=highlight><pre><span></span><code>admin:~ # cd /etc/ceph/

admin:/etc/ceph # ls -l
-rw------- 1 root root 151 Oct  5 13:13 ceph.client.admin.keyring
-rw-r--r-- 1 root root 980 Oct  5 13:13 ceph.conf
-rw-r--r-- 1 root root  92 Aug 24 22:03 rbdmap
</code></pre></div></p> <p>The value of 'key' is the key that’s on the keyring. The admin keyring is “allow”ed all capabilities (permissions) to all services in the cluster, as expected. there are more than just client keys. <div class=highlight><pre><span></span><code>admin:/etc/ceph # cat ceph.client.admin.keyring
[client.admin]
        key = &lt;your key&gt;
        caps mds = &quot;allow *&quot;
        caps mon = &quot;allow *&quot;
        caps osd = &quot;allow *&quot;
        caps mgr = &quot;allow *&quot;
</code></pre></div></p> <p>Display the existing users with the “auth” command: Below two commands are equivalent <div class=highlight><pre><span></span><code>admin:/etc/ceph # ceph -n client.admin -keyring=/etc/ceph/ceph.client.admin.keyring auth ls  -- failed???
no valid command found

admin:/etc/ceph # ceph auth ls
installed auth entries:

mds.mon1
        key: &lt;your key&gt;
        caps: [mds] allow *
        caps: [mgr] allow profile mds
        caps: [mon] allow profile mds
        caps: [osd] allow rwx
mds.mon2
        key: &lt;your key&gt;
        caps: [mds] allow *
        caps: [mgr] allow profile mds
        caps: [mon] allow profile mds
        caps: [osd] allow rwx
mds.mon3
        key: &lt;your key&gt;
        caps: [mds] allow *
        caps: [mgr] allow profile mds
        caps: [mon] allow profile mds
        caps: [osd] allow rwx
osd.0
        key: &lt;your key&gt;
        caps: [mgr] allow profile osd
        caps: [mon] allow profile osd
        caps: [osd] allow *
osd.1
        key: &lt;your key&gt;
        caps: [mgr] allow profile osd
        caps: [mon] allow profile osd
        caps: [osd] allow *
osd.10
        key: &lt;your key&gt;
        caps: [mgr] allow profile osd
        caps: [mon] allow profile osd
        caps: [osd] allow *
osd.11
        key: &lt;your key&gt;
        caps: [mgr] allow profile osd
        caps: [mon] allow profile osd
        caps: [osd] allow *
osd.2
        key: &lt;your key&gt;
        caps: [mgr] allow profile osd
        caps: [mon] allow profile osd
        caps: [osd] allow *
osd.3
        key: &lt;your key&gt;
        caps: [mgr] allow profile osd
        caps: [mon] allow profile osd
        caps: [osd] allow *
osd.4
        key: &lt;your key&gt;
        caps: [mgr] allow profile osd
        caps: [mon] allow profile osd
        caps: [osd] allow *
osd.5
        key: &lt;your key&gt;
        caps: [mgr] allow profile osd
        caps: [mon] allow profile osd
        caps: [osd] allow *
osd.6
        key: &lt;your key&gt;
        caps: [mgr] allow profile osd
        caps: [mon] allow profile osd
        caps: [osd] allow *
osd.7
        key: &lt;your key&gt;
        caps: [mgr] allow profile osd
        caps: [mon] allow profile osd
        caps: [osd] allow *
osd.8
        key: &lt;your key&gt;
        caps: [mgr] allow profile osd
        caps: [mon] allow profile osd
        caps: [osd] allow *
osd.9
        key: &lt;your key&gt;
        caps: [mgr] allow profile osd
        caps: [mon] allow profile osd
        caps: [osd] allow *
client.admin
        key: &lt;your key&gt;
        caps: [mds] allow *
        caps: [mgr] allow *
        caps: [mon] allow *
        caps: [osd] allow *
client.bootstrap-mds
        key: &lt;your key&gt;
        caps: [mon] allow profile bootstrap-mds
client.bootstrap-mgr
        key: &lt;your key&gt;
        caps: [mon] allow profile bootstrap-mgr
client.bootstrap-osd
        key: &lt;your key&gt;
        caps: [mgr] allow r
        caps: [mon] allow profile bootstrap-osd
client.bootstrap-rbd
        key: &lt;your key&gt;
        caps: [mon] allow profile bootstrap-rbd
client.bootstrap-rbd-mirror
        key: &lt;your key&gt;
        caps: [mon] allow profile bootstrap-rbd-mirror
client.bootstrap-rgw
        key: &lt;your key&gt;
        caps: [mon] allow profile bootstrap-rgw
client.igw.mon2
        key: &lt;your key&gt;
        caps: [mgr] allow r
        caps: [mon] allow *
        caps: [osd] allow *
client.rgw.mon3
        key: &lt;your key&gt;
        caps: [mgr] allow r
        caps: [mon] allow rwx
        caps: [osd] allow rwx
client.storage
        key: &lt;your key&gt;
        caps: [mon] allow rw
mgr.mon1
        key: &lt;your key&gt;
        caps: [mds] allow *
        caps: [mon] allow profile mgr
        caps: [osd] allow *
</code></pre></div></p> <h5 id=task-2-create-a-new-keyring-and-associated-user>Task 2: Create a new keyring and associated user<a class=headerlink href=#task-2-create-a-new-keyring-and-associated-user title="Permanent link"> ¶</a></h5> <p>1). There are several different ways to create a new keyring and user. This is just one way. Create a new keyring and associated user named <code>James</code>. Remembgr that typically all new users will need read rights for the mon capability, and will need read/write rights for the osd capability, including a specification of rights to a pool. <div class=highlight><pre><span></span><code>admin:/etc/ceph # ceph-authtool -g -n client.james --cap mon &#39;allow r&#39; --cap osd &#39;allow rw pool=iscsi-images&#39; -C /etc/ceph/ceph.client.james.keyring
creating /etc/ceph/ceph.client.james.keyring

admin:/etc/ceph # l
total 16
drwxr-xr-x 1 root root  130 Jan  5 19:31 ./
drwxr-xr-x 1 root root 4392 Oct  5 13:03 ../
-rw------- 1 root root  151 Oct  5 13:13 ceph.client.admin.keyring
-rw------- 1 root root  126 Jan  5 19:31 ceph.client.james.keyring
-rw-r--r-- 1 root root  980 Oct  5 13:13 ceph.conf
-rw-r--r-- 1 root root   92 Aug 24 22:03 rbdmap
</code></pre></div></p> <p>2). Show the content of the newly created keyring: <div class=highlight><pre><span></span><code>admin:/etc/ceph # cat ceph.client.james.keyring
[client.james]
        key = &lt;your key&gt;
        caps mon = &quot;allow r&quot;
        caps osd = &quot;allow rw pool=iscsi-images&quot;
</code></pre></div></p> <p>3). Officially add the new keyring to Ceph: <div class=highlight><pre><span></span><code>admin:/etc/ceph # ceph auth add client.james -i /etc/ceph/ceph.client.james.keyring
added key for client.james
</code></pre></div></p> <p>4). Show the key information using the “auth” function: <div class=highlight><pre><span></span><code>admin:/etc/ceph # ceph auth get client.james
exported keyring for client.james
[client.james]
        key = &lt;your key&gt;
        caps mon = &quot;allow r&quot;
        caps osd = &quot;allow rw pool=iscsi-images&quot;
</code></pre></div></p> <h5 id=task-3-create-a-client-key-for-rbd>Task 3: Create a client key for RBD<a class=headerlink href=#task-3-create-a-client-key-for-rbd title="Permanent link"> ¶</a></h5> <p>1). Change to the directory that contains the ceph keyrings. <div class=highlight><pre><span></span><code>admin:~ # cd /etc/ceph/
</code></pre></div></p> <p>2). List the content of the directory: Although you see the admin users’s keyring, ceph.client.admin.keyring, there is not yet a file that is appropriate for a specific application to use. Also note that the permissions on the keyring file are quite restrictive: 0600 <div class=highlight><pre><span></span><code>admin:/etc/ceph # ls -l
-rw------- 1 root root 151 Oct  5 13:13 ceph.client.admin.keyring
-rw------- 1 root root 126 Jan  5 19:31 ceph.client.james.keyring
-rw-r--r-- 1 root root 980 Oct  5 13:13 ceph.conf
-rw-r--r-- 1 root root  92 Aug 24 22:03 rbdmap
</code></pre></div></p> <p>3). Show the content of the admin user’s keyring: You will use the value associated with the “key” key to create a new file. Copy the “key” value using your favorite method. <div class=highlight><pre><span></span><code>admin:/etc/ceph # cat ceph.client.admin.keyring
[client.admin]
        key = &lt;your key&gt;
        caps mds = &quot;allow *&quot;
        caps mon = &quot;allow *&quot;
        caps osd = &quot;allow *&quot;
        caps mgr = &quot;allow *&quot;
</code></pre></div></p> <p>4). Open a new file for editing called admin.secret using your favorite editor (such as vi): The name of the file isn’t very important, but naming it this way will help to identify its purpose: it’s a secret key for the admin user. Note that there are many ways to do this. An alternative way is mentioned in the tip below that will do this in one step using grep and awk. <div class=highlight><pre><span></span><code>admin:/etc/ceph # vi admin.secret
</code></pre></div></p> <p>5). Paste the “key” value into the new file. It will be the only content of the file. It will look like this (in fact it’s probably exactly the same as this, if you’re using the demo environment provided to you): <div class=highlight><pre><span></span><code>admin:/etc/ceph # cat admin.secret
&lt;your key&gt;
</code></pre></div></p> <p>6). Save the file and exist out of the editor.</p> <p>7). Change the permissions of the file so that no other user on the host can see the content of the file: <div class=highlight><pre><span></span><code>admin:/etc/ceph # chmod 0600 admin.secret

admin:/etc/ceph # l
drwxr-xr-x 1 root root  154 Jan  5 20:03 ./
drwxr-xr-x 1 root root 4392 Oct  5 13:03 ../
-rw------- 1 root root   41 Jan  5 20:03 admin.secret
-rw------- 1 root root  151 Oct  5 13:13 ceph.client.admin.keyring
-rw------- 1 root root  126 Jan  5 19:31 ceph.client.james.keyring
-rw-r--r-- 1 root root  980 Oct  5 13:13 ceph.conf
-rw-r--r-- 1 root root   92 Aug 24 22:03 rbdmap
</code></pre></div></p> <p>Tip:</p> <p>An alternative way to create this key file is to simply use grep/awk together in one bash command, like this: <div class=highlight><pre><span></span><code>admin:/etc/ceph # grep &quot;key =&quot; ceph.client.admin.keyring | awk -F&quot; = &quot; &#39;{ print $2 }&#39;
&lt;your key&gt;

admin:/etc/ceph # grep &quot;key =&quot; ceph.client.admin.keyring | awk -F&quot; = &quot; &#39;{ print $2 }&#39; &gt; admin.secret

admin:/etc/ceph # cat admin.secret
&lt;your key&gt;
</code></pre></div></p> <h5 id=task-4-view-the-ceph-master-configuration-file>Task 4: View the Ceph master configuration file<a class=headerlink href=#task-4-view-the-ceph-master-configuration-file title="Permanent link"> ¶</a></h5> <p>View the content of the file. The file is managed and controlled by DeepSea. The comment makes reference to the control files in the <code>/srv/salt/ceph/configuration/</code> directory hierarchy. This is a very simple storage cluster. In a more diverse and sophisticated ceph cluster there may be more configuration settings defined. Although this exercise doesn’t call out any more specific information about this configuration file, you may take a moment to consider the content of the file before finishing the task. <div class=highlight><pre><span></span><code>admin:/etc/ceph # cat ceph.conf
# DeepSea default configuration. Changes in this file will be overwritten on
# package update. Include custom configuration fragments in
# /srv/salt/ceph/configuration/files/ceph.conf.d/[global,osd,mon,mgr,mds,client].conf
[global]
fsid = 343ee7d3-232f-4c71-8216-1edbc55ac6e0
mon_initial_membgrs = mon1, mon2, mon3
mon_host = 10.58.121.186, 10.58.121.187, 10.58.121.188
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx
public_network = 10.58.120.0/23
cluster_network = 10.58.120.0/23
ms_bind_msgr2 = false
# enable old ceph health format in the json output. This fixes the
# ceph_exporter. This option will only stay until the prometheus plugin takes
# over
mon_health_preluminous_compat = true
mon health preluminous compat warning = false
rbd default features = 3
[client.rgw.mon3]
rgw frontends = &quot;beast port=80&quot;
rgw dns name = mon3.sha.me.corp
rgw enable usage log = true
[osd]
[mon]
[mgr]
[mds]
[client]


admin:/etc/ceph # ls -l /srv/salt/ceph/configuration/
drwxr-xr-x 1 salt salt  18 Oct  5 13:13 cache
drwxr-xr-x 1 root root  38 Oct  5 09:04 check
drwxr-xr-x 1 root root  74 Oct  5 09:04 create
-rw-r--r-- 1 root root 217 May 14  2020 default-import.sls
-rw-r--r-- 1 root root 222 May 14  2020 default.sls
drwxr-xr-x 1 root root 276 Oct  5 12:55 files
-rw-r--r-- 1 root root  74 May 14  2020 init.sls
</code></pre></div></p> <h4 id=222-run-the-ceph-health-commands>2.2.2. Run the Ceph Health Commands<a class=headerlink href=#222-run-the-ceph-health-commands title="Permanent link"> ¶</a></h4> <p>Get overall health status <div class=highlight><pre><span></span><code>admin:~ # ceph health
HEALTH_OK

admin:~ # ceph -s
admin:~ # ceph status
  cluster:
    id:     343ee7d3-232f-4c71-8216-1edbc55ac6e0
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum mon1,mon2,mon3 (age 9w)
    mgr: mon1(active, since 5w)
    mds: cephfs:1 {0=mon3=up:active} 2 up:standby
    osd: 12 osds: 12 up (since 98m), 12 in (since 3M)
    rgw: 1 daemon active (mon3)

  task status:
    scrub status:
        mds.mon3: idle

  data:
    pools:   7 pools, 208 pgs
    objects: 246 objects, 4.7 KiB
    usage:   14 GiB used, 82 GiB / 96 GiB avail
    pgs:     208 active+clean

  io:
    client:   852 B/s rd, 0 op/s rd, 0 op/s wr
</code></pre></div></p> <p>Run the “status” command for the monitors: <div class=highlight><pre><span></span><code>admin:~ # ceph mon stat
e1: 3 mons at {
                    mon1=[v2:10.58.121.186:3300/0,v1:10.58.121.186:6789/0],
                    mon2=[v2:10.58.121.187:3300/0,v1:10.58.121.187:6789/0],
                    mon3=[v2:10.58.121.188:3300/0,v1:10.58.121.188:6789/0]
                }, 
                election epoch 22, 
                leader 0 mon1, 
                quorum 0,1,2 mon1,mon2,mon3
</code></pre></div></p> <p>Run the “status” command for the placement groups: <div class=highlight><pre><span></span><code>admin:~ # ceph pg stat
208 pgs: 208 active+clean; 4.7 KiB data, 2.1 GiB used, 82 GiB / 96 GiB avail; 852 B/s rd, 0 op/s
</code></pre></div></p> <p>Run the ceph “status” command while watching for changes to the status: <div class=highlight><pre><span></span><code>admin:~ # ceph -s --watch-debug
  cluster:
    id:     343ee7d3-232f-4c71-8216-1edbc55ac6e0
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum mon1,mon2,mon3 (age 9w)
    mgr: mon1(active, since 5w)
    mds: cephfs:1 {0=mon3=up:active} 2 up:standby
    osd: 12 osds: 12 up (since 104m), 12 in (since 3M)
    rgw: 1 daemon active (mon3)

  task status:
    scrub status:
        mds.mon3: idle

  data:
    pools:   7 pools, 208 pgs
    objects: 246 objects, 4.7 KiB
    usage:   14 GiB used, 82 GiB / 96 GiB avail
    pgs:     208 active+clean

  io:
    client:   1.2 KiB/s rd, 1 op/s rd, 0 op/s wr


2021-01-05 20:20:53.947298 mgr.mon1 [DBG] pgmap v1597415: 208 pgs: 208 active+clean; 4.7 KiB data, 2.1 GiB used, 82 GiB / 96 GiB avail; 852 B/s rd, 0 op/s
2021-01-05 20:20:55.949294 mgr.mon1 [DBG] pgmap v1597416: 208 pgs: 208 active+clean; 4.7 KiB data, 2.1 GiB used, 82 GiB / 96 GiB avail; 1.2 KiB/s rd, 1 op/s
.......
</code></pre></div></p> <h4 id=223-manipulate-pools>2.2.3. Manipulate Pools<a class=headerlink href=#223-manipulate-pools title="Permanent link"> ¶</a></h4> <h5 id=task-1-display-a-list-of-the-current-pools>Task 1: Display a list of the current pools<a class=headerlink href=#task-1-display-a-list-of-the-current-pools title="Permanent link"> ¶</a></h5> <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool ls
iscsi-images
cephfs_data
cephfs_metadata
.rgw.root
default.rgw.control
default.rgw.meta
default.rgw.log

admin:~ # ceph osd pool ls detail
pool 1 &#39;iscsi-images&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 448 lfor 0/448/446 flags hashpspool stripe_width 0 application rbd
pool 2 &#39;cephfs_data&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 last_change 1395 lfor 0/1374/1372 flags hashpspool stripe_width 0 application cephfs
pool 3 &#39;cephfs_metadata&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 16 pgp_num 16 last_change 1385 lfor 0/975/973 flags hashpspool stripe_width 0 pg_autoscale_bias 4 pg_num_min 16 recovery_priority 5 application cephfs
pool 4 &#39;.rgw.root&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 31 flags hashpspool stripe_width 0 application rgw
pool 5 &#39;default.rgw.control&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 33 flags hashpspool stripe_width 0 application rgw
pool 6 &#39;default.rgw.meta&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 35 flags hashpspool stripe_width 0 application rgw
pool 7 &#39;default.rgw.log&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 37 flags hashpspool stripe_width 0 application rgw
</code></pre></div> <p>List pools with their index numbgr. Note how the index numbgr matches the index numbgr of the detail listing above. <div class=highlight><pre><span></span><code>admin:~ # ceph osd lspools
1 iscsi-images
2 cephfs_data
3 cephfs_metadata
4 .rgw.root
5 default.rgw.control
6 default.rgw.meta
7 default.rgw.log
</code></pre></div></p> <h5 id=task-2-display-the-usage-data-and-stats-of-the-current-pools>Task 2: Display the usage data and stats of the current pools<a class=headerlink href=#task-2-display-the-usage-data-and-stats-of-the-current-pools title="Permanent link"> ¶</a></h5> <p>Display pool usages. Note again index “ID” for the pool. <div class=highlight><pre><span></span><code>admin:~ # ceph df
RAW STORAGE:
    CLASS     SIZE       AVAIL      USED        RAW USED     %RAW USED
    hdd       96 GiB     82 GiB     2.1 GiB       14 GiB         14.81
    TOTAL     96 GiB     82 GiB     2.1 GiB       14 GiB         14.81
POOLS:
    POOL                    ID     STORED      OBJECTS     USED        %USED     MAX AVAIL
    iscsi-images             1       389 B           2     192 KiB         0        25 GiB
    cephfs_data              2         0 B           0         0 B         0        25 GiB
    cephfs_metadata          3     7.2 KiB          48     1.5 MiB         0        25 GiB
    .rgw.root                4     1.2 KiB           4     768 KiB         0        25 GiB
    default.rgw.control      5         0 B           8         0 B         0        25 GiB
    default.rgw.meta         6       381 B           3     576 KiB         0        25 GiB
    default.rgw.log          7      35 KiB         208      35 KiB         0        25 GiB


admin:~ # ceph df detail
RAW STORAGE:
    CLASS     SIZE       AVAIL      USED        RAW USED     %RAW USED
    hdd       96 GiB     82 GiB     2.1 GiB       14 GiB         14.81
    TOTAL     96 GiB     82 GiB     2.1 GiB       14 GiB         14.81
POOLS:
    POOL                    ID     STORED      OBJECTS     USED        %USED     MAX AVAIL     QUOTA OBJECTS     QUOTA BYTES     DIRTY     USED COMPR     UNDER COMPR
    iscsi-images             1       389 B           2     192 KiB         0        25 GiB     N/A               N/A                 2            0 B             0 B
    cephfs_data              2         0 B           0         0 B         0        25 GiB     N/A               N/A                 0            0 B             0 B
    cephfs_metadata          3     7.2 KiB          48     1.5 MiB         0        25 GiB     N/A               N/A                48            0 B             0 B
    .rgw.root                4     1.2 KiB           4     768 KiB         0        25 GiB     N/A               N/A                 4            0 B             0 B
    default.rgw.control      5         0 B           8         0 B         0        25 GiB     N/A               N/A                 8            0 B             0 B
    default.rgw.meta         6       381 B           3     576 KiB         0        25 GiB     N/A               N/A                 3            0 B             0 B
    default.rgw.log          7      35 KiB         208      35 KiB         0        25 GiB     N/A               N/A               208            0 B             0 B
</code></pre></div></p> <p>Display pool usages using rados command <div class=highlight><pre><span></span><code>admin:~ # rados df
POOL_NAME              USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED  RD_OPS      RD  WR_OPS      WR USED COMPR UNDER COMPR
.rgw.root           768 KiB       4      0     12                  0       0        0      40  40 KiB       4   4 KiB        0 B         0 B
cephfs_data             0 B       0      0      0                  0       0        0       0     0 B       0     0 B        0 B         0 B
cephfs_metadata     1.5 MiB      48      0    144                  0       0        0       0     0 B     111  42 KiB        0 B         0 B
default.rgw.control     0 B       8      0     24                  0       0        0       0     0 B       0     0 B        0 B         0 B
default.rgw.log      35 KiB     208      0    624                  0       0        0 5919671 5.6 GiB 3945118 946 KiB        0 B         0 B
default.rgw.meta    576 KiB       3      0      9                  0       0        0      38  28 KiB       4   3 KiB        0 B         0 B
iscsi-images        192 KiB       2      0      6                  0       0        0 4184657 4.0 GiB       8   2 KiB        0 B         0 B

total_objects    246
total_used       14 GiB
total_avail      82 GiB
total_space      96 GiB
</code></pre></div></p> <p>Show the statistics of the pools: <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool stats
pool iscsi-images id 1
  client io 1.2 KiB/s rd, 1 op/s rd, 0 op/s wr

pool cephfs_data id 2
  nothing is going on

pool cephfs_metadata id 3
  nothing is going on

pool .rgw.root id 4
  nothing is going on

pool default.rgw.control id 5
  nothing is going on

pool default.rgw.meta id 6
  nothing is going on

pool default.rgw.log id 7
  nothing is going on
</code></pre></div></p> <p>Show only the statistics about a specific pool: <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool stats .rgw.root
pool .rgw.root id 4
  nothing is going on
</code></pre></div></p> <p>Show which CRUSH Map ruleset was used to create the .rgw.root pool: <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool get .rgw.root crush_rule
crush_rule: replicated_rule
</code></pre></div></p> <p>Show the list of all the attributes of a pool that can be queried: <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool get .rgw.root size
                                        min_size
                                        pg_num
                                        pgp_num
                                        crush_rule
                                        Hashpspool
                                        Nodelete
                                        Nopgchange
                                        Nosizechange
                                        write_fadvise_dontneed
                                        noscrub|nodeep-scrub
                                        hit_set_type
                                        hit_set_period
                                        hit_set_count
                                        hit_set_fpp
                                        use_gmt_hitset
                                        target_max_objects
                                        target_max_bytes
                                        cache_target_dirty_ratio
                                        cache_target_dirty_high_ratio
                                        cache_target_full_ratio
                                        cache_min_flush_age
                                        cache_min_evict_age
                                        erasure_code_profile
                                        min_read_recency_for_promote
                                        all|min_write_recency_for_promote
                                        fast_read|hit_set_grade_decay_rate
                                        hit_set_search_last_n
                                        scrub_min_interval
                                        scrub_max_interval
                                        deep_scrub_interval
                                        recovery_priority
                                        recovery_op_priority
                                        scrub_priority
                                        compression_mode
                                        compression_algorithm
                                        compression_required_ratio
                                        compression_max_blob_size
                                        compression_min_blob_size
                                        csum_type|csum_min_block
                                        csum_max_block
                                        allow_ec_overwrites
                                        fingerprint_algorithm
                                        pg_autoscale_mode
                                        pg_autoscale_bias
                                        pg_num_min
                                        target_size_bytes
                                        target_size_ratio
</code></pre></div></p> <h5 id=task-3-create-two-new-pools-one-replicated-one-ec>Task 3: Create two new pools, one replicated, one EC<a class=headerlink href=#task-3-create-two-new-pools-one-replicated-one-ec title="Permanent link"> ¶</a></h5> <p>1). Create a new replicated pool that will be used for storing block data for RBD. Use the standard replicated_ruleset CRUSH Map:</p> <p>It would be tempting to the use the better_ruleset, but this demo environment doesn’t have enough resources for that.</p> <p>This is a demo environment, so the PG numbgrs will be low. In your production environments, be sure to assign an appropriately high numbgr, or use the pg_autoscaler manager module. <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool create rbd_pool 4 4 replicated replicated_rule
pool &#39;rbd_pool&#39; created
</code></pre></div></p> <p>2). Tell the cluster that you expect to have this new rbd_pool to use 50% of the total capacity: <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool set rbd_pool target_size_ratio .5
set pool 8 target_size_ratio to .5
</code></pre></div></p> <p>3). Create a new EC pool that will be used for storing RGW buckets and objects. Use the usable_profile Erasure Code profile that was created in an earlier exercise. And use the ec_rule CRUSH Map ruleset that was created in an earlier exercise: <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool create bucket_pool 4 4 erasure usable_profile ec_rule
pool &#39;bucket_pool&#39; created
</code></pre></div></p> <p>4). Tell the cluster that you expect to have this new bucket_pool to use 100GB of data: <code>POOL_TARGET_SIZE_BYTES_OVERCOMMITTED</code> <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool set bucket_pool target_size_bytes 100000000000
set pool 9 target_size_bytes to 100000000000
</code></pre></div></p> <p>5). Enable the PG Autoscaler feature on the two new pools, to ensure that we have an appropriate assignment of placement groups in the demo cluster: This presumes that you completed an earlier exercise that enable the pg_autoscaler manager module. <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool set bucket_pool pg_autoscale_mode on
set pool 9 pg_autoscale_mode to on

admin:~ # ceph osd pool set rbd_pool pg_autoscale_mode on
set pool 8 pg_autoscale_mode to on
</code></pre></div></p> <p>6). Again display a list of all the pools, which will now include the two new pools that you’ve just created: Notice in the detail listing that the two new pools don’t have an application attribute assigned to them. <div class=highlight><pre><span></span><code>admin:~ # ceph osd lspools
1 iscsi-images
2 cephfs_data
3 cephfs_metadata
4 .rgw.root
5 default.rgw.control
6 default.rgw.meta
7 default.rgw.log
8 rbd_pool
9 bucket_pool

admin:~ # ceph osd pool ls detail
pool 1 &#39;iscsi-images&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 448 lfor 0/448/446 flags hashpspool stripe_width 0 application rbd
pool 2 &#39;cephfs_data&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 last_change 1395 lfor 0/1374/1372 flags hashpspool stripe_width 0 application cephfs
pool 3 &#39;cephfs_metadata&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 16 pgp_num 16 last_change 1385 lfor 0/975/973 flags hashpspool stripe_width 0 pg_autoscale_bias 4 pg_num_min 16 recovery_priority 5 application cephfs
pool 4 &#39;.rgw.root&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 31 flags hashpspool stripe_width 0 application rgw
pool 5 &#39;default.rgw.control&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 33 flags hashpspool stripe_width 0 application rgw
pool 6 &#39;default.rgw.meta&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 35 flags hashpspool stripe_width 0 application rgw
pool 7 &#39;default.rgw.log&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 37 flags hashpspool stripe_width 0 application rgw
pool 8 &#39;rbd_pool&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 1415 lfor 0/0/1413 flags hashpspool stripe_width 0 target_size_ratio 0.5
pool 9 &#39;bucket_pool&#39; erasure size 3 min_size 2 crush_rule 3 object_hash rjenkins pg_num 4 pgp_num 4 autoscale_mode on last_change 1410 flags hashpspool stripe_width 8192 target_size_bytes 100000000000
</code></pre></div></p> <p>7). Check the pg_autoscale status, particularly to see a comparison of how much raw space is being consumed by the two pools:</p> <p>See that the RATE column for all of the replicated pools shows the value of 3.0, while the value for the bucket_pool – which is an EC pool – is 1.5. The EC pool, with a K+M of 2+1 consumes considerably less raw storage space. </p> <p>See the TARGET RATIO for the rbd_pool. Notice that the autoscaler has automatically adjusted the numbgr PGs assigned to rbd_pool from “4” to “128” because you told the cluster to have the pool use 50% of the capacity.</p> <p>See the TARGET SIZE for the bucket_pool, roughly 100GB. But the cluster may not have changed the PG_NUM value yet. The autoscaler will adjust the numbgr of PGs gradually, so as not to disrupt the performance too dramatically.</p> <p>While you’re here, you might also notice the RAW CAPACITY column. </p> <p>All pools are expecting to divide the cluster space equally, even though you’ve explicitly told the cluster that rbd_pool and bucket_pool will deviate from that even division.</p> <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool autoscale-status
POOL                  SIZE TARGET SIZE RATE RAW CAPACITY  RATIO TARGET RATIO EFFECTIVE RATIO BIAS PG_NUM NEW PG_NUM AUTOSCALE
iscsi-images          389               3.0       98256M 0.0000                               1.0     32            on
cephfs_data             0               3.0       98256M 0.0000                               1.0     32            off
cephfs_metadata      7412               3.0       98256M 0.0000                               4.0     16            off
.rgw.root            1245               3.0       98256M 0.0000                               1.0     32            warn
default.rgw.control     0               3.0       98256M 0.0000                               1.0     32            warn
default.rgw.meta      381               3.0       98256M 0.0000                               1.0     32            warn
default.rgw.log     35900               3.0       98256M 0.0000                               1.0     32            warn
rbd_pool                0               3.0       98256M 0.0000       0.5000                  1.0     32            on
bucket_pool             0       95367M  1.5       98256M 1.4559                               1.0      4            on
</code></pre></div> <h5 id=task-4-assign-an-application-to-the-two-new-pools>Task 4: Assign an application to the two new pools<a class=headerlink href=#task-4-assign-an-application-to-the-two-new-pools title="Permanent link"> ¶</a></h5> <p>1). Assign the rbd application to the new rbd_pool that you created in the previous task: <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool application enable rbd_pool rbd
enabled application &#39;rbd&#39; on pool &#39;rbd_pool&#39;
</code></pre></div></p> <p>2). Instruct the cluster to prepare the new rbd_pool for storing block device images: <div class=highlight><pre><span></span><code>admin:~ # rbd pool init rbd_pool
</code></pre></div></p> <p>3). Assign the rgw application to the new bucket_pool that you created in the previous task: <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool application enable bucket_pool rgw
enabled application &#39;rgw&#39; on pool &#39;bucket_pool&#39;
</code></pre></div></p> <p>4). Display a list of all the pools again, this time noticing that the application attribute is set on the two new pools. <div class=highlight><pre><span></span><code>admin:~ # ceph osd lspools
1 iscsi-images
2 cephfs_data
3 cephfs_metadata
4 .rgw.root
5 default.rgw.control
6 default.rgw.meta
7 default.rgw.log
8 rbd_pool
9 bucket_pool
admin:~ # ceph osd pool ls detail
pool 1 &#39;iscsi-images&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 448 lfor 0/448/446 flags hashpspool stripe_width 0 application rbd
pool 2 &#39;cephfs_data&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 last_change 1395 lfor 0/1374/1372 flags hashpspool stripe_width 0 application cephfs
pool 3 &#39;cephfs_metadata&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 16 pgp_num 16 last_change 1385 lfor 0/975/973 flags hashpspool stripe_width 0 pg_autoscale_bias 4 pg_num_min 16 recovery_priority 5 application cephfs
pool 4 &#39;.rgw.root&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 31 flags hashpspool stripe_width 0 application rgw
pool 5 &#39;default.rgw.control&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 33 flags hashpspool stripe_width 0 application rgw
pool 6 &#39;default.rgw.meta&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 35 flags hashpspool stripe_width 0 application rgw
pool 7 &#39;default.rgw.log&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 37 flags hashpspool stripe_width 0 application rgw
pool 8 &#39;rbd_pool&#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 1420 lfor 0/0/1413 flags hashpspool,selfmanaged_snaps stripe_width 0 target_size_ratio 0.5 application rbd
        removed_snaps [1~3]
pool 9 &#39;bucket_pool&#39; erasure size 3 min_size 2 crush_rule 3 object_hash rjenkins pg_num 4 pgp_num 4 autoscale_mode on last_change 1422 flags hashpspool stripe_width 8192 target_size_bytes 100000000000 application rgw
</code></pre></div></p> <p>5). Another way to display which application is assigned to a pool is: <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool application get bucket_pool
{
    &quot;rgw&quot;: {}
}

admin:~ # ceph osd pool application get rbd_pool
{
    &quot;rbd&quot;: {}
}
</code></pre></div></p> <h5 id=task-5-manage-snapshots-of-the-new-rgw-bucket-pool>Task 5: Manage snapshots of the new RGW bucket pool<a class=headerlink href=#task-5-manage-snapshots-of-the-new-rgw-bucket-pool title="Permanent link"> ¶</a></h5> <p>1). Display a list of the snapshots that exist of the bucket_pool that you created in the previous task: The output show that there are “0 snaps.” Right, it is a little funny that you only list the snapshots with rados command; no such functionality exists with the ceph osd pool command. <div class=highlight><pre><span></span><code>admin:~ # rados -p bucket_pool lssnap
0 snaps
</code></pre></div></p> <p>2). Take (make) a snapshot of the rbd_pool: <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool mksnap bucket_pool brand_new_pool_snapshot
created pool bucket_pool snap brand_new_pool_snapshot
</code></pre></div></p> <p>3). Display the list of the snapshots again: <div class=highlight><pre><span></span><code>admin:~ # rados -p bucket_pool lssnap
1       brand_new_pool_snapshot 2021.01.05 22:23:23
1 snaps
</code></pre></div></p> <p>4). Remove the snapshot: <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool rmsnap bucket_pool brand_new_pool_snapshot
removed pool bucket_pool snap brand_new_pool_snapshot
</code></pre></div></p> <p>5). Display the list of the snapshots again: <div class=highlight><pre><span></span><code>admin:~ # rados -p bucket_pool lssnap
0 snaps
</code></pre></div></p> <h4 id=224-maintain-consistency-of-data-with-scrub-and-repair>2.2.4. Maintain consistency of data with Scrub and Repair<a class=headerlink href=#224-maintain-consistency-of-data-with-scrub-and-repair title="Permanent link"> ¶</a></h4> <p>Scrubbing is like “fsck,” which ensures that OSDs have durable, consistent data. Most of the scrubbing of OSDs happens automatically on a periodic basis. </p> <h5 id=task-1-display-a-few-of-the-scrub-settings>Task 1: Display a few of the Scrub settings<a class=headerlink href=#task-1-display-a-few-of-the-scrub-settings title="Permanent link"> ¶</a></h5> <p>1). Show the possible configuration settings related to scrub: If you simply grep for “scrub” you’ll get more than you really want; there are some mon_scrub settings that aren’t related to this exercise. <div class=highlight><pre><span></span><code>admin:~ # ceph config ls | grep osd_scrub
osd_scrub_invalid_stats
osd_scrub_during_recovery
osd_scrub_begin_hour
osd_scrub_end_hour
osd_scrub_begin_week_day
osd_scrub_end_week_day
osd_scrub_load_threshold
osd_scrub_min_interval
osd_scrub_max_interval
osd_scrub_interval_randomize_ratio
osd_scrub_backoff_ratio
osd_scrub_chunk_min
osd_scrub_chunk_max
osd_scrub_sleep
osd_scrub_auto_repair
osd_scrub_auto_repair_num_errors
osd_scrub_max_preemptions
osd_scrub_priority
osd_scrub_cost

admin:~ # ceph config ls | grep osd_deep_scrub
osd_deep_scrub_interval
osd_deep_scrub_randomize_ratio
osd_deep_scrub_stride
osd_deep_scrub_keys
osd_deep_scrub_update_digest_min_age
osd_deep_scrub_large_omap_object_key_threshold
osd_deep_scrub_large_omap_object_value_sum_threshold

admin:~ # ceph config ls | grep scrub
mon_warn_pg_not_scrubbed_ratio
mon_warn_pg_not_deep_scrubbed_ratio
mon_scrub_interval
mon_scrub_timeout
mon_scrub_max_keys
mon_scrub_inject_crc_mismatch
mon_scrub_inject_missing_keys
osd_op_queue_mclock_scrub_res
osd_op_queue_mclock_scrub_wgt
osd_op_queue_mclock_scrub_lim
osd_scrub_invalid_stats
osd_max_scrubs
osd_scrub_during_recovery
osd_scrub_begin_hour
osd_scrub_end_hour
osd_scrub_begin_week_day
osd_scrub_end_week_day
osd_scrub_load_threshold
osd_scrub_min_interval
osd_scrub_max_interval
osd_scrub_interval_randomize_ratio
osd_scrub_backoff_ratio
osd_scrub_chunk_min
osd_scrub_chunk_max
osd_scrub_sleep
osd_scrub_auto_repair
osd_scrub_auto_repair_num_errors
osd_scrub_max_preemptions
osd_deep_scrub_interval
osd_deep_scrub_randomize_ratio
osd_deep_scrub_stride
osd_deep_scrub_keys
osd_deep_scrub_update_digest_min_age
osd_deep_scrub_large_omap_object_key_threshold
osd_deep_scrub_large_omap_object_value_sum_threshold
osd_debug_deep_scrub_sleep
osd_scrub_priority
osd_scrub_cost
osd_requested_scrub_priority
mds_max_scrub_ops_in_progress
</code></pre></div></p> <p>2). Get the value of a few of the different scrub schedule settings: Note that “0” and “24” are the same setting. <div class=highlight><pre><span></span><code>admin:~ # ceph config get osd.* osd_scrub_begin_hour
0
admin:~ # ceph config get osd.* osd_scrub_end_hour
24
</code></pre></div></p> <p>3). Get the value of the scrub and repair settings: The “auto repair” feature is turned off, and the maximum numbgr of errors that “auto repair” would automatically repair is 5. <div class=highlight><pre><span></span><code>admin:~ # ceph config get osd.* osd_scrub_auto_repair
false
admin:~ # ceph config get osd.* osd_scrub_auto_repair_num_errors
5
</code></pre></div></p> <h5 id=task-2-change-the-scrub-settings-in-cephconf>Task 2: Change the Scrub settings in ceph.conf<a class=headerlink href=#task-2-change-the-scrub-settings-in-cephconf title="Permanent link"> ¶</a></h5> <p>1). Display the ceph.conf, and verify that the file doesn’t have any settings defined yet that are related to scrub. The settings would be located in the <code>[global]</code> section of the file: <div class=highlight><pre><span></span><code># DeepSea default configuration. Changes in this file will be overwritten on
# package update. Include custom configuration fragments in
# /srv/salt/ceph/configuration/files/ceph.conf.d/[global,osd,mon,mgr,mds,client].conf
[global]
fsid = 343ee7d3-232f-4c71-8216-1edbc55ac6e0
mon_initial_membgrs = mon1, mon2, mon3
mon_host = 10.58.121.186, 10.58.121.187, 10.58.121.188
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx
public_network = 10.58.120.0/23
cluster_network = 10.58.120.0/23
ms_bind_msgr2 = false
# enable old ceph health format in the json output. This fixes the
# ceph_exporter. This option will only stay until the prometheus plugin takes
# over
mon_health_preluminous_compat = true
mon health preluminous compat warning = false
rbd default features = 3
[client.rgw.mon3]
rgw frontends = &quot;beast port=80&quot;
rgw dns name = mon3.sha.me.corp
rgw enable usage log = true
[osd]
[mon]
[mgr]
[mds]
[client]
</code></pre></div></p> <p>2). Change to the Salt File Server directory that will have Salt control the master ceph.conf configuration file: <div class=highlight><pre><span></span><code>admin:~ # cd /srv/salt/ceph/configuration/files/ceph.conf.d/
</code></pre></div></p> <p>3). List the content of the directory: The directory is empty. (Well, there is a README, but no other functional files.) <div class=highlight><pre><span></span><code>admin:/srv/salt/ceph/configuration/files/ceph.conf.d # ls -l
-rw-r--r-- 1 root root 1989 May 14  2020 README
</code></pre></div></p> <p>4). Create and edit a new file called global.conf. You don’t have to use vi, but this step uses vi as one way of doing it: Be sure that you spell everything correctly, including the absence of “_” characters; there are spaces. Save the file and exit out of the editor. <div class=highlight><pre><span></span><code>admin:/srv/salt/ceph/configuration/files/ceph.conf.d # vi global.conf
</code></pre></div></p> <p>Add the following content to the file:</p> <div class=highlight><pre><span></span><code>osd scrub begin hour = 23
osd scrub end hour = 5
osd scrub auto repair = True
osd scrub auto repair num errors = 10
</code></pre></div> <p>5). Use DeepSea (Salt) to stage the file properly in Salt’s File Server on the Salt Master (admin): <div class=highlight><pre><span></span><code>admin:/srv/salt/ceph/configuration/files/ceph.conf.d # salt admin* state.apply ceph.configuration.create
admin.sha.me.corp:
  Name: /var/cache/salt/minion/files/base/ceph/configuration - Function: file.absent - Result: Changed Started: - 22:42:34.900173 Duration: 20.891 ms
  Name: /srv/salt/ceph/configuration/cache/ceph.conf - Function: file.managed - Result: Changed Started: - 22:42:34.921454 Duration: 8576.516 ms
  Name: find /var/cache/salt/master/jobs -user root -exec chown salt:salt {} &#39;;&#39; - Function: cmd.run - Result: Changed Started: - 22:42:43.535022 Duration: 71.957 ms

Summary for admin.sha.me.corp
------------
Succeeded: 3 (changed=3)
Failed:    0
------------
Total states run:     3
Total run time:   8.669 s
</code></pre></div></p> <p>6). Using DeepSea (Salt), distribute the new ceph.conf configuration settings to all the nodes in the cluster: <div class=highlight><pre><span></span><code>admin:/srv/salt/ceph/configuration/files/ceph.conf.d # salt \* state.apply ceph.configuration
mon3.sha.me.corp:
  Name: /etc/ceph/ceph.conf - Function: file.managed - Result: Changed Started: - 22:44:07.986661 Duration: 101.977 ms

Summary for mon3.sha.me.corp
------------
Succeeded: 1 (changed=1)
Failed:    0
------------
Total states run:     1
Total run time: 101.977 ms
mon1.sha.me.corp:
  Name: /etc/ceph/ceph.conf - Function: file.managed - Result: Changed Started: - 22:44:08.012479 Duration: 108.888 ms

Summary for mon1.sha.me.corp
------------
Succeeded: 1 (changed=1)
Failed:    0
------------
Total states run:     1
Total run time: 108.888 ms
data3.sha.me.corp:
  Name: /etc/ceph/ceph.conf - Function: file.managed - Result: Changed Started: - 22:44:08.052247 Duration: 98.681 ms

Summary for data3.sha.me.corp
------------
Succeeded: 1 (changed=1)
Failed:    0
------------
Total states run:     1
Total run time:  98.681 ms
admin.sha.me.corp:
  Name: /etc/ceph/ceph.conf - Function: file.managed - Result: Changed Started: - 22:44:08.072402 Duration: 97.231 ms

Summary for admin.sha.me.corp
------------
Succeeded: 1 (changed=1)
Failed:    0
------------
Total states run:     1
Total run time:  97.231 ms
data1.sha.me.corp:
  Name: /etc/ceph/ceph.conf - Function: file.managed - Result: Changed Started: - 22:44:08.076279 Duration: 104.169 ms

Summary for data1.sha.me.corp
------------
Succeeded: 1 (changed=1)
Failed:    0
------------
Total states run:     1
Total run time: 104.169 ms
data4.sha.me.corp:
  Name: /etc/ceph/ceph.conf - Function: file.managed - Result: Changed Started: - 22:44:08.081635 Duration: 105.13 ms

Summary for data4.sha.me.corp
------------
Succeeded: 1 (changed=1)
Failed:    0
------------
Total states run:     1
Total run time: 105.130 ms
mon2.sha.me.corp:
  Name: /etc/ceph/ceph.conf - Function: file.managed - Result: Changed Started: - 22:44:08.155758 Duration: 105.004 ms

Summary for mon2.sha.me.corp
------------
Succeeded: 1 (changed=1)
Failed:    0
------------
Total states run:     1
Total run time: 105.004 ms
data2.sha.me.corp:
  Name: /etc/ceph/ceph.conf - Function: file.managed - Result: Changed Started: - 22:44:08.252200 Duration: 109.552 ms

Summary for data2.sha.me.corp
------------
Succeeded: 1 (changed=1)
Failed:    0
------------
Total states run:     1
Total run time: 109.552 ms
</code></pre></div></p> <p>7). Verify that the new ceph.conf settings have been put into place on the admin node: <div class=highlight><pre><span></span><code>admin:/srv/salt/ceph/configuration/files/ceph.conf.d # cat /etc/ceph/ceph.conf
# DeepSea default configuration. Changes in this file will be overwritten on
# package update. Include custom configuration fragments in
# /srv/salt/ceph/configuration/files/ceph.conf.d/[global,osd,mon,mgr,mds,client].conf
[global]
fsid = 343ee7d3-232f-4c71-8216-1edbc55ac6e0
mon_initial_membgrs = mon1, mon2, mon3
mon_host = 10.58.121.188, 10.58.121.187, 10.58.121.186
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx
public_network = 10.58.120.0/23
cluster_network = 10.58.120.0/23
ms_bind_msgr2 = false
# enable old ceph health format in the json output. This fixes the
# ceph_exporter. This option will only stay until the prometheus plugin takes
# over
mon_health_preluminous_compat = true
mon health preluminous compat warning = false
rbd default features = 3
osd scrub begin hour = 23
osd scrub end hour = 5
osd scrub auto repair = True
osd scrub auto repair num errors = 10
[client.rgw.mon3]
rgw frontends = &quot;beast port=80&quot;
rgw dns name = mon3.sha.me.corp
rgw enable usage log = true
[osd]
[mon]
[mgr]
[mds]
[client]
</code></pre></div></p> <p>8). Also verify that other minions in the cluster have also received the updated configuration file, such as on the mon1 and data2 nodes: <div class=highlight><pre><span></span><code>admin:/srv/salt/ceph/configuration/files/ceph.conf.d # ssh mon1 cat /etc/ceph/ceph.conf
# DeepSea default configuration. Changes in this file will be overwritten on
# package update. Include custom configuration fragments in
# /srv/salt/ceph/configuration/files/ceph.conf.d/[global,osd,mon,mgr,mds,client].conf
[global]
fsid = 343ee7d3-232f-4c71-8216-1edbc55ac6e0
mon_initial_membgrs = mon1, mon2, mon3
mon_host = 10.58.121.188, 10.58.121.187, 10.58.121.186
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx
public_network = 10.58.120.0/23
cluster_network = 10.58.120.0/23
ms_bind_msgr2 = false
# enable old ceph health format in the json output. This fixes the
# ceph_exporter. This option will only stay until the prometheus plugin takes
# over
mon_health_preluminous_compat = true
mon health preluminous compat warning = false
rbd default features = 3
osd scrub begin hour = 23
osd scrub end hour = 5
osd scrub auto repair = True
osd scrub auto repair num errors = 10
[client.rgw.mon3]
rgw frontends = &quot;beast port=80&quot;
rgw dns name = mon3.sha.me.corp
rgw enable usage log = true
[osd]
[mon]
[mgr]
[mds]
[client]


admin:/srv/salt/ceph/configuration/files/ceph.conf.d # ssh data1 cat /etc/ceph/ceph.conf
# DeepSea default configuration. Changes in this file will be overwritten on
# package update. Include custom configuration fragments in
# /srv/salt/ceph/configuration/files/ceph.conf.d/[global,osd,mon,mgr,mds,client].conf
[global]
fsid = 343ee7d3-232f-4c71-8216-1edbc55ac6e0
mon_initial_membgrs = mon1, mon2, mon3
mon_host = 10.58.121.188, 10.58.121.187, 10.58.121.186
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx
public_network = 10.58.120.0/23
cluster_network = 10.58.120.0/23
ms_bind_msgr2 = false
# enable old ceph health format in the json output. This fixes the
# ceph_exporter. This option will only stay until the prometheus plugin takes
# over
mon_health_preluminous_compat = true
mon health preluminous compat warning = false
rbd default features = 3
osd scrub begin hour = 23
osd scrub end hour = 5
osd scrub auto repair = True
osd scrub auto repair num errors = 10
[client.rgw.mon3]
rgw frontends = &quot;beast port=80&quot;
rgw dns name = mon3.sha.me.corp
rgw enable usage log = true
[osd]
[mon]
[mgr]
[mds]
[client]
</code></pre></div></p> <p>9). Apply the settings of the ceph.conf file to appropriate nodes in the cluster: <div class=highlight><pre><span></span><code>admin:/srv/salt/ceph/configuration/files/ceph.conf.d # ceph config assimilate-conf -i /etc/ceph/ceph.conf
[global]
        fsid = 343ee7d3-232f-4c71-8216-1edbc55ac6e0
        mon_health_preluminous_compat = true
        mon_health_preluminous_compat_warning = false
        mon_host = 10.58.121.188, 10.58.121.187, 10.58.121.186
        mon_initial_membgrs = mon1, mon2, mon3
</code></pre></div></p> <h5 id=task-3-change-the-scrub-settings-directly-in-the-configuration-db>Task 3: Change the Scrub settings directly in the Configuration DB<a class=headerlink href=#task-3-change-the-scrub-settings-directly-in-the-configuration-db title="Permanent link"> ¶</a></h5> <p>1). Query the configuration database to see the value of “osd_scrub_auto_repair_num_errors”: You changed this value to “10” in the previous Task. <div class=highlight><pre><span></span><code>admin:~ # ceph config get osd.* osd_scrub_auto_repair_num_errors
10
</code></pre></div></p> <p>2). Change the value of “osd_scrub_auto_repair_num_errors” to “8”: <div class=highlight><pre><span></span><code>admin:~ # ceph config set osd.* osd_scrub_auto_repair_num_errors 8
</code></pre></div></p> <p>3). Show that the change has taken immediate effect by re-running the same command that was used in the first step: <div class=highlight><pre><span></span><code>admin:~ # ceph config get osd.* osd_scrub_auto_repair_num_errors
8
</code></pre></div></p> <h5 id=task-4-manually-scrub-and-repair-an-osd-and-a-pg>Task 4: Manually scrub and repair an OSD and a PG<a class=headerlink href=#task-4-manually-scrub-and-repair-an-osd-and-a-pg title="Permanent link"> ¶</a></h5> <p>This won’t do much in this demo environment, because the OSDs aren’t storing very much data. But it’s worth having some practice.</p> <p>1). Start a scrubbing of one of the OSDs: <div class=highlight><pre><span></span><code>admin:~ # ceph osd scrub osd.1
instructed osd(s) 1 to scrub
</code></pre></div></p> <p>2). Scrub a Placement Group: <div class=highlight><pre><span></span><code>admin:~ # ceph pg scrub 8.1
instructing pg 8.1 on osd.0 to scrub
</code></pre></div></p> <p>3). Repair an OSD: <div class=highlight><pre><span></span><code>admin:~ # ceph osd repair osd.1
instructed osd(s) 1 to repair
</code></pre></div></p> <p>4). Repair a PG: <div class=highlight><pre><span></span><code>admin:~ # ceph pg repair 8.1
instructing pg 8.1 on osd.0 to repair
</code></pre></div></p> <p>5). Show what’s currently happening to the OSD that you instructed to have scrubbed and repaired: <div class=highlight><pre><span></span><code>admin:~ # ceph osd dump | grep osd.1
max_osd 12
osd.1 up   in  weight 1 up_from 10 up_thru 1415 down_at 0 last_clean_interval [0,0) v1:10.58.121.185:6800/11157 v1:10.58.121.185:6801/11157 exists,up 32c78078-1878-4fac-9738-00d8bf80deea
osd.10 up   in  weight 1 up_from 18 up_thru 1413 down_at 0 last_clean_interval [0,0) v1:10.58.121.182:6808/11130 v1:10.58.121.182:6809/11130 exists,up 6cb26fdc-09b1-42de-8855-7203931a0101
osd.11 up   in  weight 1 up_from 18 up_thru 1415 down_at 0 last_clean_interval [0,0) v1:10.58.121.185:6808/11995 v1:10.58.121.185:6809/11995 exists,up cc22107d-0239-4874-8308-6c137c8a0931
</code></pre></div></p> <p>6). Show what’s currently happening to the PG that you instructed to have scrubbed and repaired: <div class=highlight><pre><span></span><code>admin:~ # ceph pg dump | grep &quot;8\.1&quot;
dumped all
8.16          0                  0        0         0       0     0           0          0    0        0 active+clean 2021-01-05 21:01:16.383909         0&#39;0      1423:27   [6,4,5]          6   [6,4,5]              6         0&#39;0 2021-01-05 20:53:47.314062             0&#39;0 2021-01-05 20:53:47.314062             0
8.17          0                  0        0         0       0     0           0          0    0        0 active+clean 2021-01-05 22:57:01.044252         0&#39;0      1424:30   [1,6,8]          1   [1,6,8]              1         0&#39;0 2021-01-05 22:57:01.044098             0&#39;0 2021-01-05 22:57:01.044098             0
8.14          0                  0        0         0       0     0           0          0    0        0 active+clean 2021-01-05 22:56:56.081480         0&#39;0      1424:30   [1,2,4]          1   [1,2,4]              1         0&#39;0 2021-01-05 22:56:56.081356             0&#39;0 2021-01-05 22:56:56.081356             0
8.15          0                  0        0         0       0     0           0          0    0        0 active+clean 2021-01-05 21:01:16.375386         0&#39;0      1423:27   [3,5,0]          3   [3,5,0]              3         0&#39;0 2021-01-05 20:53:53.231124             0&#39;0 2021-01-05 20:48:05.301705             0
8.12          0                  0        0         0       0     0           0          0    0        0 active+clean 2021-01-05 21:01:16.370121         0&#39;0      1423:27  [11,2,8]         11  [11,2,8]             11         0&#39;0 2021-01-05 20:53:48.149449             0&#39;0 2021-01-05 20:48:05.301705             0
2.18          0                  0        0         0       0     0           0          0    0        0 active+clean 2021-01-05 16:44:58.986205         0&#39;0    1423:1630  [10,1,8]         10  [10,1,8]             10         0&#39;0 2021-01-05 13:02:00.365382             0&#39;0 2021-01-02 00:38:58.134100             0
8.13          0                  0        0         0       0     0           0          0    0        0 active+clean 2021-01-05 21:01:16.387832         0&#39;0      1423:27   [0,8,1]          0   [0,8,1]              0         0&#39;0 2021-01-05 20:53:56.132358             0&#39;0 2021-01-05 20:48:05.301705             0
8.10          0                  0        0         0       0     0           0          0    0        0 active+clean 2021-01-05 21:01:16.368416         0&#39;0      1423:27  [11,3,6]         11  [11,3,6]             11         0&#39;0 2021-01-05 20:53:51.152790             0&#39;0 2021-01-05 20:48:05.301705             0
8.11          0                  0        0         0       0     0           0          0    0        0 active+clean 2021-01-05 21:01:16.377871         0&#39;0      1423:24  [3,10,5]          3  [3,10,5]              3         0&#39;0 2021-01-05 20:53:45.195257             0&#39;0 2021-01-05 20:48:05.301705             0
8.1e          0                  0        0         0       0     0           0          0    0        0 active+clean 2021-01-05 21:01:16.391754         0&#39;0      1423:47  [0,11,8]          0  [0,11,8]              0         0&#39;0 2021-01-05 20:53:55.081582             0&#39;0 2021-01-05 20:48:05.301705             0
8.1           0                  0        0         0       0     0           0          0    0        0 active+clean 2021-01-05 22:56:39.829397         0&#39;0      1424:54  [0,7,10]          0  [0,7,10]              0         0&#39;0 2021-01-05 22:56:39.829241             0&#39;0 2021-01-05 22:56:39.829241             0
8.1f          0                  0        0         0       0     0           0          0    0        0 active+clean 2021-01-05 21:01:16.392315         0&#39;0      1423:27   [7,5,9]          7   [7,5,9]              7         0&#39;0 2021-01-05 20:53:59.988252             0&#39;0 2021-01-05 20:48:05.301705             0
5.4           0                  0        0         0       0     0           0          0    0        0 active+clean 2021-01-05 18:21:28.179266         0&#39;0    1423:1554   [7,9,6]          7   [7,9,6]              7         0&#39;0 2021-01-05 18:21:28.179166             0&#39;0 2021-01-05 18:21:28.179166             0
5.b           0                  0        0         0       0     0           0          0    0        0 active+clean 2021-01-05 18:37:01.467457         0&#39;0    1423:1547  [2,0,11]          2  [2,0,11]              2         0&#39;0 2021-01-04 23:46:58.132824             0&#39;0 2021-01-02 03:35:41.214192             0
8.19          0                  0        0         0       0     0           0          0    0        0 active+clean 2021-01-05 22:57:06.059090         0&#39;0      1424:30   [1,8,2]          1   [1,8,2]              1         0&#39;0 2021-01-05 22:57:06.058935             0&#39;0 2021-01-05 22:57:06.058935             0
8.18          0                  0        0         0       0     0           0          0    0        0 active+clean 2021-01-05 22:57:05.097742         0&#39;0      1424:30   [1,3,6]          1   [1,3,6]              1         0&#39;0 2021-01-05 22:57:05.097670             0&#39;0 2021-01-05 22:57:05.097670             0
1.11          0                  0        0         0       0     0           0          0    0        0 active+clean 2021-01-05 00:30:18.193988         0&#39;0    1423:1605   [0,8,6]          0   [0,8,6]              0         0&#39;0 2021-01-05 00:30:18.193868             0&#39;0 2020-12-29 06:30:58.897565             0
8.1b          0                  0        0         0       0     0           0          0    0        0 active+clean 2021-01-05 22:57:13.146469         0&#39;0      1424:30   [1,4,6]          1   [1,4,6]              1         0&#39;0 2021-01-05 22:57:13.146390             0&#39;0 2021-01-05 22:57:13.146390             0
8.1a          1                  0        0         0       0    19           0          0    2        2 active+clean 2021-01-05 21:01:16.386166      1420&#39;2      1423:29 [9,11,10]          9 [9,11,10]              9         0&#39;0 2021-01-05 20:53:48.690239             0&#39;0 2021-01-05 20:48:05.301705             0
8.1d          0                  0        0         0       0     0           0          0    0        0 active+clean 2021-01-05 21:01:16.388079         0&#39;0      1423:56   [0,2,3]          0   [0,2,3]              0         0&#39;0 2021-01-05 20:53:54.121281             0&#39;0 2021-01-05 20:48:05.301705             0
8.1c          0                  0        0         0       0     0           0          0    0        0 active+clean 2021-01-05 21:01:16.385846         0&#39;0      1423:27  [2,11,7]          2  [2,11,7]              2         0&#39;0 2021-01-05 20:53:55.458714             0&#39;0 2021-01-05 20:48:05.301705             0
</code></pre></div></p> <h4 id=225-manipulate-manager-modules>2.2.5. Manipulate Manager Modules<a class=headerlink href=#225-manipulate-manager-modules title="Permanent link"> ¶</a></h4> <h5 id=task-1-display-the-list-of-enabled-manager-modules>Task 1: Display the list of enabled Manager Modules<a class=headerlink href=#task-1-display-the-list-of-enabled-manager-modules title="Permanent link"> ¶</a></h5> <p>1). Run the following command to show the list of enabled manager modules: Note that several modules are already enabled, such as: dashboard, iostat, pg_autosclater, prometheus, and restful. Even though they are not listed, the crash module and the balancer module are already enabled by default. <div class=highlight><pre><span></span><code>admin:~ # ceph mgr module ls | head
{
    &quot;always_on_modules&quot;: [
        &quot;balancer&quot;,
        &quot;crash&quot;,
        &quot;devicehealth&quot;,
        &quot;orchestrator_cli&quot;,
        &quot;progress&quot;,
        &quot;rbd_support&quot;,
        &quot;status&quot;,
        &quot;volumes&quot;
</code></pre></div></p> <p>2). Demonstrate that the crash module is enabled by running its command with no arguments: A list of “7 closest matches” is displayed, representing possible additional arguments to be used with the crash command. The crash module is therefore available. <div class=highlight><pre><span></span><code>admin:~ # ceph crash
                crash info &lt;id&gt;
                crash ls
                crash ls-new
                crash post
                crash prune &lt;keep&gt;
                crash rm &lt;id&gt;
                crash stat
                crash json_report &lt;hours&gt;
                crash archive &lt;id&gt;
                crash archive-all


admin:~ # ceph crash stat
0 crashes recorded
</code></pre></div></p> <p>Task 2: Use the iostat module to display statistics for the IO of the cluster The iostat module is really simple, but very helpful. Run the command: <div class=highlight><pre><span></span><code>admin:~ # ceph iostat
+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+
|                        Read |                       Write |                       Total |                   Read IOPS |                  Write IOPS |                  Total IOPS |
+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+
|                    1024 B/s |                       0 B/s |                    1024 B/s |                           1 |                           0 |                           1 |
|                    1024 B/s |                       0 B/s |                    1024 B/s |                           1 |                           0 |                           1 |
|                       0 B/s |                       0 B/s |                       0 B/s |                           0 |                           0 |                           0 |
</code></pre></div></p> <p>Task 3: Enable and configure the telemetry manager module</p> <p>1). Enable the telemetry manager module: <div class=highlight><pre><span></span><code>admin:~ # ceph mgr module enable telemetry
</code></pre></div></p> <p>2). Show the various sub-commands that are associated with the telemetry command: A list of “5 closest matches” is displayed, showing various options. <div class=highlight><pre><span></span><code>admin:~ # ceph telemetry
                telemetry status
                telemetry send {ceph|device [ceph|device...]} {&lt;license&gt;}
                telemetry show {&lt;channels&gt; [&lt;channels&gt;...]}
                telemetry show-device
                telemetry on {&lt;license&gt;}
                telemetry off
</code></pre></div></p> <p>3). Show the status of the telemetry module:</p> <p>Notice that the output is returned as key/value pairs. </p> <p>Notice also that although the module has been enabled (which you accomplished in the first step of this task), the functionality is not enabled (enable=false). And for most of the keys, a null value is set.</p> <p>See that the url value is set to https://telemetry.ceph.com/report. That means that crash reports and other usage information about this cluster are going to be sent to the Ceph Community. <div class=highlight><pre><span></span><code>admin:~ # ceph telemetry status
{
    &quot;url&quot;: &quot;https://telemetry.ceph.com/report&quot;,
    &quot;device_url&quot;: &quot;https://telemetry.ceph.com/device&quot;,
    &quot;enabled&quot;: false,
    &quot;last_opt_revision&quot;: 1,
    &quot;leaderboard&quot;: false,
    &quot;description&quot;: null,
    &quot;contact&quot;: null,
    &quot;organization&quot;: null,
    &quot;proxy&quot;: null,
    &quot;interval&quot;: 24,
    &quot;channel_basic&quot;: true,
    &quot;channel_ident&quot;: false,
    &quot;channel_crash&quot;: true,
    &quot;channel_device&quot;: true,
    &quot;last_upload&quot;: null
}
</code></pre></div></p> <p>4). Set the description, contact, and organization values: <div class=highlight><pre><span></span><code>admin:~ # ceph config set mgr mgr/telemetry/contact &#39;JD &lt;james@example.net&gt;&#39;
admin:~ # ceph config set mgr mgr/telemetry/description &#39;Training Cluster&#39;
admin:~ # ceph config set mgr mgr/telemetry/organization &#39;SUSE Training&#39;
</code></pre></div></p> <p>5). Display the telemetry data that is collected to be sent: <div class=highlight><pre><span></span><code>admin:~ # ceph telemetry show | less
</code></pre></div></p> <p>6). With the contact information properly set, enable the telemetry functionality:</p> <p>This is a demo cluster with no connection to the internet, so no telemetry data will actually be sent. <div class=highlight><pre><span></span><code>admin:~ # ceph telemetry on
Error EPERM: Telemetry data is licensed under the Community Data License Agreement - Sharing - Version 1.0 (https://cdla.io/sharing-1-0/).
</code></pre></div></p> <p>To enable, add '--license sharing-1-0' to the 'ceph telemetry on' command. <div class=highlight><pre><span></span><code>admin:~ # ceph telemetry on --license sharing-1-0
</code></pre></div></p> <p>7). Disable the telemetry module: <div class=highlight><pre><span></span><code>admin:~ # ceph mgr module disable telemetry

admin:~ # ceph telemetry show | less
Error ENOTSUP: Module &#39;telemetry&#39; is not enabled (required by command &#39;telemetry show&#39;): use `ceph mgr module enable telemetry` to enable it
</code></pre></div></p> <h5 id=task-4-briefly-attempt-to-use-the-crash-manager-module>Task 4: Briefly attempt to use the crash manager module<a class=headerlink href=#task-4-briefly-attempt-to-use-the-crash-manager-module title="Permanent link"> ¶</a></h5> <p>1). Show (again) the various sub-commands that are associated with the crash command: <div class=highlight><pre><span></span><code>admin:~ # ceph crash
                crash info &lt;id&gt;
                crash ls
                crash ls-new
                crash post
                crash prune &lt;keep&gt;
                crash rm &lt;id&gt;
                crash stat
                crash json_report &lt;hours&gt;
                crash archive &lt;id&gt;
                crash archive-all
</code></pre></div></p> <p>2). Show the current status of the crash database, including the numbgr of crash reports that have been collected so far: It’s likely that the numbgr of crashes recorded in the demo environment is 0. <div class=highlight><pre><span></span><code>admin:~ # ceph crash stat
0 crashes recorded
</code></pre></div></p> <h4 id=226-introduction-to-the-tell-command>2.2.6. Introduction to the Tell command<a class=headerlink href=#226-introduction-to-the-tell-command title="Permanent link"> ¶</a></h4> <p>Tell is a very powerful command within Ceph to control the cluster. You don’t use it everyday, but you need to know how to use it when the occasion to use it arises. </p> <p>It’s mostly an Advanced Command, but exposure to it now reduces the stress of learning about it in a more advanced setting later.</p> <p>Run the tell command in a few different circumstances to control the behavior of various Ceph services.</p> <h5 id=task-1-run-a-benchmark-test-on-an-osd>Task 1: Run a benchmark test on an OSD<a class=headerlink href=#task-1-run-a-benchmark-test-on-an-osd title="Permanent link"> ¶</a></h5> <p>1). Run the following command to run and see the result of a benchmark test on osd.8: <div class=highlight><pre><span></span><code>admin:~ # ceph tell osd.8 bench
{
    &quot;bytes_written&quot;: 1073741824,
    &quot;blocksize&quot;: 4194304,
    &quot;elapsed_sec&quot;: 3.7797023200000002,
    &quot;bytes_per_sec&quot;: 284081055.35676152,
    &quot;iops&quot;: 67.730201567831401
}
</code></pre></div></p> <h5 id=task-2-change-the-protection-setting-regarding-the-deletion-of-pools>Task 2: Change the protection setting regarding the deletion of pools<a class=headerlink href=#task-2-change-the-protection-setting-regarding-the-deletion-of-pools title="Permanent link"> ¶</a></h5> <p>1). The default behavior in Ceph is that you can’t delete pools. Try to delete a pool: The output says that you have to be VERY careful and provide more arguments in order to delete a pool <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool delete rbd_pool
Error EPERM: WARNING: this will *PERMANENTLY DESTROY* all data stored in pool rbd_pool.  If you are *ABSOLUTELY CERTAIN* that is what you want, pass the pool name *twice*, followed by --yes-i-really-really-mean-it.
</code></pre></div></p> <p>2). Try deleting the pool again, this time with the extra arguments: Ceph still won’t let you do it because the mon allow pool delete setting has the value of false. <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool delete rbd_pool rbd_pool --yes-i-really-really-mean-it
Error EPERM: pool deletion is disabled; you must first set the mon_allow_pool_delete config option to true before you can destroy a pool
</code></pre></div></p> <p>3). Show that the mon allow pool delete setting has the value of false: Indeed, the output shows that the value is false. <div class=highlight><pre><span></span><code>admin:~ # ceph config get mon.mon\* mon_allow_pool_delete
false
</code></pre></div></p> <p>4). Change to value of the setting using injectargs: Note that the “-” and “_” characters can be confusing. And note that the setting is preceded with the double “--”. The injected args must be enclosed in single quotes. You could have done this with ceph config set, but this is an alternative way to directly “tell” the cluster to change a setting. <div class=highlight><pre><span></span><code>admin:~ # ceph tell mon.\* injectargs &#39;--mon-allow-pool-delete=true&#39;
mon.mon1: injectargs:mon_allow_pool_delete = &#39;true&#39;
mon.mon2: injectargs:mon_allow_pool_delete = &#39;true&#39;
mon.mon3: injectargs:mon_allow_pool_delete = &#39;true&#39;
</code></pre></div></p> <h3 id=23-ceph-dashboard>2.3. Ceph Dashboard<a class=headerlink href=#23-ceph-dashboard title="Permanent link"> ¶</a></h3> <h4 id=231-access-dashboard>2.3.1. Access Dashboard<a class=headerlink href=#231-access-dashboard title="Permanent link"> ¶</a></h4> <h5 id=task-1-set-the-password-for-the-admin-user-of-the-ceph-dashboard>Task 1: Set the password for the admin user of the Ceph Dashboard<a class=headerlink href=#task-1-set-the-password-for-the-admin-user-of-the-ceph-dashboard title="Permanent link"> ¶</a></h5> <p>1). In a Bash terminal as the root user, show that the Dashboard module is enabled: “dashboard” should be included in the list of “enabled_modules” at the top of the output. <div class=highlight><pre><span></span><code>admin:~ # ceph mgr module ls | more
    &quot;enabled_modules&quot;: [
        &quot;dashboard&quot;,
        &quot;iostat&quot;,
        &quot;pg_autoscaler&quot;,
        &quot;prometheus&quot;,
        &quot;restful&quot;
    ],
</code></pre></div></p> <p>2). Show the valid dashboard users that have already been created by DeepSea during initial deployment: It’s possible that other users will be listed, but at least the “admin” user should be displayed in the output. <div class=highlight><pre><span></span><code>admin:~ # ceph dashboard ac-user-show
[&quot;admin&quot;]
</code></pre></div></p> <p>3). Show the “admin” user’s information as stored in the user database: You can see that the admin user has a password set, but it is stored as a hash. So you don’t really know what the password is, and have no way of discovering it. <div class=highlight><pre><span></span><code>admin:~ # ceph dashboard ac-user-show admin
{&quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &lt;your password&gt;, &quot;roles&quot;: [&quot;administrator&quot;], &quot;name&quot;: null, &quot;email&quot;: null, &quot;lastUpdate&quot;: 1601874928}
</code></pre></div></p> <p>4). Change the “admin” user’s password for the dashboard: This sets the “admin” user’s password to the string: mypassword <div class=highlight><pre><span></span><code>admin:~ # ceph dashboard ac-user-set-password admin mypassword
{&quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &lt;your password&gt;, &quot;roles&quot;: [&quot;administrator&quot;], &quot;name&quot;: null, &quot;email&quot;: null, &quot;lastUpdate&quot;: 1609860842}
admin:~ #
</code></pre></div></p> <h5 id=task-3-visit-the-ceph-dashboard-url>Task 3: Visit the Ceph Dashboard URL<a class=headerlink href=#task-3-visit-the-ceph-dashboard-url title="Permanent link"> ¶</a></h5> <div class=highlight><pre><span></span><code>admin:~ # salt-call grains.get dashboard_creds
    local:
        ----------
        admin:
            &lt;your password&gt;

admin:~ # ceph mgr services
    {
        &quot;dashboard&quot;: &quot;https://mon1.sha.me.corp:8443/&quot;,
        &quot;prometheus&quot;: &quot;http://mon1.sha.me.corp:9283/&quot;
    }


admin:~ # ceph -s
  cluster:
    id:     343ee7d3-232f-4c71-8216-1edbc55ac6e0
    health: HEALTH_WARN
            1 subtrees have overcommitted pool target_size_bytes

  services:
    mon: 3 daemons, quorum mon1,mon2,mon3 (age 9w)
    mgr: mon1(active, since 25m)
    mds: cephfs:1 {0=mon3=up:active} 2 up:standby
    osd: 12 osds: 12 up (since 5h), 12 in (since 3M)
    rgw: 1 daemon active (mon3)

  task status:
    scrub status:
        mds.mon3: idle

  data:
    pools:   9 pools, 244 pgs
    objects: 247 objects, 5.7 KiB
    usage:   14 GiB used, 82 GiB / 96 GiB avail
    pgs:     244 active+clean

  io:
    client:   1.2 KiB/s rd, 1 op/s rd, 0 op/s wr
</code></pre></div> <p>URL: </p> <ul> <li>https://mon1.sha.me.corp:8443/</li> <li>https://10.58.121.186:8443</li> </ul> <h4 id=232-explore-the-dashboard-health-performance-status>2.3.2. Explore the Dashboard Health, Performance, Status<a class=headerlink href=#232-explore-the-dashboard-health-performance-status title="Permanent link"> ¶</a></h4> <p>Dashboard</p> <ul> <li>Status<ul> <li>Cluster Status</li> <li>Monitors</li> <li>OSDs</li> <li>Manager Daemons</li> <li>Hosts</li> <li>Object Gateway</li> <li>Metadata Service</li> <li>iSCSI Gateway</li> </ul> </li> <li>Performance<ul> <li>Client IOPS</li> <li>Client Throughput</li> <li>Client Read/Write</li> <li>Recovery Throughput</li> <li>Scrub</li> </ul> </li> <li>Capacity<ul> <li>Pools</li> <li>Raw Capacity</li> <li>Objects</li> <li>PGs per OSD</li> <li>PG Status</li> </ul> </li> </ul> <p>[SUSE Enterprise Storage Portal</p> <p><img alt="SUSE Enterprise Storage Portal" src=../Assets/1.png></p> <p>Cluster&rarr;Configuration</p> <p><img alt="Cluster-->Configuration" src=../Assets/2.png></p> <p>Cluster&rarr;Manager Modules</p> <p><img alt="Cluster-->Manager Modules" src=../Assets/3.png></p> <p>Pools&rarr;Create Pool</p> <p><img alt="Pools-->Create Pool" src=../Assets/4.png></p> <p><img alt="Pools-->Create Pool" src=../Assets/5.png></p> <p><img alt="Pools-->Create Pool" src=../Assets/6.png></p> <h3 id=24-storage-data-access>2.4. Storage Data Access<a class=headerlink href=#24-storage-data-access title="Permanent link"> ¶</a></h3> <h4 id=241-ensure-the-ses-cluster-is-healthy>2.4.1. Ensure the SES Cluster is Healthy<a class=headerlink href=#241-ensure-the-ses-cluster-is-healthy title="Permanent link"> ¶</a></h4> <h5 id=task-1-check-the-clusters-health>Task 1: Check the Cluster’s health<a class=headerlink href=#task-1-check-the-clusters-health title="Permanent link"> ¶</a></h5> <p>1). Run the following command to check the status (health) of the SES cluster: <div class=highlight><pre><span></span><code>admin:~ # ceph -s
  cluster:
    id:     343ee7d3-232f-4c71-8216-1edbc55ac6e0
    health: HEALTH_WARN
            1 subtrees have overcommitted pool target_size_bytes
            1 pools have too few placement groups

  services:
    mon: 3 daemons, quorum mon1,mon2,mon3 (age 9w)
    mgr: mon1(active, since 18h)
    mds: cephfs:1 {0=mon3=up:active} 2 up:standby
    osd: 12 osds: 12 up (since 23h), 12 in (since 3M)
    rgw: 1 daemon active (mon3)

  task status:
    scrub status:
        mds.mon3: idle

  data:
    pools:   10 pools, 248 pgs
    objects: 247 objects, 5.7 KiB
    usage:   14 GiB used, 82 GiB / 96 GiB avail
    pgs:     248 active+clean

  io:
    client:   1.2 KiB/s rd, 1 op/s rd, 0 op/s wr
</code></pre></div></p> <p>2). Evaluate the output. The cluster in this demonstration environment often doesn’t startup correctly due to the nature of a demo environment and it’s less-predictable resources. Depending on whether any of the following tasks are necessary, followup accordingly to ensure that the cluster is healthy before proceeding with the course lectures or any further exercises.</p> <p>3). Run the following series of commands to restart the Monitor daemons on each of the Monitor nodes: It’s certainly not necessary to restart the monitor daemons on all of the monitor nodes if only one is down. If you prefer, you can take a different approach to starting the daemon on a single monitor node. <div class=highlight><pre><span></span><code>admin:~ # for h in mon1 mon2 mon3; \
do \
ssh $h systemctl restart ceph-mon@$h; \
done
</code></pre></div></p> <p>4). After waiting a few moments for the daemons to restart, check the status again: <div class=highlight><pre><span></span><code>admin:~ # ceph -s
  cluster:
    id:     343ee7d3-232f-4c71-8216-1edbc55ac6e0
    health: HEALTH_WARN
            1 subtrees have overcommitted pool target_size_bytes
            1 pools have too few placement groups

  services:
    mon: 3 daemons, quorum mon1,mon2,mon3 (age 15s)
    mgr: mon1(active, since 21h)
    mds: cephfs:1 {0=mon3=up:active} 2 up:standby
    osd: 12 osds: 12 up (since 26h), 12 in (since 3M)
    rgw: 1 daemon active (mon3)

  task status:
    scrub status:
        mds.mon3: idle

  data:
    pools:   10 pools, 248 pgs
    objects: 247 objects, 5.7 KiB
    usage:   14 GiB used, 82 GiB / 96 GiB avail
    pgs:     248 active+clean

  io:
    client:   767 B/s rd, 0 op/s rd, 0 op/s wr
</code></pre></div></p> <p>5). Run the following series of commands to restart the Manager daemons on each of the Monitor nodes: <div class=highlight><pre><span></span><code>admin:~ # for h in mon1 mon2 mon3; \
do \
ssh $h systemctl restart ceph-mgr@$h; \
done
</code></pre></div></p> <p>6). After waiting a few moments for the daemons to restart, check the status again: <div class=highlight><pre><span></span><code>admin:~ # ceph -s
  cluster:
    id:     343ee7d3-232f-4c71-8216-1edbc55ac6e0
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum mon1,mon2,mon3 (age 8m)
    mgr: mon1(active, since 18s)
    mds: cephfs:1 {0=mon3=up:active} 2 up:standby
    osd: 12 osds: 12 up (since 26h), 12 in (since 3M)
    rgw: 1 daemon active (mon3)

  task status:
    scrub status:
        mds.mon3: idle

  data:
    pools:   10 pools, 248 pgs
    objects: 247 objects, 6.1 KiB
    usage:   14 GiB used, 82 GiB / 96 GiB avail
    pgs:     248 active+clean

  io:
    client:   852 B/s rd, 0 op/s rd, 0 op/s wr
</code></pre></div></p> <p>7). Run the following command to restart the MDS daemon on the MDS node (mon1): <div class=highlight><pre><span></span><code>admin:~ # ssh mon1 systemctl restart ceph-mds@mon1.service
</code></pre></div></p> <p>8). After waiting a few moments for the mds daemon to restart, check the status again: Look for the mds service to be plain active rather than laggy or crashed <div class=highlight><pre><span></span><code>admin:~ # ceph -s
  cluster:
    id:     343ee7d3-232f-4c71-8216-1edbc55ac6e0
    health: HEALTH_WARN
            1 subtrees have overcommitted pool target_size_bytes
            1 pools have too few placement groups

  services:
    mon: 3 daemons, quorum mon1,mon2,mon3 (age 17m)
    mgr: mon1(active, since 8m)
    mds: cephfs:1 {0=mon3=up:active} 2 up:standby
    osd: 12 osds: 12 up (since 26h), 12 in (since 3M)
    rgw: 1 daemon active (mon3)

  task status:
    scrub status:
        mds.mon3: idle

  data:
    pools:   10 pools, 248 pgs
    objects: 247 objects, 6.2 KiB
    usage:   14 GiB used, 82 GiB / 96 GiB avail
    pgs:     248 active+clean

  io:
    client:   852 B/s rd, 0 op/s rd, 0 op/s wr
</code></pre></div></p> <p>9). Verify if the OSDs “up” and running properly. It is only necessary if the output of ceph -s shows that there are fewer than 9 OSDs shown as being “up”. It’s most likely that a storage node is simply not quite fully booted yet, such that the OSD daemons haven’t fully come up. But if you suspect that the solution requires something different than simply waiting a little longer, you should try the following steps.</p> <p>First, identify which server is hosting the down’d OSDs. One way of doing that is with this command: <div class=highlight><pre><span></span><code>admin:~ # ceph osd tree
ID CLASS WEIGHT  TYPE NAME      STATUS REWEIGHT PRI-AFF
-1       0.09357 root default
-9       0.02339     host data1
 2   hdd 0.00780         osd.2      up  1.00000 1.00000
 6   hdd 0.00780         osd.6      up  1.00000 1.00000
10   hdd 0.00780         osd.10     up  1.00000 1.00000
-3       0.02339     host data2
 0   hdd 0.00780         osd.0      up  1.00000 1.00000
 4   hdd 0.00780         osd.4      up  1.00000 1.00000
 9   hdd 0.00780         osd.9      up  1.00000 1.00000
-7       0.02339     host data3
 3   hdd 0.00780         osd.3      up  1.00000 1.00000
 7   hdd 0.00780         osd.7      up  1.00000 1.00000
 8   hdd 0.00780         osd.8      up  1.00000 1.00000
-5       0.02339     host data4
 1   hdd 0.00780         osd.1      up  1.00000 1.00000
 5   hdd 0.00780         osd.5      up  1.00000 1.00000
11   hdd 0.00780         osd.11     up  1.00000 1.00000
</code></pre></div></p> <p>Simply try restarting the storage daemon processes on the affected host, such as with this example: <div class=highlight><pre><span></span><code>admin:~ # ssh data2 systemctl restart ceph-osd@9.service

admin:~ # ceph -s
  cluster:
    id:     343ee7d3-232f-4c71-8216-1edbc55ac6e0
    health: HEALTH_WARN
            1 subtrees have overcommitted pool target_size_bytes
            1 pools have too few placement groups

  services:
    mon: 3 daemons, quorum mon1,mon2,mon3 (age 32m)
    mgr: mon1(active, since 24m)
    mds: cephfs:1 {0=mon3=up:active} 2 up:standby
    osd: 12 osds: 12 up (since 27s), 12 in (since 3M)
    rgw: 1 daemon active (mon3)

  task status:
    scrub status:
        mds.mon3: idle

  data:
    pools:   10 pools, 248 pgs
    objects: 247 objects, 6.2 KiB
    usage:   14 GiB used, 82 GiB / 96 GiB avail
    pgs:     248 active+clean

  io:
    client:   852 B/s rd, 0 op/s rd, 0 op/s wr
</code></pre></div></p> <p>If the OSD daemon processes are being stubborn and uncooperative, you may choose to reboot the storage virtual machine entirely. This is one way to do that: <div class=highlight><pre><span></span><code>admin:~ # ssh data1 systemctl reboot
</code></pre></div></p> <p>After waiting some time for the daemons to get started, verify that all the OSDs are “up” and that the cluster is healthy: <div class=highlight><pre><span></span><code>admin:~ # ceph osd tree
admin:~ # ceph status
</code></pre></div></p> <p>10). Run the following command to restart the RADOS Gateway daemon on the node that is hosting the gateway (mon3): <div class=highlight><pre><span></span><code>admin:~ # ssh mon3 systemctl restart ceph-radosgw@rgw.mon3.service

admin:~ # ssh mon3 systemctl status ceph-radosgw@rgw.mon3.service
● ceph-radosgw@rgw.mon3.service - Ceph rados gateway
   Loaded: loaded (/usr/lib/systemd/system/ceph-radosgw@.service; enabled; vendor preset: disabled)
   Active: active (running) since Wed 2021-01-06 21:37:53 CST; 23s ago
 Main PID: 781880 (radosgw)
    Tasks: 588
   CGroup: /system.slice/system-ceph\x2dradosgw.slice/ceph-radosgw@rgw.mon3.service
           └─781880 /usr/bin/radosgw -f --cluster ceph --name client.rgw.mon3 --setuser ceph --setgroup ceph

Jan 06 21:37:53 mon3 systemd[1]: Started Ceph rados gateway.
</code></pre></div></p> <p>11). After waiting a few moments for the daemon to restart, check the status again: <div class=highlight><pre><span></span><code>admin:~ # ceph -s
  cluster:
    id:     343ee7d3-232f-4c71-8216-1edbc55ac6e0
    health: HEALTH_WARN
            1 subtrees have overcommitted pool target_size_bytes
            1 pools have too few placement groups

  services:
    mon: 3 daemons, quorum mon1,mon2,mon3 (age 39m)
    mgr: mon1(active, since 30m)
    mds: cephfs:1 {0=mon3=up:active} 2 up:standby
    osd: 12 osds: 12 up (since 6m), 12 in (since 3M)
    rgw: 1 daemon active (mon3)

  task status:
    scrub status:
        mds.mon3: idle

  data:
    pools:   10 pools, 248 pgs
    objects: 247 objects, 6.2 KiB
    usage:   14 GiB used, 82 GiB / 96 GiB avail
    pgs:     248 active+clean

  io:
    client:   1.2 KiB/s rd, 1 op/s rd, 0 op/s wr
</code></pre></div></p> <h4 id=242-use-the-s3-api-to-interact-with-the-rados-gateway>2.4.2. Use the S3 API to Interact with the RADOS Gateway<a class=headerlink href=#242-use-the-s3-api-to-interact-with-the-rados-gateway title="Permanent link"> ¶</a></h4> <p>In this lab we used the s3cmd and radosgw-admin utilities to interact with the SUSE Enterprise Storage cluster. We created a new user, a new bucket, and a new file. We then uploaded the file to the cluster and verified that the object gateway stored it to the cluster.</p> <h5 id=task-1-using-the-s3cmd-tool-and-create-an-s3-user>Task 1: Using the s3cmd tool and create an S3 user<a class=headerlink href=#task-1-using-the-s3cmd-tool-and-create-an-s3-user title="Permanent link"> ¶</a></h5> <p>1). As the root user (password is linux) in a shell or terminal, verify that the s3cmd is available on the admin node: You will likely see an error about configuration files missing, etc. This is enough information to validate the utility is installed. <div class=highlight><pre><span></span><code>admin:~ # pip --version
pip 10.0.1 from /usr/lib/python3.6/site-packages/pip (python 3.6)

admin:~ # pip install s3cmd
Collecting s3cmd
  Downloading https://files.pythonhosted.org/packages/26/44/19e08f69b2169003f7307565f19449d997895251c6a6566ce21d5d636435/s3cmd-2.1.0-py2.py3-none-any.whl (145kB)
    100% | 153kB 2.7MB/s
Collecting python-magic (from s3cmd)
  Downloading https://files.pythonhosted.org/packages/59/77/c76dc35249df428ce2c38a3196e2b2e8f9d2f847a8ca1d4d7a3973c28601/python_magic-0.4.18-py2.py3-none-any.whl
Requirement already satisfied: python-dateutil in /usr/lib/python3.6/site-packages (from s3cmd) (2.7.3)
Requirement already satisfied: six&gt;=1.5 in /usr/lib/python3.6/site-packages (from python-dateutil-&gt;s3cmd) (1.11.0)
Installing collected packages: python-magic, s3cmd
Successfully installed python-magic-0.4.18 s3cmd-2.1.0
</code></pre></div></p> <p>2). Create a new S3 user to be used: The output will include an access_key value and a secret_key value. You will need both of those values in later steps. <div class=highlight><pre><span></span><code>admin:~ # radosgw-admin user create --uid=s3user --display-name=S3 User --email=s3user@example.net
{
    &quot;user_id&quot;: &quot;s3user&quot;,
    &quot;display_name&quot;: &quot;S3&quot;,
    &quot;email&quot;: &quot;s3user@example.net&quot;,
    &quot;suspended&quot;: 0,
    &quot;max_buckets&quot;: 1000,
    &quot;subusers&quot;: [],
    &quot;keys&quot;: [
        {
            &quot;user&quot;: &quot;s3user&quot;,
            &quot;access_key&quot;: &lt;your key&gt;,
            &quot;secret_key&quot;: &lt;your key&gt;
        }
    ],
    &quot;swift_keys&quot;: [],
    &quot;caps&quot;: [],
    &quot;op_mask&quot;: &quot;read, write, delete&quot;,
    &quot;default_placement&quot;: &quot;&quot;,
    &quot;default_storage_class&quot;: &quot;&quot;,
    &quot;placement_tags&quot;: [],
    &quot;bucket_quota&quot;: {
        &quot;enabled&quot;: false,
        &quot;check_on_raw&quot;: false,
        &quot;max_size&quot;: -1,
        &quot;max_size_kb&quot;: 0,
        &quot;max_objects&quot;: -1
    },
    &quot;user_quota&quot;: {
        &quot;enabled&quot;: false,
        &quot;check_on_raw&quot;: false,
        &quot;max_size&quot;: -1,
        &quot;max_size_kb&quot;: 0,
        &quot;max_objects&quot;: -1
    },
    &quot;temp_url_keys&quot;: [],
    &quot;type&quot;: &quot;rgw&quot;,
    &quot;mfa_ids&quot;: []
}
</code></pre></div></p> <p>Retrieve above information <div class=highlight><pre><span></span><code>admin:~ # radosgw-admin user info --uid=s3user
</code></pre></div></p> <h5 id=task-2-create-a-new-s3cmd-configuration-file-and-a-new-s3-bucket>Task 2: Create a new s3cmd configuration file and a new S3 bucket<a class=headerlink href=#task-2-create-a-new-s3cmd-configuration-file-and-a-new-s3-bucket title="Permanent link"> ¶</a></h5> <p>1). Generate a new s3cmd configuration file from a shell on the admin node: Fill in as listed below: <div class=highlight><pre><span></span><code>admin:~ # cd ~
admin:~ # s3cmd --configure
</code></pre></div></p> <p>Enter new values or accept defaults in brackets with Enter. Refer to user manual for detailed description of all options.</p> <p>Access key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables. <div class=highlight><pre><span></span><code>Access Key: &lt;your key&gt;
Secret Key: &lt;your key&gt;
Default Region [US]: &lt;leave blank&gt;
</code></pre></div></p> <p>Use "s3.amazonaws.com" for S3 Endpoint and not modify it to the target Amazon S3. <div class=highlight><pre><span></span><code>S3 Endpoint [s3.amazonaws.com]: mon3.sha.me.corp
</code></pre></div></p> <p>Use "%(bucket)s.s3.amazonaws.com" to the target Amazon S3. "%(bucket)s" and "%(location)s" vars can be used if the target S3 system supports dns based buckets. <div class=highlight><pre><span></span><code>DNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: %(bucket)s.mon3.sha.me.corp
</code></pre></div></p> <p>Encryption password is used to protect your files from reading by unauthorized persons while in transfer to S3 <div class=highlight><pre><span></span><code>Encryption password: &lt;leave blank&gt;
Path to GPG program [/usr/bin/gpg]: &lt;leave blank&gt;
</code></pre></div></p> <p>When using secure HTTPS protocol all communication with Amazon S3 servers is protected from 3<sup>rd</sup> party eavesdropping. This method is slower than plain HTTP, and can only be proxied with Python 2.7 or newer <div class=highlight><pre><span></span><code>Use HTTPS protocol [Yes]: No
</code></pre></div></p> <p>On some networks all internet access must go through a HTTP proxy. Try setting it here if you can't connect to S3 directly <div class=highlight><pre><span></span><code>HTTP Proxy server name: &lt;leave blank&gt;
</code></pre></div></p> <p>New settings: <div class=highlight><pre><span></span><code>  Access Key: &lt;your key&gt;
  Secret Key: &lt;your key&gt;
  Default Region: US
  S3 Endpoint: mon3.sha.me.corp
  DNS-style bucket+hostname:port template for accessing a bucket: %(bucket)s.mon3.sha.me.corp
  Encryption password:
  Path to GPG program: /usr/bin/gpg
  Use HTTPS protocol: False
  HTTP Proxy server name:
  HTTP Proxy server port: 0

Test access with supplied credentials? [Y/n] n

Save settings? [y/N] y
Configuration saved to &#39;/root/.s3cfg&#39;
</code></pre></div></p> <p>2). Test the configuration by checking for existing files or directories: Since no buckets or files have been made available for the user, no items are listed and the command returns you to the prompt with no output. This is normal. If there is an error, your configuration may have a typo in it. The configuration file will have been saved as .s3cfg. Edit the file to match the configuration in step one. <div class=highlight><pre><span></span><code>admin:~ # s3cmd ls
</code></pre></div></p> <p>3). Create a new bucket for uploading files to using the s3cmd: You should see feedback that the bucket has been created. Although not technically required by the S3 API, the bucket name needs to be in all uppercase to avoid a bug with the s3cmd tool itself. <div class=highlight><pre><span></span><code>admin:~ # s3cmd mb s3://S3CMDTEST
Bucket &#39;s3://S3CMDTEST/&#39; created

admin:~ # s3cmd ls
2021-01-06 14:04  s3://S3CMDTEST   (it&#39;s GMT timezone)
</code></pre></div></p> <h5 id=task3-create-and-upload-a-file-to-a-bucket-using-the-s3-api>Task3: Create and upload a file to a bucket using the S3 API<a class=headerlink href=#task3-create-and-upload-a-file-to-a-bucket-using-the-s3-api title="Permanent link"> ¶</a></h5> <p>1). Create a file with a few words of text: <div class=highlight><pre><span></span><code>admin:~ # echo &quot;The mountains are beautiful&quot; &gt; newfile
</code></pre></div></p> <p>2). Put the new file into your bucket using s3cmd: You should see the file being uploaded. <div class=highlight><pre><span></span><code>admin:~ # s3cmd put newfile s3://S3CMDTEST
upload: &#39;newfile&#39; -&gt; &#39;s3://S3CMDTEST/newfile&#39;  [1 of 1]
 28 of 28   100% in    3s     7.66 B/s  done
</code></pre></div></p> <p>3). Verify the file is now in your bucket, safely stored in you SES cluster: <div class=highlight><pre><span></span><code>admin:~ # s3cmd ls s3://S3CMDTEST
2021-01-06 14:11           28  s3://S3CMDTEST/newfile
</code></pre></div></p> <h4 id=243-use-the-swift-api-to-interact-with-the-rados-gateway>2.4.3. Use the swift API to Interact with the RADOS Gateway<a class=headerlink href=#243-use-the-swift-api-to-interact-with-the-rados-gateway title="Permanent link"> ¶</a></h4> <p><a href=https://docs.openstack.org/install-guide/environment-packages-obs.html>OpenStack packages for SUSE</a></p> <p><a href=https://docs.openstack.org/swift/latest/install/storage-install-obs.html>Install and configure the storage nodes for openSUSE and SUSE Linux Enterprise</a></p> <p><a href=https://packagehub.suse.com/packages/python-pastedeploy/ >SUSE Package Hub: python-PasteDeploy</a></p> <p>Enable SUSE Package Hub extension <div class=highlight><pre><span></span><code>admin:~ # SUSEConnect -p PackageHub/15.1/x86_64
</code></pre></div></p> <p>Install python3-PasteDeploy, which is dependency of python-swift installation <div class=highlight><pre><span></span><code>admin:~ # zypper in python3-PasteDeploy
admin:~ # rpm -ivh python3-PyECLib-1.6.0-1.6.x86_64.rpm
warning: python3-PyECLib-1.6.0-1.6.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID 3dbdc284: NOKEY
error: Failed dependencies:
        python(abi) = 3.8 is needed by python3-PyECLib-1.6.0-1.6.x86_64
        rpmlib(PayloadIsZstd) &lt;= 5.4.18-1 is needed by python3-PyECLib-1.6.0-1.6.x86_64
</code></pre></div></p> <p>Add OpenStack Swift Repository for SUSE <div class=highlight><pre><span></span><code>admin:~ # zypper addrepo -f obs://Cloud:OpenStack:Train/SLE_15_SP1 Train
admin:~ # zypper in openstack-swift openstack-swift-account openstack-swift-container openstack-swift-object
</code></pre></div></p> <h5 id=task-1-create-a-swift-subuser>Task 1: Create a swift subuser<a class=headerlink href=#task-1-create-a-swift-subuser title="Permanent link"> ¶</a></h5> <p>1). In a shell or terminal as the root user (password of linux) on the admin node, create a new subuser: The output will contain the access and secret keys for the s3user and a secret key for the new swift subuser.. <div class=highlight><pre><span></span><code>admin:~ # radosgw-admin subuser create --uid=s3user --subuser=s3user:swift --access=full
{
    &quot;user_id&quot;: &quot;s3user&quot;,
    &quot;display_name&quot;: &quot;S3&quot;,
    &quot;email&quot;: &quot;s3user@example.net&quot;,
    &quot;suspended&quot;: 0,
    &quot;max_buckets&quot;: 1000,
    &quot;subusers&quot;: [
        {
            &quot;id&quot;: &quot;s3user:swift&quot;,
            &quot;permissions&quot;: &quot;full-control&quot;
        }
    ],
    &quot;keys&quot;: [
        {
            &quot;user&quot;: &quot;s3user&quot;,
            &quot;access_key&quot;: &lt;your key&gt;,
            &quot;secret_key&quot;: &lt;your key&gt;
        }
    ],
    &quot;swift_keys&quot;: [
        {
            &quot;user&quot;: &quot;s3user:swift&quot;,
            &quot;secret_key&quot;: &lt;your key&gt;
        }
    ],
    &quot;caps&quot;: [],
    &quot;op_mask&quot;: &quot;read, write, delete&quot;,
    &quot;default_placement&quot;: &quot;&quot;,
    &quot;default_storage_class&quot;: &quot;&quot;,
    &quot;placement_tags&quot;: [],
    &quot;bucket_quota&quot;: {
        &quot;enabled&quot;: false,
        &quot;check_on_raw&quot;: false,
        &quot;max_size&quot;: -1,
        &quot;max_size_kb&quot;: 0,
        &quot;max_objects&quot;: -1
    },
    &quot;user_quota&quot;: {
        &quot;enabled&quot;: false,
        &quot;check_on_raw&quot;: false,
        &quot;max_size&quot;: -1,
        &quot;max_size_kb&quot;: 0,
        &quot;max_objects&quot;: -1
    },
    &quot;temp_url_keys&quot;: [],
    &quot;type&quot;: &quot;rgw&quot;,
    &quot;mfa_ids&quot;: []
}
</code></pre></div></p> <p>2). Verify that the subuser has access to at least one bucket and list the buckets with a swift command: <div class=highlight><pre><span></span><code>swift -A http://mon3.sha.me.corp/auth/1.0 -U s3user:swift -K &#39;{SECRET_KEY_FROM_STEP_1}&#39; list

admin:~ # swift -A http://mon3.sha.me.corp/auth/1.0 -U s3user:swift -K &#39;&lt;your key&gt;&#39; list
S3CMDTEST
</code></pre></div></p> <h5 id=task-2-use-the-swift-command-to-access-a-file-created-with-the-s3cmd-tool>Task 2: Use the swift command to access a file created with the S3cmd tool<a class=headerlink href=#task-2-use-the-swift-command-to-access-a-file-created-with-the-s3cmd-tool title="Permanent link"> ¶</a></h5> <p>1). Since the S3 API and the swift API are accessing the same SUSE Enterprise Storage cluster, and since the RADOS gateway is built to be inter-operable with both, you can use the swift API to retrieve the object which was uploaded to SES via the S3 API: <div class=highlight><pre><span></span><code>swift -A http://mon3.example.net/auth/1.0 -U s3user:swift -K &#39;{SECRET_KEY_FROM_STEP_1}&#39; download -a 
</code></pre></div></p> <p>An example of the command is listed here: <div class=highlight><pre><span></span><code>admin:~ # swift -A http://mon3.sha.me.corp/auth/1.0 -U s3user:swift -K &#39;&lt;your key&gt;&#39; download -a
</code></pre></div></p> <p>Although we have taken a shortcut by using the -a option (meaning grab every object this user has access to), it illustrates the tool’s capability. We’ve uploaded the newfile with S3, we’ve retrieved it with swift.</p> <h4 id=244-create-snapshots-on-ses-using-rbd>2.4.4. Create Snapshots on SES using RBD<a class=headerlink href=#244-create-snapshots-on-ses-using-rbd title="Permanent link"> ¶</a></h4> <p>In this lab we worked with rbd images. We mapped an rbd image to a Linux device file, then created a filesystem and mounted it. Then we created snapshots to preserve the images data state at a particular time, and rolled it back to demonstrate functionality.</p> <h5 id=task-1-create-a-new-pool-for-rbd-images>Task 1: Create a new pool for RBD images<a class=headerlink href=#task-1-create-a-new-pool-for-rbd-images title="Permanent link"> ¶</a></h5> <p>1). Access https://mon1.pvg.me.corp:8443 or https://10.58.121.186:8443</p> <p>2). Log in with the following credentials:</p> <div class=highlight><pre><span></span><code>Username: admin
Password: mypassword
</code></pre></div> <p>3). Click on the Pools tab near the top of the page</p> <p>4). Click the Create button and use the following in the available fields:</p> <div class=highlight><pre><span></span><code>Name: rbd-images
Pool type: replicated
Placement groups: 16
Crush ruleset: replicated_rule
Replicted size: 2
Applications: rbd
Compression Mode: none
</code></pre></div> <p>5). Click Create Pool</p> <h5 id=task-2-create-a-new-rbd-image-in-the-rbd-images-pool>Task 2: Create a new RBD image in the rbd-images pool<a class=headerlink href=#task-2-create-a-new-rbd-image-in-the-rbd-images-pool title="Permanent link"> ¶</a></h5> <p>6). Create a new RBD image using the rbd command: <div class=highlight><pre><span></span><code>admin:~ # rbd create --size 1024 rbd-images/barfoo
</code></pre></div></p> <p>7). Verify the new image has been created in the rbd-images pool: The new image named barfoo should be displayed. <div class=highlight><pre><span></span><code>admin:~ # rbd ls -p rbd-images
barfoo
</code></pre></div></p> <h5 id=task-3-mount-the-new-image-on-the-admin-node-and-create-a-filesystem>Task 3: Mount the new image on the admin node and create a filesystem<a class=headerlink href=#task-3-mount-the-new-image-on-the-admin-node-and-create-a-filesystem title="Permanent link"> ¶</a></h5> <p>1). As the root user in a shell or terminal on the admin node, map the new rbd image to a block device: <div class=highlight><pre><span></span><code>admin:~ # rbd map rbd-images/barfoo
/dev/rbd0
</code></pre></div></p> <p>2). Create a filesystem on the newly mapped device: <div class=highlight><pre><span></span><code>admin:~ # mkfs.ext4 /dev/rbd0
mke2fs 1.43.8 (1-Jan-2018)
Discarding device blocks: done
Creating filesystem with 262144 4k blocks and 65536 inodes
Filesystem UUID: 19da6b86-1989-4834-a365-2f654fcce6f6
Superblock backups stored on blocks:
        32768, 98304, 163840, 229376

Allocating group tables: done
Writing inode tables: done
Creating journal (8192 blocks): done
Writing superblocks and filesystem accounting information: done
</code></pre></div></p> <p>3). Mount the image to the /mnt directory: <div class=highlight><pre><span></span><code>admin:~ # mount /dev/rbd0 /mnt

admin:~ # l /mnt
total 20
drwxr-xr-x 3 root root  4096 Jan  6 23:48 ./
drwxr-xr-x 1 root root   156 Oct  5 08:53 ../
drwx------ 2 root root 16384 Jan  6 23:48 lost+found/
</code></pre></div></p> <h5 id=task-4-create-a-file-on-the-new-filesystem-and-snapshot-the-rbd-image-and-make-some-additional-changes>Task 4: Create a file on the new filesystem and snapshot the rbd image and make some additional changes<a class=headerlink href=#task-4-create-a-file-on-the-new-filesystem-and-snapshot-the-rbd-image-and-make-some-additional-changes title="Permanent link"> ¶</a></h5> <p>1). Change to the /mnt directory and create a simple file: <div class=highlight><pre><span></span><code>admin:~ # cd /mnt
admin:/mnt # echo &quot;This is some sample text&quot; &gt; start.txt
</code></pre></div></p> <p>2). List the directories contents to see that the start.txt file has been created on the storage cluster. <div class=highlight><pre><span></span><code>admin:/mnt # ls -l
total 20
drwx------ 2 root root 16384 Jan  6 23:48 lost+found
-rw-r--r-- 1 root root    25 Jan  6 23:50 start.txt
</code></pre></div></p> <p>3). Create a snapshot of what the rbd image contained: Wait for confirmation that the snapshot has been created. It should only take a few seconds. <div class=highlight><pre><span></span><code>admin:/mnt # rbd snap create rbd-images/barfoo@begin
</code></pre></div></p> <p>4). List the rbd snapshots for the rbd-images/barfoo image: You should see the new snap called begin listed. <div class=highlight><pre><span></span><code>admin:/mnt # rbd snap ls rbd-images/barfoo
SNAPID NAME  SIZE  PROTECTED TIMESTAMP
     4 begin 1 GiB           Wed Jan  6 23:51:12 2021
</code></pre></div></p> <p>5). Add another file to the filesystem: <div class=highlight><pre><span></span><code>admin:/mnt # echo &quot;Some more text&quot; &gt; end.txt
</code></pre></div></p> <p>6). List the contents of the /mnt to verify the existence of two files. <div class=highlight><pre><span></span><code>admin:/mnt # ls -l
total 24
-rw-r--r-- 1 root root    15 Jan  6 23:52 end.txt
drwx------ 2 root root 16384 Jan  6 23:48 lost+found
-rw-r--r-- 1 root root    25 Jan  6 23:50 start.txt
</code></pre></div></p> <p>7). Create a second snapshot of the rbd-images/barfoo image: <div class=highlight><pre><span></span><code>admin:/mnt # rbd snap create rbd-images/barfoo@finish
</code></pre></div></p> <p>8). List the rbd snapshots: There should be begin and finish snapshots. <div class=highlight><pre><span></span><code>admin:/mnt # rbd snap ls rbd-images/barfoo
SNAPID NAME   SIZE  PROTECTED TIMESTAMP
     4 begin  1 GiB           Wed Jan  6 23:51:12 2021
     5 finish 1 GiB           Wed Jan  6 23:53:15 2021
</code></pre></div></p> <p>9). List the contents of the <code>/mnt</code> directory again and verify the two files.</p> <p>10). Rollback the data to the begin snapshot: This process will be relatively quick because the size of the image is small and we have very little data on it. <div class=highlight><pre><span></span><code>admin:/mnt # rbd snap rollback rbd-images/barfoo@begin
Rolling back to snapshot: 100% complete...done.
</code></pre></div></p> <p>11). Change to the root user’s home directory, then remount the image in order to see that the rbd image has been rolled back: <div class=highlight><pre><span></span><code>admin:/mnt # cd ~
admin:~ # umount /mnt
admin:~ # mount /dev/rbd0 /mnt
</code></pre></div></p> <p>12). List the contents of the /mnt directory to verify that only the start.txt file exists on the image. <div class=highlight><pre><span></span><code>admin:/mnt # ls -l
total 20
drwx------ 2 root root 16384 Jan  6 23:48 lost+found
-rw-r--r-- 1 root root    25 Jan  6 23:50 start.txt
</code></pre></div></p> <p>13). Rollback the data to the finish snapshot: <div class=highlight><pre><span></span><code>admin:/mnt # rbd snap rollback rbd-images/barfoo@finish
Rolling back to snapshot: 100% complete...done.
</code></pre></div></p> <p>14). Unmount and remount the image: <div class=highlight><pre><span></span><code>admin:/mnt # cd ~
admin:~ # umount /mnt
admin:~ # mount /dev/rbd0 /mnt
</code></pre></div></p> <p>15). List the contents of the /mnt directory to show it has indeed been rolled back and contains the start.txt and end.txt files <div class=highlight><pre><span></span><code>admin:/mnt # ls -l
total 24
-rw-r--r-- 1 root root    15 Jan  6 23:52 end.txt
drwx------ 2 root root 16384 Jan  6 23:48 lost+found
-rw-r--r-- 1 root root    25 Jan  6 23:50 start.txt
</code></pre></div></p> <p>16). Change to the root user’s home directory and unmount the image: <div class=highlight><pre><span></span><code>admin:/mnt # cd ~
admin:~ # umount /mnt
</code></pre></div></p> <h4 id=245-create-and-manage-cow-clones-with-rbd>2.4.5. Create and manage COW Clones with rbd<a class=headerlink href=#245-create-and-manage-cow-clones-with-rbd title="Permanent link"> ¶</a></h4> <p>In this lab you will created a new pool and block device image in the pool. You then mapped the block storage to a linux device and took a snapshot. Finally you protected the snapshot from modification. This would be done so the snapshot can be safely used as a parent cow image which can then be cloned to create new virtual machines.</p> <h5 id=task-1-create-a-new-pool>Task 1: Create a new pool<a class=headerlink href=#task-1-create-a-new-pool title="Permanent link"> ¶</a></h5> <p>1). View the current osds and pools: <div class=highlight><pre><span></span><code>admin:~ # ceph osd ls
0
1
2
3
4
5
6
7
8
9
10
11

admin:~ # ceph osd pool ls
iscsi-images
cephfs_data
cephfs_metadata
.rgw.root
default.rgw.control
default.rgw.meta
default.rgw.log
rbd_pool
bucket_pool
EC_RBD_Pool
default.rgw.buckets.index
default.rgw.buckets.data
rbd-images
</code></pre></div></p> <p>2). Create a new pool called cow-pool: <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool create cow-pool 128
pool &#39;cow-pool&#39; created
</code></pre></div></p> <p>3). List the available pools to view the new pool using either of the following commands: <div class=highlight><pre><span></span><code>admin:~ # ceph osd pool ls
iscsi-images
cephfs_data
cephfs_metadata
.rgw.root
default.rgw.control
default.rgw.meta
default.rgw.log
rbd_pool
bucket_pool
EC_RBD_Pool
default.rgw.buckets.index
default.rgw.buckets.data
rbd-images
cow-pool

admin:~ # rados lspools
iscsi-images
cephfs_data
cephfs_metadata
.rgw.root
default.rgw.control
default.rgw.meta
default.rgw.log
rbd_pool
bucket_pool
EC_RBD_Pool
default.rgw.buckets.index
default.rgw.buckets.data
rbd-images
cow-pool
</code></pre></div></p> <p>Task 2: Create a block device image in a pool</p> <p>1). Create a format 2 rbd image called cow-base in the cow-pool storage pool with a size of 1GB *Note that the –image-format statement is optional as format 2 is default <div class=highlight><pre><span></span><code>admin:~ # rbd create -p cow-pool cow-base --size 1024 --image-format 2
</code></pre></div></p> <p>2). Check that the image has been created, that the format is 2 and that layering (COW Clones) is supported <div class=highlight><pre><span></span><code>admin:~ # rbd -p cow-pool list
cow-base
admin:~ # rbd -p cow-pool info cow-base
rbd image &#39;cow-base&#39;:
        size 1 GiB in 256 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 269d5b817222aa
        block_name_prefix: rbd_data.269d5b817222aa
        format: 2
        features: layering
        op_features:
        flags:
        create_timestamp: Thu Jan  7 10:12:31 2021
        access_timestamp: Thu Jan  7 10:12:31 2021
        modify_timestamp: Thu Jan  7 10:12:31 2021
</code></pre></div></p> <p>Task 3: Map the block storage image to a Linux host</p> <p>1). In a shell or terminal as user root open a terminal window. Using the rbd map command, map an rbd device to the block-storage image created above. <div class=highlight><pre><span></span><code>admin:~ # rbd map -p cow-pool --image cow-base
/dev/rbd1
</code></pre></div></p> <p>2). View the mapped block devices <div class=highlight><pre><span></span><code>admin:~ # rbd showmapped
id pool       namespace image    snap device
0  rbd-images           barfoo   -    /dev/rbd0
1  cow-pool             cow-base -    /dev/rbd1
</code></pre></div></p> <p>3). Note the rbd index numbgr (e.g. rbd0, rbd1) associated with the cow-base image: <div class=highlight><pre><span></span><code>RBD ___1____
</code></pre></div></p> <p>4). View the devices in /dev. Note the device name(s) <div class=highlight><pre><span></span><code>admin:~ # ls -l /dev/rbd*
brw-rw---- 1 root disk 252,  0 Jan  6 23:49 /dev/rbd0
brw-rw---- 1 root disk 252, 16 Jan  7 10:14 /dev/rbd1

/dev/rbd:
total 0
drwxr-xr-x 2 root root 60 Jan  7 10:14 cow-pool
drwxr-xr-x 2 root root 60 Jan  6 23:48 rbd-images
</code></pre></div></p> <p>5). View the block device:(use the device numbgr from step above) <div class=highlight><pre><span></span><code>admin:~ # fdisk -l /dev/rbd1
Disk /dev/rbd1: 1 GiB, 1073741824 bytes, 2097152 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 4194304 bytes / 4194304 bytes
</code></pre></div></p> <p>6). Format the device with an ext4 filesystem: <div class=highlight><pre><span></span><code>admin:~ # mkfs.ext4 /dev/rbd1
mke2fs 1.43.8 (1-Jan-2018)
Discarding device blocks: done
Creating filesystem with 262144 4k blocks and 65536 inodes
Filesystem UUID: 64c9a973-cf31-4239-881f-ec5642bf34e3
Superblock backups stored on blocks:
        32768, 98304, 163840, 229376

Allocating group tables: done
Writing inode tables: done
Creating journal (8192 blocks): done
Writing superblocks and filesystem accounting information: done
</code></pre></div></p> <p>7). Mount the block device on the local filesystem: <div class=highlight><pre><span></span><code>admin:~ # mkdir /mnt/cow-base
admin:~ # mount /dev/rbd1 /mnt/cow-base
</code></pre></div></p> <p>8). Test the storage access by creating a file: <div class=highlight><pre><span></span><code>admin:~ # cd /mnt/cow-base
admin:/mnt/cow-base # touch base-image-file
admin:/mnt/cow-base # ls
base-image-file  lost+found
</code></pre></div></p> <h5 id=task-4-snapshot-the-rbd-image-and-protect-the-snapshot>Task 4: Snapshot the rbd image and protect the snapshot<a class=headerlink href=#task-4-snapshot-the-rbd-image-and-protect-the-snapshot title="Permanent link"> ¶</a></h5> <p>1). List the snapshots in the cow-base image: <div class=highlight><pre><span></span><code>admin:/mnt/cow-base # rbd snap ls cow-pool/cow-base
</code></pre></div></p> <p>2). Create a snapshot of the cow-base rbd image which contains the base-image-file file <div class=highlight><pre><span></span><code>admin:/mnt/cow-base # rbd snap create cow-pool/cow-base@base-snap
</code></pre></div></p> <p>3). List the snapshot: <div class=highlight><pre><span></span><code>admin:/mnt/cow-base # rbd snap ls cow-pool/cow-base
SNAPID NAME      SIZE  PROTECTED TIMESTAMP
     4 base-snap 1 GiB           Thu Jan  7 10:37:13 2021
</code></pre></div></p> <p>4). This snapshot will form the parent snapshot for COW clone images so you will now protected it from modification: <div class=highlight><pre><span></span><code>admin:/mnt/cow-base # rbd snap protect cow-pool/cow-base@base-snap

admin:/mnt/cow-base # rbd snap ls cow-pool/cow-base
SNAPID NAME      SIZE  PROTECTED TIMESTAMP
     4 base-snap 1 GiB yes       Thu Jan  7 10:37:13 2021
</code></pre></div></p> <p>Task 5: Create writable COW clones from the parent snapshot</p> <p>1). Create a COW clone from the cow-base with the base-snap snapshot as the parent image <div class=highlight><pre><span></span><code>admin:/mnt/cow-base # rbd clone cow-pool/cow-base@base-snap cow-pool/cow-image1
</code></pre></div></p> <p>2). Check the information for the new image <div class=highlight><pre><span></span><code>admin:/mnt/cow-base # rbd -p cow-pool --image cow-image1 info
rbd image &#39;cow-image1&#39;:
        size 1 GiB in 256 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 26a1209678cad4
        block_name_prefix: rbd_data.26a1209678cad4
        format: 2
        features: layering
        op_features:
        flags:
        create_timestamp: Thu Jan  7 10:38:58 2021
        access_timestamp: Thu Jan  7 10:38:58 2021
        modify_timestamp: Thu Jan  7 10:38:58 2021
        parent: cow-pool/cow-base@base-snap
        overlap: 1 GiB
</code></pre></div></p> <p>3). Note that the image has details of the parent image and overlap</p> <p>4). Repeat steps 2 &amp; 3 for an additional image called cow-image2 <div class=highlight><pre><span></span><code>admin:/mnt/cow-base # rbd clone cow-pool/cow-base@base-snap cow-pool/cow-image2

admin:/mnt/cow-base # rbd -p cow-pool --image cow-image2 info
rbd image &#39;cow-image2&#39;:
        size 1 GiB in 256 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 26a2fbcec7b8d9
        block_name_prefix: rbd_data.26a2fbcec7b8d9
        format: 2
        features: layering
        op_features:
        flags:
        create_timestamp: Thu Jan  7 10:47:28 2021
        access_timestamp: Thu Jan  7 10:47:28 2021
        modify_timestamp: Thu Jan  7 10:47:28 2021
        parent: cow-pool/cow-base@base-snap
        overlap: 1 GiB
</code></pre></div></p> <p>Task 6: Test that the COW clones are functional</p> <p>1). Create a new directory and mount the COW clone called cow-image1 Note the rbd device name and use it to mount the file system <div class=highlight><pre><span></span><code>admin:/mnt # mkdir /mnt/cow-image1

admin:/mnt # rbd map -p cow-pool --image cow-image1
/dev/rbd2

admin:/mnt # l
total 4
drwxr-xr-x 1 root root   36 Jan  7 10:54 ./
drwxr-xr-x 1 root root  156 Oct  5 08:53 ../
drwxr-xr-x 3 root root 4096 Jan  7 10:19 cow-base/
drwxr-xr-x 1 root root    0 Jan  7 10:54 cow-image1/

admin:/mnt # ls -l /dev/rbd*
brw-rw---- 1 root disk 252,  0 Jan  6 23:49 /dev/rbd0
brw-rw---- 1 root disk 252, 16 Jan  7 10:18 /dev/rbd1
brw-rw---- 1 root disk 252, 32 Jan  7 10:55 /dev/rbd2

/dev/rbd:
total 0
drwxr-xr-x 2 root root 80 Jan  7 10:55 cow-pool
drwxr-xr-x 2 root root 60 Jan  6 23:48 rbd-images

admin:/mnt # mount /dev/rbd2 /mnt/cow-image1
</code></pre></div></p> <p>2). Check that the base-image-file which was created in the parent snapshot is present <div class=highlight><pre><span></span><code>admin:/mnt # cd /mnt/cow-image1

admin:/mnt/cow-image1 # ls
base-image-file  lost+found
</code></pre></div></p> <p>3). Repeat steps 1 and 2 for cow-image2 <div class=highlight><pre><span></span><code>admin:/mnt # mkdir /mnt/cow-image2

admin:/mnt # rbd map -p cow-pool --image cow-image2
/dev/rbd3

admin:/mnt # mount /dev/rbd3 /mnt/cow-image2

admin:/mnt # ls ./cow-image2/
base-image-file  lost+found   --&gt; same file with image1
</code></pre></div></p> <p>4). Create a new file in the directory where cow-image1 is mounted <div class=highlight><pre><span></span><code>admin:/mnt # cd cow-image1

admin:/mnt/cow-image1 # touch additional-file

admin:/mnt/cow-image1 # ls
additional-file  base-image-file  lost+found
</code></pre></div></p> <p>5). Look in the cow-image2 directory. Although they share the same parent snapshot, you can see that the files contained in each COW image are now different. <div class=highlight><pre><span></span><code>admin:/mnt # ls ./cow-image2/
base-image-file  lost+found   
</code></pre></div></p> <h5 id=task-7-flatten-a-cow-clone-and-remove-the-parent-image>Task 7: Flatten a COW Clone and remove the parent image<a class=headerlink href=#task-7-flatten-a-cow-clone-and-remove-the-parent-image title="Permanent link"> ¶</a></h5> <p>1). Convert the COW clone called cow-image1 to a standalone rbd image Wait while the flatten process completes. Unlike a clone process this is not instantaneous and can take considerable time. <div class=highlight><pre><span></span><code>admin:/mnt # rbd flatten cow-pool/cow-image1
Image flatten: 100% complete...done.
</code></pre></div></p> <p>2). Check to see that the flatten process has removed the link to the parent snapshot <div class=highlight><pre><span></span><code>admin:/mnt # rbd -p cow-pool --image cow-image1 info
rbd image &#39;cow-image1&#39;:
        size 1 GiB in 256 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 26a1209678cad4
        block_name_prefix: rbd_data.26a1209678cad4
        format: 2
        features: layering
        op_features:
        flags:
        create_timestamp: Thu Jan  7 10:38:58 2021
        access_timestamp: Thu Jan  7 10:38:58 2021
        modify_timestamp: Thu Jan  7 10:38:58 2021


admin:/mnt # rbd -p cow-pool --image cow-image2 info
rbd image &#39;cow-image2&#39;:
        size 1 GiB in 256 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 26a2fbcec7b8d9
        block_name_prefix: rbd_data.26a2fbcec7b8d9
        format: 2
        features: layering
        op_features:
        flags:
        create_timestamp: Thu Jan  7 10:47:28 2021
        access_timestamp: Thu Jan  7 10:47:28 2021
        modify_timestamp: Thu Jan  7 10:47:28 2021
        parent: cow-pool/cow-base@base-snap
        overlap: 1 GiB
</code></pre></div></p> <p>3). Unmount the images <div class=highlight><pre><span></span><code>admin:/mnt # umount /mnt/cow-image1
admin:/mnt # umount /mnt/cow-image2
admin:/mnt # umount /mnt/cow-base
</code></pre></div></p> <h4 id=246-configure-iscsi-on-ses>2.4.6. Configure iSCSI on SES<a class=headerlink href=#246-configure-iscsi-on-ses title="Permanent link"> ¶</a></h4> <p>In this lab an iSCSI Target was configured via the iSCSI gateway on our SUSE Enterprise Storage. An image was added to it. An iSCSI initiator then connected to the target, created a filesystem, and mounted it.</p> <h5 id=task-1-create-a-new-rbd-image-in-the-iscsi-images-pool>Task 1: Create a new RBD image in the iscsi-images pool<a class=headerlink href=#task-1-create-a-new-rbd-image-in-the-iscsi-images-pool title="Permanent link"> ¶</a></h5> <p>1). Create a new RBD image using the rbd command: <div class=highlight><pre><span></span><code>admin:~ # rbd create --size 1024 iscsi-images/fooiscsi
</code></pre></div></p> <p>2). Verify the new image has been created in the iscsi-images pool: The new image named fooiscsi should be displayed. <div class=highlight><pre><span></span><code>admin:~ # rbd ls iscsi-images
fooiscsi
</code></pre></div></p> <h5 id=task-2-define-a-new-iscsi-target-with-the-ceph-dashboard>Task 2: Define a new iSCSI target with the Ceph Dashboard<a class=headerlink href=#task-2-define-a-new-iscsi-target-with-the-ceph-dashboard title="Permanent link"> ¶</a></h5> <p>1). Access the Ceph Dashboard and log in: https://mon1.pvg.me.corp:8443 or https://10.58.121.186:8443</p> <div class=highlight><pre><span></span><code>Username: admin
Password: mypassword
</code></pre></div> <p>2). Once logged in, click on the Block drop-down item near the top. Select iSCSI.</p> <p>3). With the Overview tab showing for iSCSI, click on the Targets tab near the top.</p> <p>Note:</p> <div class=highlight><pre><span></span><code>When clicking on the Targets tab, if you see an error that says something about “Unsupported `ceph-iscsi` config version…”, perform the following steps:
1. Close the browser window where the error occurred

2. Restart the mon1 virtual machine. Do this with the following steps: 
from the Virtual Machine Manager on your lab machine (not in the admin virtual machine), 
restart the mon1 virtual machine by right-clicking on the mon1 virtual machine &gt; Shut Down &gt; Reboot

3. Wait at least 30 seconds, then from the admin node, open up the browser again and log in to the Ceph Dashboard:
https://mon1.pvg.me.corp:8443 or https://10.58.121.186:8443

    Username: admin
    Password: mypassword

4. Continue the lab as directed below by navigating to the Block &gt; iSCSI section, clicking on the Targets tab, and completing the steps below
</code></pre></div> <p>4). Click Add. Use the following values:</p> <div class=highlight><pre><span></span><code>Target IQN: &lt;accept default&gt;
Portals: mon2.example.net:172.17.6.132
Images: iscsi-images/fooiscsi
ACL authentication: &lt;leave unchecked&gt;
Click Create Target.
</code></pre></div> <p><img alt="Crete Target" src=../Assets/7.png></p> <h5 id=task-3-access-the-new-iscsi-target-from-the-admin-node>Task 3: Access the new iSCSI target from the admin node<a class=headerlink href=#task-3-access-the-new-iscsi-target-from-the-admin-node title="Permanent link"> ¶</a></h5> <p>1). On the admin node, launch YaST either the ncurses or GUI interface, and select the iSCSI Initiator module:</p> <div class=highlight><pre><span></span><code>YaST &gt; Network Services &gt; iSCSI Initiator
</code></pre></div> <p>2). Select the Discovered Targets tab (alt-v)</p> <p>3). Select Discovery at the bottom of the frame (alt-d)</p> <p>4). Add the ip address of mon2: 10.58.121.187. Leave the port as the default of 3260. Select Next.</p> <p>5). Once again on the Discovered Targets tab, the mon2 target should be listed. With the new target highlighted, select Connect (alt-e) at the bottom of the frame.</p> <p>6). Leave the Startup (in YaST2) or On boot (in YaST) value as manual. Select Next.</p> <p>7). Select OK to exit the iscsi client configuration module</p> <p>8). To verify that the iscsi device is now connected, use the lsscsi command to list devices: You should see there is one disk of type RBD connected on a device file similar to the following: <div class=highlight><pre><span></span><code>admin:~ # lsscsi
[0:0:0:0]    cd/dvd  QEMU     QEMU DVD-ROM     1.4.  /dev/sr0
[2:0:0:0]    disk    SUSE     RBD              4.0   /dev/sda
</code></pre></div></p> <p>9). Create an ext4 filesystem on the connected device file: <div class=highlight><pre><span></span><code>admin:~ # mkfs.ext4 /dev/sda
mke2fs 1.43.8 (1-Jan-2018)
Creating filesystem with 262144 4k blocks and 65536 inodes
Filesystem UUID: e3896f7e-0664-4b14-85db-0f77cb234c43
Superblock backups stored on blocks:
        32768, 98304, 163840, 229376

Allocating group tables: done
Writing inode tables: done
Creating journal (8192 blocks): done
Writing superblocks and filesystem accounting information: done
</code></pre></div></p> <p>10). Mount the device to /mnt: <div class=highlight><pre><span></span><code>admin:~ # mount /dev/sda /mnt
</code></pre></div></p> <p>11). Use the mount command to list the connected device: <div class=highlight><pre><span></span><code>admin:/mnt # mount | grep sda
/dev/sda on /mnt type ext4 (rw,relatime,stripe=1024,data=ordered)
</code></pre></div></p> <p>12).Change the root user’s home directory and unmount the device: <div class=highlight><pre><span></span><code>admin:/mnt # cd ..
admin:/ # umount /mnt
</code></pre></div></p> <h4 id=247-mount-cephfs-provided-by-suse-enterprise-storage>2.4.7. Mount CephFS Provided by SUSE Enterprise Storage<a class=headerlink href=#247-mount-cephfs-provided-by-suse-enterprise-storage title="Permanent link"> ¶</a></h4> <p>In this lab a ceph user was configured to mount the ceph filesystem provided by the SUSE Enterprise Cluster. A keyfile was generated, then used in the process.</p> <h5 id=task-1-verify-cephfs-configuration-of-the-ses-cluster>Task 1: Verify cephfs configuration of the SES cluster<a class=headerlink href=#task-1-verify-cephfs-configuration-of-the-ses-cluster title="Permanent link"> ¶</a></h5> <p>1). Cephfs requires two pools for operation: one for data, the other for metadata. Verify that the cluster has two pools for this purpose: <div class=highlight><pre><span></span><code>admin:~ # ceph fs ls
name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]
</code></pre></div></p> <p>Task 2: Create a secret key file for the admin user on the admin node</p> <p>1). Because cephx authentication is enabled by default on SUSE Enterprise Storage, a secret key will need to be provided to allow access to mount the ceph filesystem. The admin user (identified – in this case – on the system as root) on the admin node has a key, but we will need to either provide it on the command line during the mount process (less secure), or put it in a permissions-restricted file and point to the file when mounting (more secure). If we do not specify the key or a file with the key, we will get an error. The following command will return an error: <div class=highlight><pre><span></span><code>admin:~ # mount -t ceph mon1:6789:/ /mnt
2021-01-07 14:16:36.924 7f45108a9d80 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.guest.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory
mount error 22 = Invalid argument
</code></pre></div></p> <p>2). Take secret key value found in the <code>/etc/ceph/ceph.client.admin.keyring</code> file and put it in a new file: <div class=highlight><pre><span></span><code>admin:~ # cat /etc/ceph/ceph.client.admin.keyring
[client.admin]
        key = &lt;your key&gt;
        caps mds = &quot;allow *&quot;
        caps mon = &quot;allow *&quot;
        caps osd = &quot;allow *&quot;
        caps mgr = &quot;allow *&quot;
</code></pre></div></p> <p>3). Create a new file and paste the secret key value into it: <div class=highlight><pre><span></span><code>admin:~ # vi /etc/ceph/admin.secret
Put the key value &lt;your key&gt; into the file and save it.
</code></pre></div></p> <p>4). Change the permissions of the file to be read only by the user: <div class=highlight><pre><span></span><code>admin:~ # ls -l /etc/ceph/admin.secret
-r-------- 1 root root 41 Jan  5 20:05 /etc/ceph/admin.secret
</code></pre></div></p> <h5 id=task-3-mount-the-ceph-filesystem-on-the-admin-node>Task 3: Mount the ceph filesystem on the admin node<a class=headerlink href=#task-3-mount-the-ceph-filesystem-on-the-admin-node title="Permanent link"> ¶</a></h5> <p>1). Now that the keyfile is created, we can mount the filesystem: <div class=highlight><pre><span></span><code>admin:~ # mount -t ceph mon1:6789:/ /mnt -o name=admin,secretfile=/etc/ceph/admin.secret

admin:~ # ls -l /mnt
total 0
</code></pre></div></p> <p>2). Verify that the mount shows as expected: <div class=highlight><pre><span></span><code>admin:~ # mount | grep ceph
10.58.121.186:6789:/ on /mnt type ceph (rw,relatime,name=admin,secret=&lt;hidden&gt;,acl)
</code></pre></div></p> <p>3). Change to the root user’s home directory and unmount the filesystem: <div class=highlight><pre><span></span><code>admin:~ # cd ~
admin:~ # umount /mnt
</code></pre></div></p> <h4 id=248-export-an-nfs-share-from-ses-with-nfs-ganesha>2.4.8. Export an NFS Share from SES with NFS Ganesha<a class=headerlink href=#248-export-an-nfs-share-from-ses-with-nfs-ganesha title="Permanent link"> ¶</a></h4> <h5 id=task-0-install-and-configure-ganesha-ganesha-config-location-is-not-configured-please-set-the-ganesha_rados_pool_namespace-setting>Task 0: Install and configure Ganesha (Ganesha config location is not configured. Please set the GANESHA_RADOS_POOL_NAMESPACE setting.)<a class=headerlink href=#task-0-install-and-configure-ganesha-ganesha-config-location-is-not-configured-please-set-the-ganesha_rados_pool_namespace-setting title="Permanent link"> ¶</a></h5> <div class=highlight><pre><span></span><code>admin:~ # zypper in nfs-ganesha

admin:/etc/ganesha # cat ganesha.conf
    NFSv4 {
            RecoveryBackend = &#39;rados_cluster&#39;;
            #RecoveryBackend = &#39;rados_ng&#39;;
    }

    RADOS_URLS {
            ceph_conf = &#39;/etc/ceph/ceph.conf&#39;;
            userid = &quot;admin&quot;;
            watch_url = &quot;rados://data/ganesha-export-index/conf-nfs1&quot;;
    }

    RADOS_KV {
            pool = &quot;metadata&quot;;
            namespace = &quot;ganesha-grace&quot;;
            nodeid = &quot;nfs1&quot;;
    }

    %url rados://data/ganesha-export-index/conf-nfs1

admin:/etc/ganesha # ganesha-rados-grace -p metadata -n ganesha-grace add nfs1 nfs2 nfs3

admin:/etc/ganesha # ganesha-rados-grace -p metadata -n ganesha-grace
cur=1 rec=0
======================================================
nfs1     E
nfs2     E
nfs3     E


http://images.45drives.com/ceph/cephfs/nfs-ganesha-ceph.conf
</code></pre></div> <h5 id=task-1-create-an-nfs-export-using-the-ceph-dashboard>Task 1: Create an NFS export using the Ceph Dashboard<a class=headerlink href=#task-1-create-an-nfs-export-using-the-ceph-dashboard title="Permanent link"> ¶</a></h5> <p>1). In a browser, navigate to a monitor to access the Ceph Dashboard and log in: https://10.58.121.186:8443/</p> <div class=highlight><pre><span></span><code>Username: admin
Password: mypassword
</code></pre></div> <p>2). Click on the NFS tab near the top of the page</p> <p>3). Click on the green Add button</p> <p>4). Click on the Add daemon button to the right and select mon1</p> <p>5). Complete the configuration with the following values:</p> <div class=highlight><pre><span></span><code>Storage Backend: Object Gateway
Object Storage User: s3user
Path: S3CMDTEST
NFS Protocol: NFSv3 and NFSv4 checked
NFS Tag: &lt;leave blank&gt;
Pseudo: /S3BKT
Access Type: RW
Squash: root_squash
Transport Protocol: UDP and TCP checked
Clients: &lt;leave default&gt;
Click Submit
</code></pre></div> <h5 id=task-2-mount-the-nfs-export-on-the-admin-node>Task 2: Mount the NFS export on the admin node<a class=headerlink href=#task-2-mount-the-nfs-export-on-the-admin-node title="Permanent link"> ¶</a></h5> <p>1). As the root user on the admin node, query the NFS Ganesha gateway node to see what mounts are available: <div class=highlight><pre><span></span><code>showmount -e mon1
</code></pre></div> You should see something similar to the following: <div class=highlight><pre><span></span><code>Export list for mon1:
S3CMDTEST (everyone)
</code></pre></div></p> <p>2). Mount the available nfs share to the /mnt directory on the admin server: <div class=highlight><pre><span></span><code>mount -t nfs mon1:/S3BKT /mnt
</code></pre></div></p> <p>3). List the nfs mount: <div class=highlight><pre><span></span><code>mount | grep mnt
</code></pre></div></p> <p>Note the type listed as nfs4</p> <p>4). Change to the root user’s home directory and unmount the export: <div class=highlight><pre><span></span><code>cd
umount /mnt
</code></pre></div></p> <h4 id=249-configure-and-mount-cifs>2.4.9. Configure and Mount CIFS<a class=headerlink href=#249-configure-and-mount-cifs title="Permanent link"> ¶</a></h4> <p>In this lab the Samba gateway was configured. A keyring for the Samba gateway was created, the Samba service was modified, and a user created to allow CIFS access to the SES cluster.</p> <h5 id=task-1-prepare-the-cephfs-share-for-cifs>Task 1: Prepare the CephFS share for CIFS<a class=headerlink href=#task-1-prepare-the-cephfs-share-for-cifs title="Permanent link"> ¶</a></h5> <p>1). In order for CIFS to work in our SES environment, a valid CephFS share must be available. The CephFS lab previously done in this workbook is sufficient. Using the same configuration that we used previously, mount the CephFS share and give permissions to all users at the root of the share: <div class=highlight><pre><span></span><code>admin:~ # mount -t ceph mon1:6789:/ /mnt -o name=admin,secretfile=/etc/ceph/admin.secret

admin:~ # chmod 777 /mnt

admin:~ # l /mnt
total 0
drwxrwxrwx 2 root root   0 Oct  5 14:30 ./
drwxr-xr-x 1 root root 156 Oct  5 08:53 ../

admin:~ # umount /mnt
</code></pre></div></p> <h5 id=task-2-create-a-samba-gateway-specific-keyring-on-the-ceph-admin-node-and-copy-it-to-the-samba-gateway-node>Task 2: Create a Samba gateway specific keyring on the Ceph admin node and copy it to the Samba gateway node<a class=headerlink href=#task-2-create-a-samba-gateway-specific-keyring-on-the-ceph-admin-node-and-copy-it-to-the-samba-gateway-node title="Permanent link"> ¶</a></h5> <p>1). A new keyring will be needed for the Samba gateway to allow access to the Ceph cluster. As root, perform the following: <div class=highlight><pre><span></span><code>admin:~ # ceph auth get-or-create client.samba.gw mon &#39;allow r&#39; osd &#39;allow *&#39; mds &#39;allow *&#39; -o ceph.client.samba.gw.keyring
</code></pre></div></p> <p>2). Copy the new keyring to the Samba gateway node: <div class=highlight><pre><span></span><code>admin:~ # scp ceph.client.samba.gw.keyring mon1:/etc/ceph/
ceph.client.samba.gw.keyring

admin:~ # ssh mon1
Last login: Thu Jan  7 14:35:58 2021 from 10.58.121.181

mon1:~ # ls -l /etc/ceph/
total 12
-rw-r--r-- 1 root root   66 Jan  7 15:15 ceph.client.samba.gw.keyring
-rw-r--r-- 1 root root 1095 Jan  5 22:44 ceph.conf
-rw-r--r-- 1 root root   92 Aug 24 22:03 rbdmap
</code></pre></div></p> <h5 id=task-3-configure-samba-on-the-samba-gateway-node>Task 3: Configure Samba on the Samba gateway node<a class=headerlink href=#task-3-configure-samba-on-the-samba-gateway-node title="Permanent link"> ¶</a></h5> <p>1). The /etc/samba/smb.conf file will need to be edited to allow CIFS access to the storage cluster. On the mon1 node, replace all of the contents of the file with the following: <div class=highlight><pre><span></span><code>admin:~ # ssh mon1

mon1:~ # vi /etc/samba/smb.conf
mon1:/etc/samba # cat smb.conf
    [global]
    netbios name = SAMBA-GW
    clustering = no
    idmap config * : backend = tdb2
    passdb backend = tdbsam
    # disable print server
    load printers = no
    smbd: backgroundqueue = no

    [ceph-smb]
    path = /
    vfs objects = ceph
    ceph: config_file = /etc/ceph/ceph.conf
    ceph: user_id = samba.gw
    read only = no
    oplocks = no
    kernel share modes = no
</code></pre></div></p> <p>2). Create a smb user on the mon1 node named joesmb with a password of mypassword: <div class=highlight><pre><span></span><code>mon1:/etc/samba # useradd joesmb
mon1:/etc/samba # passwd joesmb     ---&gt; 123
</code></pre></div></p> <p>Add joesmb to the smb password database with a password of mypassword: <div class=highlight><pre><span></span><code>mon1:/etc/samba # smbpasswd -a joesmb
New SMB password:     ---&gt; 123
Retype new SMB password:     ---&gt; 123
Added user joesmb.
</code></pre></div></p> <p>3). Start and enable the smb and nmb daemons on mon1: <div class=highlight><pre><span></span><code>mon1:/etc/samba # systemctl start smb nmb

mon1:/etc/samba # systemctl enable smb nmb
Created symlink /etc/systemd/system/multi-user.target.wants/smb.service → /usr/lib/systemd/system/smb.service.
Created symlink /etc/systemd/system/multi-user.target.wants/nmb.service → /usr/lib/systemd/system/nmb.service.
</code></pre></div></p> <p>4). Unmount the filesystem: <div class=highlight><pre><span></span><code>mon1:~ # umount /mnt
umount: /mnt: not mounted.
</code></pre></div></p> <h5 id=task-4-connect-a-client-to-the-samba-gateway>Task 4: Connect a client to the Samba gateway<a class=headerlink href=#task-4-connect-a-client-to-the-samba-gateway title="Permanent link"> ¶</a></h5> <p>1). On the admin node, verify that the Samba gateway is sharing via CIFS. The password is 123 <div class=highlight><pre><span></span><code>admin:~ # smbclient -U joesmb -L //mon1
Enter WORKGROUP\joesmb&#39;s password:  ---&gt; 123

        Sharename       Type      Comment
        ---------       ----      -------
        ceph-smb        Disk
        IPC$            IPC       IPC Service (Samba 4.9.5-git.373.26895a83dbf3.44.1-SUSE-oS15.0-x86_64)
Reconnecting with SMB1 for workgroup listing.

        Server               Comment
        ---------            -------

        Workgroup            Master
        ---------            -------
        GLOBAL               CNPVGVSYB900
        WORKGROUP            SAMBA-GW
</code></pre></div></p> <p>2). Connect to the ceph-smb share as joesmb. The password is 123 <div class=highlight><pre><span></span><code>admin:~ # smbclient -U joesmb //mon1/ceph-smb
Enter WORKGROUP\joesmb&#39;s password:   ---&gt; 123
tree connect failed: NT_STATUS_BAD_NETWORK_NAME
</code></pre></div></p> <p>You should see output similar to the following: Try “help” to get a list of possible commands. <div class=highlight><pre><span></span><code>smb: \&gt;
</code></pre></div></p> </article> </div> </div> <a href=# class="md-top md-icon" data-md-component=top data-md-state=hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg> Back to top </a> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../linux_ses_memo/ class="md-footer__link md-footer__link--prev" aria-label="Previous: SUSE Enterprise Storage Foundation" rel=prev> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </div> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Previous </span> SUSE Enterprise Storage Foundation </div> </div> </a> <a href=../../../k8s/ class="md-footer__link md-footer__link--next" aria-label="Next: Index" rel=next> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Next </span> Index </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.instant", "navigation.top"], "search": "../../../assets/javascripts/workers/search.fcfe8b6d.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "version": null}</script> <script src=../../../assets/javascripts/bundle.b1047164.min.js></script> </body> </html>