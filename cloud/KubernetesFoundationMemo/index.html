
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="canonical" href="https://huyuhui001.github.io/mySite/index.html/cloud/KubernetesFoundationMemo/">
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-7.3.6">
    
    
      
        <title>Kubernetes Foundation - MEMO</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.a57b2b03.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.3f5d1f46.min.css">
        
          
          
          <meta name="theme-color" content="#2094f3">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
    
      


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="blue" data-md-color-accent="deep-blue">
  
    
    <script>function __prefix(e){return new URL("../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#kubernetes-foundation" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="MEMO" class="md-header__button md-logo" aria-label="MEMO" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MEMO
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Kubernetes Foundation
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="MEMO" class="md-nav__button md-logo" aria-label="MEMO" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    MEMO
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../linux/" class="md-nav__link">
        Linux
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../python/" class="md-nav__link">
        Python
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        Cloud
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../about/" class="md-nav__link">
        About
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#docker-fundamentals" class="md-nav__link">
    Docker Fundamentals
  </a>
  
    <nav class="md-nav" aria-label="Docker Fundamentals">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#demo-environment" class="md-nav__link">
    Demo environment
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#linux-primitives" class="md-nav__link">
    Linux Primitives
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#installing-docker" class="md-nav__link">
    Installing Docker
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#container-lifecycle" class="md-nav__link">
    Container lifecycle
  </a>
  
    <nav class="md-nav" aria-label="Container lifecycle">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    Overview
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ports-and-volumes" class="md-nav__link">
    Ports and volumes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dockerfile" class="md-nav__link">
    Dockerfile
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-stage-dockerfile" class="md-nav__link">
    Multi-stage Dockerfile
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basic-concepts-of-kubernetes" class="md-nav__link">
    Basic Concepts of Kubernetes
  </a>
  
    <nav class="md-nav" aria-label="Basic Concepts of Kubernetes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#kubernetes-components" class="md-nav__link">
    Kubernetes Components
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kubernetes-api" class="md-nav__link">
    Kubernetes API
  </a>
  
    <nav class="md-nav" aria-label="Kubernetes API">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#api-version" class="md-nav__link">
    API Version
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#api-group" class="md-nav__link">
    API Group
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kubernetes-objects" class="md-nav__link">
    Kubernetes Objects
  </a>
  
    <nav class="md-nav" aria-label="Kubernetes Objects">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#objects-overview" class="md-nav__link">
    Objects Overview:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#object-management" class="md-nav__link">
    Object Management:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#object-names-and-ids" class="md-nav__link">
    Object Names and IDs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#namespaces" class="md-nav__link">
    Namespaces
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#labels-and-selectors" class="md-nav__link">
    Labels and Selectors
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#annotations" class="md-nav__link">
    Annotations
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#field-selectors" class="md-nav__link">
    Field Selectors
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#finalizers" class="md-nav__link">
    Finalizers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#owners-and-dependents" class="md-nav__link">
    Owners and Dependents
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#resource" class="md-nav__link">
    Resource
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#workload-resources" class="md-nav__link">
    Workload Resources
  </a>
  
    <nav class="md-nav" aria-label="Workload Resources">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pods" class="md-nav__link">
    Pods
  </a>
  
    <nav class="md-nav" aria-label="Pods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#initcontainer" class="md-nav__link">
    InitContainer
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#static-pod" class="md-nav__link">
    Static Pod
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#container-probes" class="md-nav__link">
    Container probes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deployment" class="md-nav__link">
    Deployment
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#replicaset" class="md-nav__link">
    ReplicaSet
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#statefulset" class="md-nav__link">
    StatefulSet
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#daemonset" class="md-nav__link">
    DaemonSet
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#job" class="md-nav__link">
    Job
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cronjob" class="md-nav__link">
    CronJob
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#service-resource" class="md-nav__link">
    Service Resource
  </a>
  
    <nav class="md-nav" aria-label="Service Resource">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#service" class="md-nav__link">
    Service
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#endpoints" class="md-nav__link">
    Endpoints
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#tutorials" class="md-nav__link">
    Tutorials
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="kubernetes-foundation">Kubernetes Foundation</h1>
<h2 id="docker-fundamentals">Docker Fundamentals</h2>
<h3 id="demo-environment">Demo environment</h3>
<p>Linux: openSUSE 15.3</p>
<pre><code>james@lizard:/opt&gt; cat /etc/os-release 
NAME=&quot;openSUSE Leap&quot;
VERSION=&quot;15.3&quot;
ID=&quot;opensuse-leap&quot;
ID_LIKE=&quot;suse opensuse&quot;
VERSION_ID=&quot;15.3&quot;
PRETTY_NAME=&quot;openSUSE Leap 15.3&quot;
ANSI_COLOR=&quot;0;32&quot;
CPE_NAME=&quot;cpe:/o:opensuse:leap:15.3&quot;
BUG_REPORT_URL=&quot;https://bugs.opensuse.org&quot;
HOME_URL=&quot;https://www.opensuse.org/&quot;
</code></pre>
<h3 id="linux-primitives">Linux Primitives</h3>
<p>chroot(using pivot_root)</p>
<ul>
<li>Changes the root directory for a process to any given directory</li>
</ul>
<p>namespaces</p>
<ul>
<li>Different processes see different environments even though they are on the same host/OS<ul>
<li>mnt (mount points)</li>
<li>pid (process tree)</li>
<li>net (network interfaces and connectivity)</li>
<li>ipc (interprocess communication framework)</li>
<li>uts (unix timesharing - domain name, hostname, etc.)</li>
<li>uid (user IDs and mappings)</li>
</ul>
</li>
</ul>
<p>cgroups(control groups)</p>
<ul>
<li>manage/limit resource allocation to individual processes</li>
<li>Prioritization of processes</li>
</ul>
<p>Apparmor and SELinux profiles
- Security profiles to govern access to resources</p>
<p>Kernel capabilities</p>
<ul>
<li>without capabilities: root can do everything, everybody else may do nothing</li>
<li>38 granular facilities to control privileges</li>
</ul>
<p>seccomp policies</p>
<ul>
<li>Limitation of allowed kernel syscalls</li>
<li>Unallowed syscalls lead to process termination</li>
</ul>
<p>Netlink
- A Linux kernel interface used for inter-process communication (IPC) between both the kernel and userspace processes, and between different userspace processes. </p>
<p>Netfilter</p>
<ul>
<li>A framework provided by the Linux kernel that allows various networking-related operations</li>
<li>Packet filtering, network address translation, and port translation(iptables/nftables)</li>
<li>used to direct network packages to individual containers</li>
</ul>
<p>More inforamtion could refer to <a href="https://linuxcontainers.org/">LXC/LXD</a></p>
<p>Let's download an image <code>alpine</code> to simulate an root file system under <code>/opt/test</code> folder.</p>
<pre><code>james@lizard:/opt&gt; mkdir test
james@lizard:/opt&gt; cd test
james@lizard:/opt/test&gt; wget https://dl-cdn.alpinelinux.org/alpine/v3.13/releases/x86_64/alpine-minirootfs-3.13.4-x86_64.tar.gz
james@lizard:/opt/test&gt; tar zxvf alpine-minirootfs-3.13.4-x86_64.tar.gz -C alpine-minirootfs/

james@lizard:/opt&gt; tree ./test -L 1
./test
├── alpine-minirootfs-3.13.4-x86_64.tar.gz
├── bin
├── dev
├── etc
├── home
├── lib
├── media
├── mnt
├── opt
├── proc
├── root
├── run
├── sbin
├── srv
├── sys
├── tmp
├── usr
└── var
</code></pre>
<p>Mount folder <code>/opt/test/proc</code> to a file and use command <code>unshare</code> to build a guest system.</p>
<pre><code>james@lizard:/opt&gt; sudo mount -t tmpfs tmpfs /opt/test/proc

james@lizard:/opt&gt; sudo unshare --pid --mount-proc=$PWD/test/proc --fork chroot ./test/ /bin/sh
/ # ps -ef
PID   USER     TIME  COMMAND
    1 root      0:00 /bin/sh
    2 root      0:00 ps -ef
/ # touch 123
/ # ls 123
123
</code></pre>
<p>The file <code>123</code> created in guest system is accessable and writable from host system.</p>
<pre><code>james@lizard:/opt&gt; su -

lizard:/opt/test # ls 123
123

lizard:/opt/test # echo hello &gt; 123
</code></pre>
<p>We will see above change in guest system.</p>
<pre><code>/ # cat 123
hello
</code></pre>
<p>Let's create two folders <code>/opt/test-1</code> and <code>/opt/test-2</code>.</p>
<pre><code>james@lizard:/opt&gt; mkdir test-1
james@lizard:/opt&gt; mkdir test-2
</code></pre>
<p>Create two guests system. Mount <code>/opt/test/home/</code> to different folders for different guests.</p>
<pre><code>james@lizard:/opt&gt; sudo mount --bind /opt/test-1 /opt/test/home/
james@lizard:/opt&gt; sudo unshare --pid --mount-proc=$PWD/test/proc --fork chroot ./test/ /bin/sh
/ # cd /home
/home # echo &quot;test-1&quot; &gt; 123.1
/home # cat 123.1
test-1

james@lizard:/opt&gt; sudo mount --bind /opt/test-2 /opt/test/home/
james@lizard:/opt&gt; sudo unshare --pid --mount-proc=$PWD/test/proc --fork chroot ./test/ /bin/sh
/ # cd /home
/home # echo &quot;test-2&quot; &gt; 123.2
/home # cat 123.2
test-2

james@lizard:/opt&gt; ll test/home
-rw-r--r-- 1 root root 7 May 31 22:47 123.1
-rw-r--r-- 1 root root 7 May 31 22:47 123.2

james@lizard:/opt&gt; ll test-1/
-rw-r--r-- 1 root root 7 May 31 22:47 123.1
-rw-r--r-- 1 root root 7 May 31 22:47 123.2

james@lizard:/opt&gt; ll test-2/
-rw-r--r-- 1 root root 7 May 31 22:47 123.1
-rw-r--r-- 1 root root 7 May 31 22:47 123.2
</code></pre>
<p>With above demo, the conclusion is that two guests share same home folder on host system and will impact each other.</p>
<h3 id="installing-docker">Installing Docker</h3>
<p>Install Docker engine by referring the <a href="https://docs.docker.com/engine/">guide</a>, and Docker Desktop by referring the <a href="https://docs.docker.com/desktop/">guide</a>.</p>
<p>Install engine via openSUSE repository automatically.</p>
<pre><code>james@lizard:/opt&gt; sudo zypper in docker
</code></pre>
<p>The docker group is automatically created at package installation time. 
The user can communicate with the local Docker daemon upon its next login. 
The Docker daemon listens on a local socket which is accessible only by the root user and by the members of the docker group. </p>
<p>Add current user to <code>docker</code> group.</p>
<pre><code>james@lizard:/opt&gt; sudo usermod -aG docker $USER
</code></pre>
<p>Enable and start Docker engine.</p>
<pre><code>james@lizard:/opt&gt; sudo systemctl enable docker.service 
Created symlink /etc/systemd/system/multi-user.target.wants/docker.service → /usr/lib/systemd/system/docker.service.

james@lizard:/opt&gt; sudo systemctl start docker.service 

james@lizard:/opt&gt; sudo systemctl status docker.service 
● docker.service - Docker Application Container Engine
     Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)
     Active: active (running) since Sat 2022-05-28 14:36:45 CST; 6s ago
       Docs: http://docs.docker.com
   Main PID: 31565 (dockerd)
      Tasks: 20
     CGroup: /system.slice/docker.service
             ├─31565 /usr/bin/dockerd --add-runtime oci=/usr/sbin/docker-runc
             └─31574 containerd --config /var/run/docker/containerd/containerd.toml --log-level warn

May 28 14:36:44 lizard systemd[1]: Starting Docker Application Container Engine...
May 28 14:36:44 lizard dockerd[31565]: time=&quot;2022-05-28T14:36:44+08:00&quot; level=info msg=&quot;SUSE:secrets :: enabled&quot;
May 28 14:36:44 lizard dockerd[31574]: time=&quot;2022-05-28T14:36:44+08:00&quot; level=warning msg=&quot;deprecated version : `1`, please switch to version `2`&quot;
May 28 14:36:44 lizard dockerd[31574]: time=&quot;2022-05-28T14:36:44.659346964+08:00&quot; level=warning msg=&quot;failed to load plugin io.containerd.snapshotter.v1.devmapper&quot; error=&quot;devmapper no&gt;
May 28 14:36:44 lizard dockerd[31574]: time=&quot;2022-05-28T14:36:44.660040930+08:00&quot; level=warning msg=&quot;could not use snapshotter devmapper in metadata plugin&quot; error=&quot;devmapper not conf&gt;
May 28 14:36:45 lizard dockerd[31565]: time=&quot;2022-05-28T14:36:45.018458102+08:00&quot; level=warning msg=&quot;Your kernel does not support swap memory limit&quot;
May 28 14:36:45 lizard dockerd[31565]: time=&quot;2022-05-28T14:36:45.018495482+08:00&quot; level=warning msg=&quot;Your kernel does not support CPU realtime scheduler&quot;
May 28 14:36:45 lizard dockerd[31565]: time=&quot;2022-05-28T14:36:45.018502682+08:00&quot; level=warning msg=&quot;Your kernel does not support cgroup blkio weight&quot;
May 28 14:36:45 lizard dockerd[31565]: time=&quot;2022-05-28T14:36:45.018506223+08:00&quot; level=warning msg=&quot;Your kernel does not support cgroup blkio weight_device&quot;
May 28 14:36:45 lizard systemd[1]: Started Docker Application Container Engine.
</code></pre>
<h3 id="container-lifecycle">Container lifecycle</h3>
<h4 id="overview">Overview</h4>
<p>Pull down below images in advance.</p>
<pre><code>james@lizard:~&gt; docker image pull busybox
james@lizard:~&gt; docker image pull nginx
james@lizard:~&gt; docker image pull alpine
james@lizard:~&gt; docker image pull jenkins/jenkins:lts
james@lizard:~&gt; docker image pull golang:1.12-alpine
james@lizard:~&gt; docker image pull golang
</code></pre>
<p>Download some docker images.
Create and run a new busybox container interactively and connect a pseudo terminal to it.
Inside the container, use the top command to find out that <code>/bin/sh</code> is running as process with the PID 1 and <code>top</code> process is also running. 
After that, just exit.</p>
<pre><code>james@lizard:~&gt; docker image ls  (or docker images)
REPOSITORY        TAG           IMAGE ID       CREATED         SIZE
golang            latest        80d9a75ccb38   5 days ago      941MB
nginx             latest        c316d5a335a5   6 days ago      141MB
jenkins/jenkins   lts           9aee0d53624f   2 weeks ago     441MB
busybox           latest        beae173ccac6   4 weeks ago     1.24MB
alpine            latest        c059bfaa849c   2 months ago    5.58MB
golang            1.12-alpine   76bddfb5e55e   23 months ago   346MB

james@lizard:~&gt; docker run -d -it --name busybox_v1 -v /opt/test:/docker busybox:latest /bin/sh

james@lizard:~&gt; docker container ps -a
CONTAINER ID   IMAGE            COMMAND     CREATED          STATUS         PORTS     NAMES
185efe490507   busybox:latest   &quot;/bin/sh&quot;   11 seconds ago   Up 9 seconds             busybox_v1

james@lizard:~&gt; docker exec -it 185efe490507 /bin/sh
/ # top
Mem: 3627396K used, 12731512K free, 10080K shrd, 2920K buff, 2999340K cached
CPU:  0.0% usr  0.1% sys  0.0% nic 99.8% idle  0.0% io  0.0% irq  0.0% sirq
Load average: 0.38 1.09 1.29 2/277 14
  PID  PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND
    1     0 root     S     1332  0.0   1  0.0 /bin/sh
    8     0 root     S     1332  0.0   2  0.0 /bin/sh
   14     8 root     R     1328  0.0   1  0.0 top
/ # exitbuild 
</code></pre>
<p>Start a new nginx container in detached mode.
Use the <code>docker exec</code> command to start another shell (<code>/bin/sh</code>) in the nginx container. 
Use ps to find out that <code>sh</code> and <code>ps</code> commands are running in your container.</p>
<pre><code>james@lizard:~&gt; docker run -d -it --name nginx_v1 -v /opt/test:/docker nginx:latest /bin/sh

james@lizard:~&gt; docker container ps -a
CONTAINER ID   IMAGE            COMMAND                  CREATED         STATUS         PORTS     NAMES
edb640127a0d   nginx:latest     &quot;/docker-entrypoint.…&quot;   3 seconds ago   Up 2 seconds   80/tcp    nginx_v1
185efe490507   busybox:latest   &quot;/bin/sh&quot;                2 minutes ago   Up 2 minutes             busybox_v1

james@lizard:~&gt; docker exec -it edb640127a0d /bin/sh
# ps
/bin/sh: 2: ps: not found
# apt-get update &amp;&amp; apt-get install -y procps
# ps
   PID TTY          TIME CMD
     8 pts/1    00:00:00 sh
   351 pts/1    00:00:00 ps
# exit
</code></pre>
<p>Now we have two running containers below.</p>
<pre><code>james@lizard:~&gt; docker container ps -a
CONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS          PORTS     NAMES
edb640127a0d   nginx:latest     &quot;/docker-entrypoint.…&quot;   7 minutes ago    Up 7 minutes    80/tcp    nginx_v1
185efe490507   busybox:latest   &quot;/bin/sh&quot;                10 minutes 
Let's make use of this to create a new stage:ago   Up 10 minutes             busybox_v1
</code></pre>
<p>Let's use <code>docker logs</code> to display the logs of the container we just exited from. 
The option <code>--since 35m</code> means display log in last 35 minutes.</p>
<pre><code>james@lizard:~&gt; docker logs nginx_v1 --details --since 35m
james@lizard:~&gt; docker logs busybox_v1 --details --since 35m
</code></pre>
<p>Let's make use of this to create a new stage:</p>
<p>Use the <code>docker stop</code> command to end your nginx container.</p>
<pre><code>james@lizard:~&gt; docker stop busybox_v1
busybox_v1
james@lizard:~&gt; docker stop nginx_v1 
nginx_v1

james@lizard:~&gt; docker container ps -a
CONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS                        PORTS     NAMES
edb640127a0d   nginx:latest     &quot;/docker-entrypoint.…&quot;   10 minutes ago   Exited (137) 4 seconds ago              nginx_v1
185efe490507   busybox:latest   &quot;/bin/sh&quot;                13 minutes ago   Exited (137) 16 seconds ago             busybox_v1
</code></pre>
<p>With above command <code>docker container ps -a</code>, we get a list of all running and exited containers. 
Remove them with docker rm.
Use <code>docker rm $(docker ps -aq)</code> to clean up all containers on your host. Use it with caution!</p>
<pre><code>james@lizard:~&gt; docker rm busybox_v1
busybox_v1

james@lizard:~&gt; docker container ps -a
CONTAINER ID   IMAGE          COMMAND                  CREATED          STATUS                        PORTS     NAMES
edb640127a0d   nginx:latest   &quot;/docker-entrypoint.…&quot;   11 minutes ago   Exited (137) 53 seconds ago             nginx_v1
</code></pre>
<h4 id="ports-and-volumes">Ports and volumes</h4>
<p>Now, let's run an nginx webserver in a container and serve a website to the outside world.</p>
<p>Start a new nginx container and export the port of the nginx webserver to a random port that is chosen by Docker. </p>
<p>Use command <code>docker ps</code> to find you which port the webserver is forwarded. Access the docker with the forwarded port number on host <code>http://localhost:&lt;port#&gt;</code>.</p>
<pre><code>james@lizard:~&gt; docker container ps -a
CONTAINER ID   IMAGE          COMMAND                  CREATED          STATUS                        PORTS     NAMES
edb640127a0d   nginx:latest   &quot;/docker-entrypoint.…&quot;   11 minutes ago   Exited (137) 53 seconds ago             nginx_v1

james@lizard:~&gt; docker run -d -P --name nginx_v2 nginx:latest

james@lizard:~&gt; docker container ps -a
CONTAINER ID   IMAGE          COMMAND                  CREATED          STATUS                       PORTS                                     NAMES
3349a84e5024   nginx:latest   &quot;/docker-entrypoint.…&quot;   15 seconds ago   Up 14 seconds                0.0.0.0:49153-&gt;80/tcp, :::49153-&gt;80/tcp   nginx_v2
edb640127a0d   nginx:latest   &quot;/docker-entrypoint.…&quot;   13 minutes ago   Exited (137) 3 minutes ago                                             nginx_v1
</code></pre>
<p>Start another nginx container and expose port to <code>1080</code> on host as an example via <code>http://localhost:1080</code>.</p>
<pre><code>james@lizard:~&gt; docker run -d -p 1080:80 --name nginx_v3 nginx:latest

james@lizard:~&gt; docker container ps -a
CONTAINER ID   IMAGE          COMMAND                  CREATED          STATUS              PORTS                                     NAMES
214ded9b8645   nginx:latest   &quot;/docker-entrypoint.…&quot;   30 seconds ago   Up 28 seconds       0.0.0.0:1080-&gt;80/tcp, :::1080-&gt;80/tcp     nginx_v3
3349a84e5024   nginx:latest   &quot;/docker-entrypoint.…&quot;   3 hours ago      Up About a minute   0.0.0.0:49153-&gt;80/tcp, :::49153-&gt;80/tcp   nginx_v2
edb640127a0d   nginx:latest   &quot;/docker-entrypoint.…&quot;   3 hours ago      Up 3 seconds        80/tcp                                    nginx_v1
</code></pre>
<p>Let's make use of this to create a new stage:</p>
<p>Use command <code>docker inspect</code> to find out which port is exposed by the image. Network information (ip, gateway, ports, etc.) is part of the output JSON format.</p>
<pre><code>james@lizard:~&gt; docker inspect nginx_v3 
</code></pre>
<p>Create a file <code>index.html</code> in folder <code>/opt/test</code> with below sample content. </p>
<pre><code>&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;Sample Website from my container&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;h1&gt;This is a custom website.&lt;/h1&gt;
    &lt;p&gt;This website is served from my &lt;a href="http://www.docker.com" target="_blank"&gt;Docker&lt;/a&gt; container.&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>Start a new container that bind-mounts host directory <code>/opt/test</code> to container directory <code>/usr/share/nginx/html</code> as a volume, so that NGINX will publish the HTML file wee just created instead of its default message via <code>http://localhost:49159/</code> below.</p>
<pre><code>james@lizard:~&gt; docker run -d -P --mount type=bind,source=/opt/test/,target=/usr/share/nginx/html --name nginx_v3-1 nginx:latest

james@lizard:~&gt; docker container ps -a
CONTAINER ID   IMAGE          COMMAND                  CREATED          STATUS              PORTS                                     NAMES
bd94e4df65cf   nginx:latest   &quot;/docker-entrypoint.…&quot;   30 seconds ago   Up About a minute   0.0.0.0:49159-&gt;80/tcp, :::49154-&gt;80/tcp   nginx_v3-1                                                                                                
214ded9b8645   nginx:latest   &quot;/docker-entrypoint.…&quot;   30 seconds ago   Up 28 seconds       0.0.0.0:1080-&gt;80/tcp, :::1080-&gt;80/tcp     nginx_v3
3349a84e5024   nginx:latest   &quot;/docker-entrypoint.…&quot;   3 hours ago      Up About a minute   0.0.0.0:49153-&gt;80/tcp, :::49153-&gt;80/tcp   nginx_v2
edb640127a0d   nginx:latest   &quot;/docker-entrypoint.…&quot;   3 hours ago      Up 3 seconds        80/tcp                                    nginx_v1
</code></pre>
<p>Check nginx config file on where is the html home page stored in container.</p>
<pre><code>james@lizard:~&gt; docker exec -it nginx_v3-1 /bin/sh
# cd /etc/nginx/conf.d
# ls
default.conf
# cat default.conf
server {
    listen       80;
    listen  [::]:80;
    server_name  localhost;

    #access_log  /var/log/nginx/host.access.log  main;

    location / {
        root   /usr/share/nginx/html;  &lt;--
        index  index.html index.htm;
    }

    #error_page  404              /404.html;

    # redirect server error pages to the static page /50x.html
    #
    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
        root   /usr/share/nginx/html;
    }

    # proxy the PHP scripts to Apache listening on 127.0.0.1:80
    #
    #location ~ \.php$ {
    #    proxy_pass   http://127.0.0.1;
    #}

    # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000
    #
    #location ~ \.php$ {
    #    root           html;
    #    fastcgi_pass   127.0.0.1:9000;
    #    fastcgi_index  index.php;
    #    fastcgi_param  SCRIPT_FILENAME  /scripts$fastcgi_script_name;
    #    include        fastcgi_params;
    #}

    # deny access to .htaccess files, if Apache's document root
    # concurs with nginx's one
    #
    #location ~ /\.ht {
    #    deny  all;
    #}
}
# cd /usr/share/nginx/html
# cat index.html              
  &lt;html&gt;
  &lt;head&gt;
      &lt;title&gt;Sample Website from my container&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
      &lt;h1&gt;This is a custom website.&lt;/h1&gt;
      &lt;p&gt;This website is served from my &lt;a href=&quot;http://www.docker.com&quot; target=&quot;_blank&quot;&gt;Docker&lt;/a&gt; container.&lt;/p&gt;
  &lt;/body&gt;
  &lt;/html&gt;
# 
</code></pre>
<p>It's recommendable to add a persistence with volumes API, instead of storing data in a docker container. Docker supports 2 ways of mount:</p>
<ul>
<li>Bind mounts: <ul>
<li>mount a local host directory onto a certain path in the container. </li>
<li>Everything that was present before in the target directory is hidden (nature of the bind mount). </li>
<li>For example, if you have some configuration you want to inject, write your config file, store it on your docker host at <code>/home/container/config</code> and mount the content of this directory to <code>/usr/application/config</code> (assuming the application reads config from there). </li>
<li>Command: <code>docker run --mount type=bind,source=&lt;source path&gt;,target=&lt;container path&gt; …</code></li>
</ul>
</li>
<li>Named volumes: <ul>
<li>docker can create a separated storage volume. </li>
<li>Its lifecycle is independent from the container but still managed by docker. </li>
<li>Upon creation, the content of the mount target is merged into the volume. </li>
<li>Command: <code>docker run --mount source=&lt;vol name&gt;,target=&lt;container path&gt; …</code></li>
</ul>
</li>
</ul>
<p>How to differentiate between bind mountbuild s and named volumes? </p>
<ul>
<li>When specifying an absolute path, docker assumes a bind mount. </li>
<li>When you just give a name (like in a relative path “config”), it will assume a named volume and create a volume “config”.</li>
<li>Note: Persistent storage is 'provided' by the host. It can be a part of the file system on the host directly but also an NFS mount. </li>
</ul>
<h4 id="dockerfile">Dockerfile</h4>
<p>Let's build an image with a Dockerfile,build  tag it and upload it to a registry. </p>
<p>Get docker image build history.</p>
<pre><code>james@lizard:~&gt; docker image history nginx:latest 
IMAGE          CREATED      CREATED BY                                      SIZE      COMMENT
c316d5a335a5   6 days ago   /bin/sh -c #(nop)  CMD [&quot;nginx&quot; &quot;-g&quot; &quot;daemon…   0B
&lt;missing&gt;      6 days ago   /bin/sh -c #(nop)  STOPSIGNAL SIGQUIT           0B
&lt;missing&gt;      6 days ago   /bin/sh -c #(nop)  EXPOSE 80                    0B
&lt;missing&gt;      6 days ago   /bin/sh -c #(nop)  ENTRYPOINT [&quot;/docker-entr…   0B
&lt;missing&gt;      6 days ago   /bin/sh -c #(nop) COPY file:09a214a3e07c919a…   4.61kB
&lt;missing&gt;      6 days ago   /bin/sh -c build #(nop) COPY file:0fd5fca330dcd6a7…   1.04kB
&lt;missing&gt;      6 days ago   /bin/sh -c #(nop) COPY file:0b866ff3fc1ef5b0…   1.96kB
&lt;missing&gt;      6 days ago   /bin/sh -c #(nop) COPY file:65504f71f5855ca0…   1.2kB
&lt;missing&gt;      6 days ago   /bin/sh -c set -x     &amp;&amp; addgroup --system -…   61.1MB
&lt;missing&gt;      6 days ago   /bin/sh -c #(nop)  ENV PKG_RELEASE=1~bullseye   0B
&lt;missing&gt;      6 days ago   /bin/sh -c #(nop)  ENV NJS_VERSION=0.7.2        0B
&lt;missing&gt;      6 days ago   /bin/sh -c #(nop)  ENV NGINX_VERSION=1.21.6     0B
&lt;missing&gt;      6 days ago   /bin/sh -c #(nop)  LABEL maintainer=NGINX Do…   0B
&lt;missing&gt;      7 days ago   /bin/sh -c #(nop)  CMD [&quot;bash&quot;]                 0B
&lt;missing&gt;      7 days ago   /bin/sh -c #(nop) ADD file:90495c24c897ec479…   80.4MB
</code></pre>
<p>Create an empty directory <code>/opt/tmp-1</code>, change to the directory and create an sample <code>index.html</code> file in <code>/opt/tmp-1</code>.</p>
<pre><code>james@lizard:/opt/tmp-1&gt; cat index.html 
  &lt;html&gt;
  &lt;head&gt;
      &lt;title&gt;Sample Website from my container&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
      &lt;h1&gt;This is a custom website.&lt;/h1&gt;
      &lt;p&gt;This website is served from my &lt;a href=&quot;http://www.docker.com&quot; target=&quot;_blank&quot;&gt;Docker&lt;/a&gt; container.&lt;/p&gt;
  &lt;/body&gt;
  &lt;/html&gt;
</code></pre>
<p>Use <code>FROM</code> to extend an existing image, specify the release number.</p>
<p>Use <code>COPY</code> to copy a new default website into the image, e.g., <code>/usr/share/nginx/html</code></p>
<p>Create SSL configuration <code>/opt/tmp-1/ssl.conf</code> for nginx.</p>
<pre><code>server {
    listen       443 ssl;
    server_name  localhost;

    ssl_certificate /etc/nginx/ssl/nginx.crt;
    ssl_certificate_key /etc/nginx/ssl/nginx.key;

    location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
    }
}
</code></pre>
<p>Use OpenSSL to create a self-signed certificate so SSL/TLS to work would work.</p>
<p>Use the following command to create an encryption key and a certificate.</p>
<pre><code>james@lizard:/opt/tmp-1&gt; openssl req -x509 -nodes -newkey rsa:4096 -keyout nginx.key -out nginx.crt -days 365 -subj &quot;/CN=$(hostname)&quot;
Generating a RSA private key
........++++
................................++++
writing new private key to 'nginx.key'
-----
</code></pre>
<p>To enable encrypted HTTPS, we need to expose port 443 with the EXPOSE directive. The default nginx image only exposes port 80 for unencrypted HTTP.</p>
<p>In summary, we create below Dockerfile in foder <code>/opt/tmp-1</code>. </p>
<pre><code>james@lizard:/opt/tmp-1&gt; cat Dockerfile 
FROM nginx:latest

# copy the custom website into the image
COPY index.html /usr/share/nginx/html

# copy the SSL configuration file into the image
COPY ssl.conf /etc/nginx/conf.d/ssl.conf

# download the SSL key and certificate into the image
COPY nginx.key /etc/nginx/ssl/
COPY nginx.crt /etc/nginx/ssl/

# expose the HTTPS port
EXPOSE 443
</code></pre>
<p>We have five files in foder <code>/opt/tmp-1</code> till now.</p>
<pre><code>james@lizard:/opt/tmp-1&gt; ls
Dockerfile  index.html  nginx.crt  nginx.key  ssl.conf
</code></pre>
<p>Now let's use the <code>docker build</code> command to build the image, forward the containers ports 80 and 443.</p>
<pre><code>james@lizard:~&gt; docker build -t nginx:my1 /opt/tmp-1/
Sending build context to Docker daemon  62.98kB
Let's make use of this to create a new stage:
Step 1/6 : FROM nginx:latest
 ---&gt; c316d5a335a5
Step 2/6 : COPY index.html /usr/share/nginx/html
 ---&gt; 4a71ac8a2624
Step 3/6 : COPY ssl.conf /etc/nginx/conf.d/ssl.conf
 ---&gt; ad574bc8080c
Step 4/6 : COPY nginx.key /etc/nginx/ssl/
 ---&gt; 90c41ec98809
Step 5/6 : COPY nginx.crt /etc/nginx/ssl/
 ---&gt; 5801c1e5e02f
Step 6/6 : EXPOSE 443
 ---&gt; Running in 0db1bffe7eb3
Removing intermediate container 0db1bffe7eb3
 ---&gt; 748439b24876
Successfully built 748439b24876
Successfully tagged nginx:my1

james@lizard:~&gt; docker image ls
REPOSITORY        TAG           IMAGE ID       CREATED          SIZE
nginx             my1           748439b24876   44 seconds ago   142MB
golang            latest        80d9a75ccb38   5 days ago       941MB
nginx             latest        c316d5a335a5   6 days ago       141MB
jenkins/jenkins   lts           9aee0d53624f   2 weeks ago      441MB
busybox           latest        beae173ccac6   4 weeks ago      1.24MB
alpine            latest        c059bfaa849c   2 months ago     5.58MB
golang            1.12-alpine   76bddfb5e55e   23 months ago    346MB


james@lizard:~&gt; docker run -d -p 1086:80 -p 1088:443 --name nginx_v5 nginx:my1

james@lizard:~&gt; docker container ps -a
CONTAINER ID   IMAGE          COMMAND  build                 CREATED             STATUS                           PORTS                                                                            NAMES
70126d22e48b   nginx:my1      &quot;/docker-entrypoint.…&quot;   7 seconds ago       Up 6 seconds                     0.0.0.0:1086-&gt;80/tcp, :::1086-&gt;80/tcp, 0.0.0.0:1088-&gt;443/tcp, :::1088-&gt;443/tcp   nginx_v5
7714058076c0   nginx:latest   &quot;/docker-entrypoint.…&quot;   About an hour ago   Exited (0) 48 seconds ago                                                                                         nginx_v4
214ded9b8645   nginx:latest   &quot;/docker-entrypoint.…&quot;   2 hours ago         Exited (0) About an hour ago                                                                                      nginx_v3
3349a84e5024   nginx:latest   &quot;/docker-entrypoint.…&quot;   5 hours ago         Exited (0) About an hour ago                                                                                      nginx_v2
edb640127a0d   nginx:latest   &quot;/docker-entrypoint.…&quot;   5 hours ago         Exited (137) About an hour ago                                                                                    nginx_v1
</code></pre>
<p>Above changes can be validated via below links:</p>
<pre><code>http://localhost:1086/
https://localhost:1088/
</code></pre>
<p>Register an account in <a href="https://hub.docker.com/">DockerHub</a> and enable access token in Docker Hub for CLI client authentication.</p>
<pre><code>james@lizard:~&gt; docker login
Username: &lt;your account id&gt;
Password: &lt;token&gt;
</code></pre>
<p>Tag the image to give image a nice name and a release number as tag, e.g., name is <code>secure_nginx_0001</code>, tag is <code>v1</code>.</p>
<pre><code>james@lizard:~&gt; docker tag nginx:my1 &lt;your account id&gt;secure_nginx_0001:v1

james@lizard:~&gt; docker push &lt;your account id&gt;secure_nginx_0001:v1

james@lizard:~&gt; docker image ls
REPOSITORY                            TAG           IMAGE ID       CREATED         SIZE
nginx                                 my1           748439b24876   7 minutes ago   142MB
&lt;your account id&gt;secure_nginx_0001    v1            748439b24876   7 minutes ago   142MB
golang                                latest        80d9a75ccb38   5 days ago      941MB
nginx                                 latest        c316d5a335a5   6 days ago      141MB
jenkins/jenkins                       lts           9aee0d53624f   2 weeks ago     441MB
busybox                               latest        beae173ccac6   4 weeks ago     1.24MB
alpine                                latest        c059bfaa849c   2 months ago    5.58MB
golang                                1.12-alpine   76bddfb5e55e   23 months ago   346MB
</code></pre>
<h4 id="multi-stage-dockerfile">Multi-stage Dockerfile</h4>
<p>Let's show an example of multi-stage build. The multi-stage in the context of Docker means, we can have more than one line with a FROM keyword. </p>
<p>Create folder <code>/opt/tmp-2</code> and <code>/opt/tmp-2/tmpl</code>. </p>
<p>Create files <a href="../assets/edit.html">edit.html</a>, <a href="../assets/view.html">view.html</a>, <a href="../assets/wiki.go">wiki.go</a> and structure likes below.</p>
<pre><code>james@lizard:/opt/tmp-2&gt; tree -l
.
├── tmpl
│   ├── edit.html
│   └── view.html
└── wiki.go
</code></pre>
<p>Create an new Dockerfile that starts </p>
<pre><code>james@lizard:/opt/tmp-2&gt; cat Dockerfile
# app builder stage
FROM golang:1.12-alpine as builder

## copy the go source code over and build the binary
WORKDIR /go/src
COPY wiki.go /go/src/wiki.go
RUN go build wiki.go

# app exec stage
# separate &amp; new image starts here!#
FROM alpine:3.9

# prepare file system etc
RUN mkdir -p /app/data /app/tmpl &amp;&amp; adduser -S -D -H -h /app appuser
COPY tmpl/* /app/tmpl/

# get the compiled binary from the previous stage
COPY --from=builder /go/src/wiki /app/wiki

# prepare runtime env
RUN chown -R appuser /app
USER appuser
WORKDIR /app

# expose app port &amp; set default command
EXPOSE 8080
CMD [&quot;/app/wiki&quot;]
</code></pre>
<p>Build the images by Dockerfile we created above.</p>
<pre><code>james@lizard:~&gt; docker build -t lizard/golang:my1 /opt/tmp-2/
Sending build context to Docker daemon  9.728kB
Step 1/13 : FROM golang:1.12-alpine as builder
 ---&gt; 76bddfb5e55e
Step 2/13 : WORKDIR /go/src
 ---&gt; Running in 279957765a67
Removing intermediate container 279957765a67
 ---&gt; d74f3297387b
Step 3/13 : COPY wiki.go /go/src/wiki.go
 ---&gt; f14f358f10c0
Step 4/13 : RUN go build wiki.go
 ---&gt; Running in af4a9d2d1dcc
Removing intermediate container af4a9d2d1dcc
 ---&gt; 101e734099a3
Step 5/13 : FROM alpine:3.9
3.9: Pulling from library/alpine
31603596830f: Pull complete
Digest: sha256:414e0518bb9228d35e4cd5165567fb91d26c6a214e9c95899e1e056fcd349011
Status: Downloaded newer image for alpine:3.9
 ---&gt; 78a2ce922f86
Step 6/13 : RUN mkdir -p /app/data /app/tmpl &amp;&amp; adduser -S -D -H -h /app appuser
 ---&gt; Running in c7a8793fc95d
Removing intermediate container c7a8793fc95d
 ---&gt; a6e83922a81f
Step 7/13 : COPY tmpl/* /app/tmpl/
 ---&gt; e48d44caf735
Step 8/13 : COPY --from=builder /go/src/wiki /app/wiki
 ---&gt; 26cc829fe32b
Step 9/13 : RUN chown -R appuser /app
 ---&gt; Running in 22f3af57f969
Removing intermediate container 22f3af57f969
 ---&gt; ea7d678adf67
Step 10/13 : USER appuser
 ---&gt; Running in 03c5d8e9ad45
Removing intermediate container 03c5d8e9ad45
 ---&gt; 40a692198491
Step 11/13 : WORKDIR /app
 ---&gt; Running in 7c1b04e38306
Removing intermediate container 7c1b04e38306
 ---&gt; 45eaaebb0c12
Step 12/13 : EXPOSE 8080
 ---&gt; Running in 84f06d2e5f90
Removing intermediate container 84f06d2e5f90
 ---&gt; 3750bfa8c032
Step 13/13 : CMD [&quot;/app/wiki&quot;]
 ---&gt; Running in 9ce20ca3a834
Removing intermediate container 9ce20ca3a834
 ---&gt; 8621174bab0d
Successfully built 8621174bab0d
Successfully tagged lizard/golang:my1
</code></pre>
<p>Run the image in detached mode, create a port forwarding from port 8080 in the container to port 1090 on the host.</p>
<pre><code>james@lizard:~&gt; docker run -d -p 1090:8080 --name golan_v1 lizard/golang:my1
</code></pre>
<p>Access the container via link http://localhost:1090</p>
<p>Tab the golang image we created and push it to DockerHub.</p>
<pre><code>james@lizard:~&gt; docker tag lizard/golang:my1 &lt;your acccount id&gt;/golang_0001:v1

james@lizard:~&gt; docker push &lt;your acccount id&gt;/golang_0001:v1
</code></pre>
<h2 id="basic-concepts-of-kubernetes">Basic Concepts of Kubernetes</h2>
<h3 id="kubernetes-components">Kubernetes Components</h3>
<p>A Kubernetes cluster consists of the components that represent the <strong>control plane</strong> and a set of machines called <strong>nodes</strong>.</p>
<p><img alt="The components of a Kubernetes cluster" src="https://d33wubrfki0l68.cloudfront.net/2475489eaf20163ec0f54ddc1d92aa8d4c87c96b/e7c81/images/docs/components-of-kubernetes.svg" /></p>
<p><strong>Kubernetes Components</strong>: </p>
<ul>
<li>Control Plane Components<ul>
<li>kube-apiserver: <ul>
<li>query and manipulate the state of objects in Kubernetes.</li>
<li>play as "communication hub" among all resources in cluster.</li>
<li>provide cluster security authentication, authorization, and role assignment.</li>
<li>the only one can connect to <code>etcd</code>.</li>
</ul>
</li>
<li>etcd: <ul>
<li>all Kubernetes objects are stored on etcd. </li>
<li>Kubernetes objects are persistent <strong>entities</strong> in the Kubernetes system, which are used to represent the state of your cluster.</li>
</ul>
</li>
<li>kube-scheduler: <ul>
<li>watches for newly created Pods with no assigned node, and selects a node for them to run on.</li>
</ul>
</li>
<li>kube-controller-manager: runs controller processes.<ul>
<li><em>Node controller</em>: Responsible for noticing and responding when nodes go down.</li>
<li><em>Job controller</em>: Watches for Job objects that represent one-off tasks, then creates Pods to run those tasks to completion.</li>
<li><em>Endpoints controller</em>: Populates the Endpoints object (that is, joins Services &amp; Pods).</li>
<li><em>Service Account &amp; Token controllers</em>: Create default accounts and API access tokens for new namespaces.</li>
</ul>
</li>
<li>cloud-controller-manager: embeds cloud-specific control logic and only runs controllers that are specific to your cloud provider, no need for own premises and learning environment.<ul>
<li><em>Node controller</em>: For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding</li>
<li><em>Route controller</em>: For setting up routes in the underlying cloud infrastructure</li>
<li><em>Service controller</em>: For creating, updating and deleting cloud provider load balancers</li>
</ul>
</li>
</ul>
</li>
<li>Node Components<ul>
<li>kubelet: <ul>
<li>An agent that runs on each node in the cluster. </li>
<li>Manage node. It makes sure that containers are running in a Pod. <code>kubelet</code> registers and updates nodes information to APIServer, and APIServer stores them into <code>etcd</code>.</li>
<li>Manage pod. Watch pod via APIServer, and action on pods or containers in pods.</li>
<li>Health check at container level.</li>
</ul>
</li>
<li>kube-proxy: <ul>
<li>is a network proxy that runs on each node in cluster.<ul>
<li>iptables</li>
<li>ipvs</li>
</ul>
</li>
<li>maintains network rules on nodes.</li>
</ul>
</li>
<li>Container runtime: <ul>
<li>is the software that is responsible for running containers.</li>
</ul>
</li>
</ul>
</li>
<li>Addons<ul>
<li>DNS: is a DNS server and required by all Kubernetes clusters.</li>
<li>Web UI (Dashboard): web-based UI for Kubernetes clusters. </li>
<li>Container Resource Monitoring: records generic time-series metrics about containers in a central database</li>
<li>Cluster-level Logging: is responsible for saving container logs to a central log store with search/browsing interface.</li>
</ul>
</li>
</ul>
<p>Scalability:</p>
<ul>
<li><strong>Scaling out</strong> (horizontal scaling) by adding more servers to your architecture to spread the workload across more machines.</li>
<li><strong>Scaling up</strong> (vertical scaling) by adding more hard drives and memory to increase the computing capacity of physical servers. </li>
</ul>
<h3 id="kubernetes-api">Kubernetes API</h3>
<p>The REST API is the fundamental fabric of Kubernetes. 
All operations and communications between components, and external user commands are REST API calls that the API Server handles. 
Consequently, everything in the Kubernetes platform is treated as an <em>API object</em> and has a corresponding entry in the API.</p>
<p>The core of Kubernetes' control plane is the API server. </p>
<ul>
<li>CRI: Container Runtime Interface</li>
<li>CNI: Container Network Interface</li>
<li>CSI: Container Storage Interface</li>
</ul>
<p>The API server exposes an HTTP API that lets end users, different parts of cluster, and external components communicate with one another.</p>
<p>The Kubernetes API lets we query and manipulate the state of API objects in Kubernetes (for example: Pods, Namespaces, ConfigMaps, and Events).</p>
<p>Kubernetes API:</p>
<ul>
<li>OpenAPI specification<ul>
<li>OpenAPI V2</li>
<li>OpenAPI V3</li>
</ul>
</li>
<li>Persistence. Kubernetes stores the serialized state of objects by writing them into etcd.</li>
<li>API groups and versioning. Versioning is done at the API level. API resources are distinguished by their API group, resource type, namespace (for namespaced resources), and name.<ul>
<li>API changes</li>
</ul>
</li>
<li>API Extension</li>
</ul>
<h4 id="api-version">API Version</h4>
<p>The API versioning and software versioning are indirectly related. 
The API and release versioning proposal describes the relationship between API versioning and software versioning.
Different API versions indicate different levels of stability and support. </p>
<p>Here's a summary of each level:</p>
<ul>
<li>Alpha:<ul>
<li>The version names contain alpha (for example, v1alpha1).</li>
<li>The software may contain bugs. Enabling a feature may expose bugs. A feature may be disabled by default.</li>
<li>The support for a feature may be dropped at any time without notice.</li>
<li>The API may change in incompatible ways in a later software release without notice.</li>
<li>The software is recommended for use only in short-lived testing clusters, due to increased risk of bugs and lack of long-term support.</li>
</ul>
</li>
<li>Beta:<ul>
<li>The version names contain beta (for example, v2beta3).</li>
<li>The software is well tested. Enabling a feature is considered safe. Features are enabled by default.</li>
<li>The support for a feature will not be dropped, though the details may change.</li>
<li>The schema and/or semantics of objects may change in incompatible ways in a subsequent beta or stable release. When this happens, migration instructions are provided. Schema changes may require deleting, editing, and re-creating API objects. The editing process may not be straightforward. The migration may require downtime for applications that rely on the feature.</li>
<li>The software is not recommended for production uses. Subsequent releases may introduce incompatible changes. If you have multiple clusters which can be upgraded independently, you may be able to relax this restriction.
  Note: Please try beta features and provide feedback. After the features exit beta, it may not be practical to make more changes.</li>
</ul>
</li>
<li>Stable:<ul>
<li>The version name is vX where X is an integer.</li>
<li>The stable versions of features appear in released software for many subsequent versions.</li>
</ul>
</li>
</ul>
<p>Command to get current API</p>
<pre><code>kubectl api-resources
</code></pre>
<h4 id="api-group">API Group</h4>
<p><a href="https://git.k8s.io/design-proposals-archive/api-machinery/api-group.md">API groups</a> make it easier to extend the Kubernetes API. 
The API group is specified in a REST path and in the apiVersion field of a serialized object.</p>
<p>There are several API groups in Kubernetes:</p>
<ul>
<li>The core (also called legacy) group is found at REST path <code>/api/v1</code>. <ul>
<li>The core group is not specified as part of the apiVersion field, for example, apiVersion: v1.</li>
</ul>
</li>
<li>The named groups are at REST path <code>/apis/$GROUP_NAME/$VERSION</code> and use apiVersion: <code>$GROUP_NAME/$VERSION</code> (for example, apiVersion: batch/v1). </li>
</ul>
<h3 id="kubernetes-objects">Kubernetes Objects</h3>
<h4 id="objects-overview">Objects Overview:</h4>
<ul>
<li>Object Spec:<ul>
<li>providing a description of the characteristics the resource created to have: <em>its desired state</em>.</li>
</ul>
</li>
<li>Object Status:<ul>
<li>describes the current state of the object.</li>
</ul>
</li>
</ul>
<p>Example of Deployment as an object that can represent an application running on cluster.</p>
<pre><code>apiVersion: apps/v1  # Which version of the Kubernetes API you're using to create this object
kind: Deployment     # What kind of object you want to create
metadata:            # Data that helps uniquely identify the object, including a name string, UID, and optional namespace
  name: nginx-deployment
spec:                # What state you desire for the object
  selector:
    matchLabels:
      app: nginx
  replicas: 2 # tells deployment to run 2 pods matching the template
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
</code></pre>
<h4 id="object-management">Object Management:</h4>
<p>The <code>kubectl</code> command-line tool supports several different ways to create and manage Kubernetes objects. Read the <a href="https://kubectl.docs.kubernetes.io/">Kubectl book</a> for details.</p>
<p>A Kubernetes object should be managed using ONLY one technique. Mixing and matching techniques for the same object results in undefined behavior. </p>
<p>Three management techniques:</p>
<ul>
<li>Imperative commands<ul>
<li>operates directly on live objects in a cluster. </li>
<li><code>kubectl create deployment nginx --image nginx</code></li>
</ul>
</li>
<li>Imperative object configuration<ul>
<li><code>kubectl create -f nginx.yaml</code></li>
<li><code>kubectl delete -f nginx.yaml -f redis.yaml</code></li>
<li><code>kubectl replace -f nginx.yaml</code></li>
</ul>
</li>
<li>Declarative object configuration<ul>
<li><code>kubectl diff -f configs/</code></li>
<li><code>kubectl apply -f configs/</code></li>
</ul>
</li>
</ul>
<h4 id="object-names-and-ids">Object Names and IDs</h4>
<p>Each object in your cluster has a <em>Name</em> that is unique for that type of resource.</p>
<ul>
<li>DNS Subdomain Names</li>
<li>Label Names</li>
<li>Path Segment Names</li>
</ul>
<p>Every Kubernetes object also has a <em>UID</em> that is unique across the whole cluster.</p>
<h4 id="namespaces">Namespaces</h4>
<p>In Kubernetes, namespaces provides a mechanism for isolating groups of resources within a single cluster. </p>
<p>Names of resources need to be unique within a namespace, but not across namespaces. </p>
<p>Namespace-based scoping is applicable only for namespaced objects (e.g. Deployments, Services, etc) and not for cluster-wide objects (e.g. StorageClass, Nodes, PersistentVolumes, etc)</p>
<p>Not All Objects are in a Namespace.</p>
<p>Kubernetes starts with four initial namespaces:</p>
<ul>
<li><code>default</code> 
    The default namespace for objects with no other namespace</li>
<li><code>kube-system</code> 
    The namespace for objects created by the Kubernetes system</li>
<li><code>kube-public</code> 
    This namespace is created automatically and is readable by all users (including those not authenticated). 
    This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. 
    The public aspect of this namespace is only a convention, not a requirement.</li>
<li><code>kube-node-lease</code> This namespace holds Lease objects associated with each node. Node leases allow the kubelet to send heartbeats so that the control plane can detect node failure.</li>
</ul>
<p>Viewing namespaces: </p>
<ul>
<li><code>kubectl get namespace</code></li>
</ul>
<p>Setting the namespace for a request</p>
<ul>
<li><code>kubectl run nginx --image=nginx --namespace=&lt;insert-namespace-name-here&gt;</code></li>
<li><code>kubectl get pods --namespace=&lt;insert-namespace-name-here&gt;</code></li>
</ul>
<h4 id="labels-and-selectors">Labels and Selectors</h4>
<p>Labels are key/value pairs that are attached to objects, such as pods. 
Valid label keys have two segments: an optional prefix and name, separated by a slash (<code>/</code>).</p>
<p>Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users.</p>
<p>Labels can be used to organize and to select subsets of objects. 
Labels can be attached to objects at creation time and subsequently added and modified at any time. 
Each object can have a set of key/value labels defined. 
Each Key must be unique for a given object.</p>
<p>Example of labels:</p>
<pre><code>&quot;metadata&quot;: {
    &quot;labels&quot;: {
        &quot;key1&quot; : &quot;value1&quot;,
        &quot;key2&quot; : &quot;value2&quot;
    }
}
</code></pre>
<p>Unlike names and UIDs, labels do not provide uniqueness. In general, we expect many objects to carry the same label(s).</p>
<p>The API currently supports two types of selectors: </p>
<ul>
<li>equality-based, e.g., <code>environment = production</code>, <code>tier != frontend</code></li>
<li>set-based, e.g., <code>environment in (production, qa)</code>, <code>tier notin (frontend, backend)</code></li>
</ul>
<p>Sample commands:</p>
<pre><code>kubectl get pods -l environment=production,tier=frontend
kubectl get pods -l 'environment in (production),tier in (frontend)'
kubectl get pods -l 'environment in (production, qa)'
kubectl get pods -l 'environment,environment notin (frontend)'
</code></pre>
<h4 id="annotations">Annotations</h4>
<p>Use Kubernetes annotations to attach arbitrary non-identifying metadata to objects. 
Clients such as tools and libraries can retrieve this metadata.</p>
<p>Use either labels or annotations to attach metadata to Kubernetes objects. </p>
<ul>
<li>Labels can be used to select objects and to find collections of objects that satisfy certain conditions. </li>
<li>Annotations are not used to identify and select objects. </li>
</ul>
<p>Annotations, like labels, are key/value maps. The keys and the values in the map must be strings. </p>
<pre><code>&quot;metadata&quot;: {
    &quot;annotations&quot;: {
      &quot;key1&quot; : &quot;value1&quot;,
      &quot;key2&quot; : &quot;value2&quot;
    }
}
</code></pre>
<p>Valid annotation keys have two segments: an optional prefix and name, separated by a slash (<code>/</code>). </p>
<h4 id="field-selectors">Field Selectors</h4>
<p>Field selectors let you select Kubernetes resources based on the value of one or more resource fields. </p>
<p>Here are some examples of field selector queries:</p>
<pre><code>metadata.name=my-service
metadata.namespace!=default
status.phase=Pending
</code></pre>
<p>This kubectl command selects all Pods for which the value of the status.phase field is Running:
<code>kubectl get pods --field-selector status.phase=Running</code></p>
<p>Supported field selectors vary by Kubernetes resource type. All resource types support the <code>metadata.name</code> and <code>metadata.namespace</code> fields. </p>
<p>Use the <code>=</code>, <code>==</code>, and <code>!=</code> operators with field selectors (<code>=</code> and <code>==</code> mean the same thing). </p>
<p>For example:</p>
<p><code>kubectl get ingress --field-selector foo.bar=baz</code></p>
<p>With operators, 
<code>kubectl get services  --all-namespaces --field-selector metadata.namespace!=default</code></p>
<p>Chained selectors, 
<code>kubectl get pods --field-selector=status.phase!=Running,spec.restartPolicy=Always</code></p>
<p>Multiple resource types, 
<code>kubectl get statefulsets,services --all-namespaces --field-selector metadata.namespace!=default</code></p>
<h4 id="finalizers">Finalizers</h4>
<p>Finalizers are <em>namespaced keys</em> that tell Kubernetes to wait until specific conditions are met before it fully deletes resources marked for <em>deletion</em>. 
<em>Finalizers alert controllers</em> to clean up resources the deleted object owned.</p>
<p>Finalizers are usually added to resources for a reason, so forcefully removing them can lead to issues in the cluster.</p>
<p>Like labels, <em>owner references</em> describe the relationships between objects in Kubernetes, but are used for a different purpose.</p>
<p>Kubernetes uses the owner references (not labels) to determine which Pods in the cluster need cleanup.</p>
<p>Kubernetes processes finalizers when it identifies owner references on a resource targeted for deletion.</p>
<h4 id="owners-and-dependents">Owners and Dependents</h4>
<p>In Kubernetes, some objects are owners of other objects. For example, a ReplicaSet is the owner of a set of Pods. 
These owned objects are dependents of their owner.</p>
<p>Dependent objects have a <code>metadata.ownerReferences</code> field that references their owner object.</p>
<p>A valid owner reference consists of the object name and a UID within the same namespace as the dependent object.</p>
<p>Dependent objects also have an <code>ownerReferences.blockOwnerDeletion</code> field that takes a boolean value and controls whether specific dependents can block garbage collection from deleting their owner object. </p>
<h3 id="resource">Resource</h3>
<p>Kubernetes resources and "records of intent" are all stored as API objects, and modified via RESTful calls to the API. 
The API allows configuration to be managed in a declarative way. 
Users can interact with the Kubernetes API directly, or via tools like kubectl. 
The core Kubernetes API is flexible and can also be extended to support custom resources.</p>
<ul>
<li>Workload Resources<ul>
<li><em>Pod</em>. Pod is a collection of containers that can run on a host.</li>
<li><em>PodTemplate</em>. PodTemplate describes a template for creating copies of a predefined pod.</li>
<li><em>ReplicationController</em>. ReplicationController represents the configuration of a replication controller.</li>
<li><em>ReplicaSet</em>. ReplicaSet ensures that a specified number of pod replicas are running at any given time.</li>
<li><em>Deployment</em>. Deployment enables declarative updates for Pods and ReplicaSets.</li>
<li><em>StatefulSet</em>. StatefulSet represents a set of pods with consistent identities.</li>
<li><em>ControllerRevision</em>. ControllerRevision implements an immutable snapshot of state data.</li>
<li><em>DaemonSet</em>. DaemonSet represents the configuration of a daemon set.</li>
<li><em>Job</em>. Job represents the configuration of a single job.</li>
<li><em>CronJob</em>. CronJob represents the configuration of a single cron job.</li>
<li><em>HorizontalPodAutoscaler</em>. configuration of a horizontal pod autoscaler.</li>
<li><em>HorizontalPodAutoscaler</em>. HorizontalPodAutoscaler is the configuration for a horizontal pod autoscaler, which automatically manages the replica count of any resource implementing the scale subresource based on the metrics specified.</li>
<li><em>HorizontalPodAutoscaler v2beta2</em>. HorizontalPodAutoscaler is the configuration for a horizontal pod autoscaler, which automatically manages the replica count of any resource implementing the scale subresource based on the metrics specified.</li>
<li><em>PriorityClass</em>. PriorityClass defines mapping from a priority class name to the priority integer value.</li>
</ul>
</li>
<li>Service Resources<ul>
<li><em>Service</em>. Service is a named abstraction of software service (for example, mysql) consisting of local port (for example 3306) that the proxy listens on, and the selector that determines which pods will answer requests sent through the proxy.</li>
<li><em>Endpoints</em>. Endpoints is a collection of endpoints that implement the actual service.</li>
<li><em>EndpointSlice</em>. EndpointSlice represents a subset of the endpoints that implement a service.</li>
<li><em>Ingress</em>. Ingress is a collection of rules that allow inbound connections to reach the endpoints defined by a backend.</li>
<li><em>IngressClass</em>. IngressClass represents the class of the Ingress, referenced by the Ingress Spec.</li>
</ul>
</li>
<li>Config and Storage Resources<ul>
<li><em>ConfigMap</em>. ConfigMap holds configuration data for pods to consume.</li>
<li><em>Secret</em>. Secret holds secret data of a certain type.</li>
<li><em>Volume</em>. Volume represents a named volume in a pod that may be accessed by any container in the pod.</li>
<li><em>PersistentVolumeClaim</em>. PersistentVolumeClaim is a user's request for and claim to a persistent volume.</li>
<li><em>PersistentVolume</em>. PersistentVolume (PV) is a storage resource provisioned by an administrator.</li>
<li><em>StorageClass</em>. StorageClass describes the parameters for a class of storage for which PersistentVolumes can be dynamically provisioned.</li>
<li><em>VolumeAttachment</em>. VolumeAttachment captures the intent to attach or detach the specified volume to/from the specified node.</li>
<li><em>CSIDriver</em>. CSIDriver captures information about a Container Storage Interface (CSI) volume driver deployed on the cluster.</li>
<li><em>CSINode</em>. CSINode holds information about all CSI drivers installed on a node.</li>
<li><em>CSIStorageCapacity</em>. CSIStorageCapacity stores the result of one CSI GetCapacity call.</li>
</ul>
</li>
<li>Authentication Resources<ul>
<li><em>ServiceAccount</em>. ServiceAccount binds together: <ul>
<li>a name, understood by users, and perhaps by peripheral systems, for an identity </li>
<li>a principal that can be authenticated and authorized </li>
<li>a set of secrets.</li>
</ul>
</li>
<li><em>TokenRequest</em>. TokenRequest requests a token for a given service account.</li>
<li><em>TokenReview</em>. TokenReview attempts to authenticate a token to a known user.</li>
<li><em>CertificateSigningRequest</em>. CertificateSigningRequest objects provide a mechanism to obtain x509 certificates by submitting a certificate signing request, and having it asynchronously approved and issued.</li>
</ul>
</li>
<li>Authorization Resources<ul>
<li><em>LocalSubjectAccessReview</em>. LocalSubjectAccessReview checks whether or not a user or group can perform an action in a given namespace.</li>
<li><em>SelfSubjectAccessReview</em>. SelfSubjectAccessReview checks whether or the current user can perform an action.</li>
<li><em>SelfSubjectRulesReview</em>. SelfSubjectRulesReview enumerates the set of actions the current user can perform within a namespace.</li>
<li><em>SubjectAccessReview</em>. SubjectAccessReview checks whether or not a user or group can perform an action.</li>
<li><em>ClusterRole</em>. ClusterRole is a cluster level, logical grouping of PolicyRules that can be referenced as a unit by a RoleBinding or ClusterRoleBinding.</li>
<li><em>ClusterRoleBinding</em>. ClusterRoleBinding references a ClusterRole, but not contain it.</li>
<li><em>Role</em>. Role is a namespaced, logical grouping of PolicyRules that can be referenced as a unit by a RoleBinding.</li>
<li><em>RoleBinding</em>. RoleBinding references a role, but does not contain it.</li>
</ul>
</li>
<li>Policy Resources<ul>
<li><em>LimitRange</em>. LimitRange sets resource usage limits for each kind of resource in a Namespace.</li>
<li><em>ResourceQuota</em>. ResourceQuota sets aggregate quota restrictions enforced per namespace.</li>
<li><em>NetworkPolicy</em>. NetworkPolicy describes what network traffic is allowed for a set of Pods.</li>
<li><em>PodDisruptionBudget</em>. PodDisruptionBudget is an object to define the max disruption that can be caused to a collection of pods.</li>
<li><em>PodSecurityPolicy v1beta1</em>. PodSecurityPolicy governs the ability to make requests that affect the Security Context that will be applied to a pod and container.</li>
</ul>
</li>
<li>Extend Resources<ul>
<li><em>CustomResourceDefinition</em>. CustomResourceDefinition represents a resource that should be exposed on the API server.</li>
<li><em>MutatingWebhookConfiguration</em>. MutatingWebhookConfiguration describes the configuration of and admission webhook that accept or reject and may change the object.</li>
<li>*ValidatingWebhookConfiguration(). ValidatingWebhookConfiguration describes the configuration of and admission webhook that accept or reject and object without changing it.</li>
</ul>
</li>
<li>Cluster Resources<ul>
<li><em>Node</em>. Node is a worker node in Kubernetes.</li>
<li><em>Namespace</em>. Namespace provides a scope for Names.</li>
<li><em>Event</em>. Event is a report of an event somewhere in the cluster.</li>
<li><em>APIService</em>. APIService represents a server for a particular GroupVersion.</li>
<li><em>Lease</em>. Lease defines a lease concept.</li>
<li><em>RuntimeClass</em>. RuntimeClass defines a class of container runtime supported in the cluster.</li>
<li><em>FlowSchema v1beta2</em>. FlowSchema defines the schema of a group of flows.</li>
<li><em>PriorityLevelConfiguration v1beta2</em>. PriorityLevelConfiguration represents the configuration of a priority level.</li>
<li><em>Binding</em>. Binding ties one object to another; for example, a pod is bound to a node by a scheduler.</li>
<li><em>ComponentStatus</em>. ComponentStatus (and ComponentStatusList) holds the cluster validation info.</li>
</ul>
</li>
</ul>
<p>Command <code>kube api-resources</code> to get the supported API resources.</p>
<p>Command <code>kubectl explain RESOURCE [options]</code> describes the fields associated with each supported API resource. 
Fields are identified via a simple JSONPath identifier:</p>
<pre><code>kubectl explain binding
kubectl explain binding.metadata
kubectl explain binding.metadata.name
</code></pre>
<h2 id="workload-resources">Workload Resources</h2>
<h3 id="pods">Pods</h3>
<p>Pods are the smallest deployable units of computing that you can create and manage in Kubernetes.</p>
<p>A Pod is a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers.</p>
<p>A Pod's contents are always co-located and co-scheduled, and run in a shared context. </p>
<p>A Pod models an application-specific "logical host": it contains one or more application containers which are relatively tightly coupled. </p>
<p>In non-cloud contexts, applications executed on the same physical or virtual machine are analogous to cloud applications executed on the same logical host.</p>
<p>The shared context of a Pod is a set of Linux namespaces, cgroups, and potentially other facets of isolation - the same things that isolate a Docker container.</p>
<p>In terms of Docker concepts, a Pod is similar to a group of Docker containers with shared namespaces and shared filesystem volumes.</p>
<p>Usually you don't need to create Pods directly, even singleton Pods. Instead, create them using workload resources such as <em>Deployment</em> or <em>Job</em>. 
If your Pods need to track state, consider the StatefulSet resource.</p>
<p>Pods in a Kubernetes cluster are used in two main ways:</p>
<ul>
<li>Pods that run a single container. </li>
<li>Pods that run multiple containers that need to work together. </li>
</ul>
<p>The "one-container-per-Pod" model is the most common Kubernetes use case; 
in this case, you can think of a Pod as a wrapper around a single container; 
Kubernetes manages Pods rather than managing the containers directly.</p>
<p>A Pod can encapsulate an application composed of multiple co-located containers that are tightly coupled and need to share resources. </p>
<p>These co-located containers form a single cohesive unit of service—for example, one container serving data stored in a shared volume to the public, 
while a separate sidecar container refreshes or updates those files. 
The Pod wraps these containers, storage resources, and an ephemeral network identity together as a single unit.</p>
<p>Grouping multiple co-located and co-managed containers in a single Pod is a relatively advanced use case. 
You should use this pattern <em>only</em> in specific instances in which your containers are tightly coupled.</p>
<p>Each Pod is meant to run a single instance of a given application. 
If you want to scale your application horizontally (to provide more overall resources by running more instances), you should use multiple Pods, one for each instance. 
In Kubernetes, this is typically referred to as <em>replication</em>. Replicated Pods are usually created and managed as a group by a workload resource and its controller.</p>
<p>Pods natively provide two kinds of shared resources for their constituent containers: <em><a href="https://kubernetes.io/docs/concepts/workloads/pods/#pod-networking">networking</a></em> and <em><a href="https://kubernetes.io/docs/concepts/workloads/pods/#pod-storage">storage</a></em>.</p>
<p>A Pod can specify a set of shared storage volumes. All containers in the Pod can access the shared volumes, allowing those containers to share data. </p>
<p>Each Pod is assigned a unique IP address for each address family.
Within a Pod, containers share an IP address and port space, and can find each other via <code>localhost</code>.
Containers that want to interact with a container running in a different Pod can use IP networking to communicate.</p>
<p>When a Pod gets created, the new Pod is scheduled to run on a Node in your cluster. 
The Pod remains on that node until the Pod finishes execution, the Pod object is deleted, the Pod is evicted for lack of resources, or the node fails.</p>
<p>Restarting a container in a Pod should not be confused with restarting a Pod. 
A Pod is not a process, but an environment for running container(s). 
A Pod persists until it is deleted.</p>
<p>You can use workload resources (e.g., Deployment, StatefulSet, DaemonSet) to create and manage multiple Pods for you. 
A controller for the resource handles replication and rollout and automatic healing in case of Pod failure.</p>
<p><img alt="Pod with multiple containers" src="https://d33wubrfki0l68.cloudfront.net/aecab1f649bc640ebef1f05581bfcc91a48038c4/728d6/images/docs/pod.svg" /></p>
<h4 id="initcontainer">InitContainer</h4>
<p>Some Pods have init containers as well as app containers. Init containers run and complete before the app containers are started.</p>
<p>You can specify init containers in the Pod specification alongside the containers array (which describes app containers).</p>
<h4 id="static-pod">Static Pod</h4>
<p>Static Pods are managed directly by the kubelet daemon on a specific node, without the API server observing them. </p>
<p>Static Pods are always bound to one Kubelet on a specific node. </p>
<p>The main use for static Pods is to run a self-hosted control plane: in other words, using the kubelet to supervise the individual control plane components.</p>
<p>The kubelet automatically tries to create a mirror Pod on the Kubernetes API server for each static Pod. 
This means that the Pods running on a node are visible on the API server, but cannot be controlled from there.</p>
<h4 id="container-probes">Container probes</h4>
<p>A probe is a diagnostic performed periodically by the kubelet on a container. </p>
<p>To perform a diagnostic, the kubelet either executes code within the container, or makes a network request.</p>
<p>There are four different ways to check a container using a probe. Each probe must define exactly one of these four mechanisms:</p>
<ul>
<li><em>exec</em>. The diagnostic is considered successful if the command exits with a status code of 0.</li>
<li><em>grpc</em>. The diagnostic is considered successful if the status of the response is SERVING.</li>
<li><em>httpGet</em>. The diagnostic is considered successful if the response has a status code greater than or equal to 200 and less than 400.</li>
<li><em>tcpSocket</em>. The diagnostic is considered successful if the port is open.</li>
</ul>
<p>Each probe has one of three results:</p>
<ul>
<li>Success</li>
<li>Failure</li>
<li>Unknown</li>
</ul>
<p>Types of probe:</p>
<ul>
<li><em>livenessProbe</em>. Indicates whether the container is running. </li>
<li><em>readinessProbe</em>. Indicates whether the container is ready to respond to requests.</li>
<li><em>startupProbe</em>. Indicates whether the application within the container is started.</li>
</ul>
<h3 id="deployment">Deployment</h3>
<h3 id="replicaset">ReplicaSet</h3>
<p>A ReplicaSet’s purpose is to maintain a stable set of replica Pods running at any given time. 
As such, it is often used to guarantee the availability of a specified number of identical Pods.</p>
<p>You may never need to manipulate ReplicaSet objects: use a Deployment instead, and define your application in the spec section.</p>
<p>You can specify how many Pods should run concurrently by setting <code>replicaset.spec.replicas</code>. 
The ReplicaSet will create/delete its Pods to match this number.
If you do not specify <code>replicaset.spec.replicas</code>, then it defaults to <code>1</code>.</p>
<h3 id="statefulset">StatefulSet</h3>
<h3 id="daemonset">DaemonSet</h3>
<p>A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are removed from the cluster, those Pods are garbage collected. </p>
<p>Deleting a DaemonSet will clean up the Pods it created.</p>
<p>Some typical uses of a DaemonSet are:</p>
<ul>
<li>running a cluster storage daemon on every node</li>
<li>running a logs collection daemon on every node</li>
<li>running a node monitoring daemon on every node</li>
</ul>
<p>In a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon. </p>
<p>A more complex setup might use multiple DaemonSets for a single type of daemon, but with different flags and/or different memory and cpu requests for different hardware types.</p>
<p>The DaemonSet controller reconciliation process reviews both existing nodes and newly created nodes. </p>
<p>By default, the Kubernetes scheduler ignores the pods created by the DamonSet, and lets them exist on the node until the node itself is shut down. </p>
<p>Running Pods on select Nodes:</p>
<ul>
<li>If you specify a <code>daemonset.spec.template.spec.nodeSelector</code>, then the DaemonSet controller will create Pods on nodes which match that node selector. </li>
<li>If you specify a <code>daemonset.spec.template.spec.affinity</code>, then DaemonSet controller will create Pods on nodes which match that node affinity. </li>
<li>If you do not specify either, then the DaemonSet controller will create Pods on all nodes.</li>
</ul>
<p>There is no field <code>replicas</code> in <code>kubectl explain daemonset.spec</code> against with <code>kubectl explain deployment.spec.replicas</code>.
When a DaemonSet is created, each node will have <em>one</em> DaemonSet Pod running.</p>
<p>We’ll use a <code>Deployment</code>/<code>ReplicaSet</code> for services, mostly stateless, where we don’t care where the node is running, 
but we care more about the number of copies of our pod is running, and we can scale those copies/replicas up or down. 
Rolling updates would also be a benefit here.</p>
<p>We’ll use a <code>DaemonSet</code> when a copy of our pod must be running on the specific nodes that we require. 
Our daemon pod also needs to start before any of our other pods.</p>
<p>A DaemonSet is a simple scalability strategy for background services. 
When more eligible nodes are added to the cluster, the background service scales up. 
When nodes are removed, it will automatically scale down.</p>
<h3 id="job">Job</h3>
<h3 id="cronjob">CronJob</h3>
<h2 id="service-resource">Service Resource</h2>
<h3 id="service">Service</h3>
<p>Service is a named abstraction of software service (for example, mysql) consisting of local port (for example 3306) that the proxy listens on, and the selector that determines which pods will answer requests sent through the proxy.</p>
<p>The set of Pods targeted by a Service is usually determined by a selector (label selector). </p>
<p>Type of service resource:</p>
<ul>
<li>ClusterIP Service (default): Reliable IP, DNS, and Port. Internal acess only.</li>
<li>NodePort Service: Expose to external access.</li>
<li>LoadBalancer: Based on NodePort and integrated with loader balance provided by cloud venders (e.g., AWS, GCP, etc.).</li>
<li>ExternalName: Acces will be trafficed to external service.</li>
</ul>
<p>Here is an example of yaml file to create a Service.</p>
<pre><code>apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  labels:
    tier: application
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    run: nginx
  type: NodePort
</code></pre>
<p>Here is an example of Service.</p>
<ul>
<li>IP<code>10.96.17.77</code> is ClusterIP(VIP) of the service</li>
<li>Port <code>&lt;unset&gt;  80/TCP</code> is the port on Pod that service listening within the cluster.</li>
<li>TargetPort <code>8080/TCP</code> is the port on the container that the service should direct traffic to.</li>
<li>NodePort <code>&lt;unset&gt;  31893/TCP</code> is the port that can be accessed outside. </li>
<li>Endpoints show the list of Pods matched the service labels. </li>
</ul>
<pre><code>Name:                     nginx-deployment
Namespace:                jh-namespace
Labels:                   tier=application
Annotations:              &lt;none&gt;
Selector:                 run=nginx
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.96.17.77
IPs:                      10.96.17.77
Port:                     &lt;unset&gt;  80/TCP
TargetPort:               8080/TCP
NodePort:                 &lt;unset&gt;  31893/TCP
Endpoints:                10.244.1.177:8080,10.244.1.178:8080,10.244.1.179:8080 + 7 more...
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   &lt;none&gt;
</code></pre>
<p>Service <code>kube-dns</code> beyond Deployment <code>coredns</code> provides cluster DNS service in Kubernetes cluster. </p>
<p>Service registration:</p>
<ul>
<li>Kubernetes uses cluster DNS as service registration.</li>
<li>Registration is Service based, not Pod based.</li>
<li>Cluster DNS (CoreDNS) is monitoring and discvering new service actively.</li>
<li>Service Name, IP, Port will be registered.</li>
</ul>
<p>Procedure of Service registration.</p>
<ul>
<li>POST new Service to API Server.</li>
<li>Assign ClusterIP to the new Service.</li>
<li>Save new Service configuration info to etcd.</li>
<li>Create endpoints with related Pod IPs associated with the new Service.</li>
<li>Explore the new Service by ClusterDNS.</li>
<li>Create DNS info.</li>
<li>kube-proxy fetch Service configration info.</li>
<li>Create IPSV rule.</li>
</ul>
<p>Procedure of Service discovery.</p>
<ul>
<li>Request DNS name resolution for a Service name.</li>
<li>Receive ClusterIP.</li>
<li>Traffic access to ClusterIP.</li>
<li>No router. Forward request to Pod's default gateway.</li>
<li>Forward request to node.</li>
<li>No router. Forward request to Node's default gateway.</li>
<li>Proceed the request by Node kernel.</li>
<li>Trap the request by IPSV rule.</li>
<li>Put destination Pod's IP into the request's destination IP. </li>
<li>The request arrives destination Pod.</li>
</ul>
<p>Get DNS configuration.</p>
<pre><code>root@cka001:/etc# kubectl get service kube-dns -n kube-system
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
kube-dns   ClusterIP   10.96.0.10   &lt;none&gt;        53/UDP,53/TCP,9153/TCP   7d7h


root@cka001:/etc# cat resolv.conf 
# Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8)
#     DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN
# 127.0.0.53 is the systemd-resolved stub resolver.
# run &quot;systemd-resolve --status&quot; to see details about the actual nameservers.
nameserver 127.0.0.53
options timeout:2 attempts:3 rotate single-request-reopen


root@cka001:/etc# systemd-resolve --status
  Current DNS Server: 100.100.2.136
         DNS Servers: 100.100.2.136
                      100.100.2.138
</code></pre>
<p>Get information of <code>kube-dns</code>.</p>
<pre><code>root@cka001:~# kubectl describe service kube-dns -n kube-system
Name:              kube-dns
Namespace:         kube-system
Labels:            k8s-app=kube-dns
                   kubernetes.io/cluster-service=true
                   kubernetes.io/name=CoreDNS
Annotations:       prometheus.io/port: 9153
                   prometheus.io/scrape: true
Selector:          k8s-app=kube-dns
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.0.10
IPs:               10.96.0.10
Port:              dns  53/UDP
TargetPort:        53/UDP
Endpoints:         10.244.0.2:53,10.244.0.3:53
Port:              dns-tcp  53/TCP
TargetPort:        53/TCP
Endpoints:         10.244.0.2:53,10.244.0.3:53
Port:              metrics  9153/TCP
TargetPort:        9153/TCP
Endpoints:         10.244.0.2:9153,10.244.0.3:9153
Session Affinity:  None
Events:            &lt;none&gt;
</code></pre>
<p>FQDN format: <code>&lt;object-name&gt;.&lt;namespace&gt;.svc.cluster.local</code>. We call <code>&lt;object-name&gt;</code> as unqualified name, or short name.</p>
<p>Namespaces can partition the cluster's address space. At the same time, it can also be used to implement access control and resource quotas.</p>
<h3 id="endpoints">Endpoints</h3>
<p>Endpoints is a collection of endpoints that implement the actual service.</p>
<p>When a service is created, it associates with a Endpoint object, <code>kubectl get endpoints &lt;service_name&gt;</code>.</p>
<p>A list of matched Pod by service label is maintained as Endpoint object, add new matched Pods and remove not matched Pods.</p>
<h2 id="tutorials">Tutorials</h2>
<ul>
<li>
<p><a href="../KubernetesTutorials-Local-Deploy/">Tutorials: local deployment</a></p>
</li>
<li>
<p><a href="../KubernetesTutorials-BTP-trail/">Tutorials: Kyma@SAP BTP</a></p>
</li>
<li>
<p><a href="../KubernetesTutorials-Aliyun-Ubuntu/">Tutorials: Ubuntu@Aliyun</a></p>
</li>
<li>
<p><a href="../KubernetesTutorials-Aliyun-openSUSE/">Tutorials: openSUSE@Aliyun</a></p>
</li>
</ul>
                
              
              
                


              
            </article>
          </div>
        </div>
        
      </main>
      
        
<footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        
          Made with
          <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
            Material for MkDocs
          </a>
        
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.fcfe8b6d.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.b1047164.min.js"></script>
      
    
  </body>
</html>