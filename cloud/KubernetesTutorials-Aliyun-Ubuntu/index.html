
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="canonical" href="https://huyuhui001.github.io/mySite/index.html/cloud/KubernetesTutorials-Aliyun-Ubuntu/">
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-7.3.6">
    
    
      
        <title>Kubernetes Tutourials: Ubuntu@Aliyun - MEMO</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.a57b2b03.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.3f5d1f46.min.css">
        
          
          
          <meta name="theme-color" content="#2094f3">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
    
      


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="blue" data-md-color-accent="deep-blue">
  
    
    <script>function __prefix(e){return new URL("../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#kubernetes-tutourials-ubuntualiyun" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="MEMO" class="md-header__button md-logo" aria-label="MEMO" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MEMO
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Kubernetes Tutourials: Ubuntu@Aliyun
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="MEMO" class="md-nav__button md-logo" aria-label="MEMO" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    MEMO
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../linux/" class="md-nav__link">
        Linux
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../python/" class="md-nav__link">
        Python
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        Cloud
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../about/" class="md-nav__link">
        About
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#deployment" class="md-nav__link">
    Deployment
  </a>
  
    <nav class="md-nav" aria-label="Deployment">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#preparation" class="md-nav__link">
    Preparation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#initialize-vms" class="md-nav__link">
    Initialize VMs
  </a>
  
    <nav class="md-nav" aria-label="Initialize VMs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#configure-etchosts-file" class="md-nav__link">
    Configure /etc/hosts file
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#disable-firewall" class="md-nav__link">
    Disable firewall
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#turn-off-swap" class="md-nav__link">
    Turn off swap
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set-timezone-and-locale" class="md-nav__link">
    Set timezone and locale
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kernel-setting" class="md-nav__link">
    Kernel setting
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#install-containerd" class="md-nav__link">
    Install Containerd
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#install-nerdctl" class="md-nav__link">
    Install nerdctl
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#install-kubeadm" class="md-nav__link">
    Install kubeadm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup-master-node" class="md-nav__link">
    Setup Master Node
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup-work-nodes" class="md-nav__link">
    Setup Work Nodes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#install-flannel" class="md-nav__link">
    Install Flannel
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#check-cluster-status" class="md-nav__link">
    Check Cluster Status
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reset-cluster" class="md-nav__link">
    Reset cluster
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#troubleshooting" class="md-nav__link">
    Troubleshooting
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#snapshot-of-deployment" class="md-nav__link">
    Snapshot of deployment
  </a>
  
    <nav class="md-nav" aria-label="Snapshot of deployment">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#container-layer" class="md-nav__link">
    Container Layer
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kubernetes-layer" class="md-nav__link">
    Kubernetes Layer
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#case-study" class="md-nav__link">
    Case Study
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kubectl" class="md-nav__link">
    kubectl
  </a>
  
    <nav class="md-nav" aria-label="kubectl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#config-file" class="md-nav__link">
    Config File
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bash-autocomplete" class="md-nav__link">
    Bash Autocomplete
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#common-usage" class="md-nav__link">
    Common Usage
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kubernetes-api-and-resource" class="md-nav__link">
    Kubernetes API and Resource
  </a>
  
    <nav class="md-nav" aria-label="Kubernetes API and Resource">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#demo-static-pod" class="md-nav__link">
    Demo: Static Pod
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#demo-init-containers" class="md-nav__link">
    Demo: Init containers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#demo-mutil-container-pod" class="md-nav__link">
    Demo Mutil-Container Pod
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#demo-usage-of-kubectl" class="md-nav__link">
    Demo: Usage of kubectl
  </a>
  
    <nav class="md-nav" aria-label="Demo: Usage of kubectl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#grant-authorization-to-serviceaccount" class="md-nav__link">
    Grant Authorization to ServiceAccount
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#label-node" class="md-nav__link">
    Label Node
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deployment_1" class="md-nav__link">
    Deployment
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#namespace" class="md-nav__link">
    Namespace
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#expose-service" class="md-nav__link">
    Expose Service
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scalling" class="md-nav__link">
    Scalling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rolling" class="md-nav__link">
    Rolling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#event" class="md-nav__link">
    Event
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logging" class="md-nav__link">
    Logging
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#demo-workload-resources" class="md-nav__link">
    Demo: Workload Resources
  </a>
  
    <nav class="md-nav" aria-label="Demo: Workload Resources">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#deployment_2" class="md-nav__link">
    Deployment
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#statefulset" class="md-nav__link">
    StatefulSet
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#daemonset" class="md-nav__link">
    DaemonSet
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#job" class="md-nav__link">
    Job
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cronjob" class="md-nav__link">
    Cronjob
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#label-and-annotation" class="md-nav__link">
    Label and Annotation
  </a>
  
    <nav class="md-nav" aria-label="Label and Annotation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#demo-label-and-annotation" class="md-nav__link">
    Demo: Label and Annotation
  </a>
  
    <nav class="md-nav" aria-label="Demo: Label and Annotation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#label" class="md-nav__link">
    Label
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#annotation" class="md-nav__link">
    Annotation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#health-check" class="md-nav__link">
    Health Check
  </a>
  
    <nav class="md-nav" aria-label="Health Check">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#status-of-pod-and-container" class="md-nav__link">
    Status of Pod and Container
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#livenessprobe" class="md-nav__link">
    LivenessProbe
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#readinessprobe" class="md-nav__link">
    ReadinessProbe
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#demo-of-health-check" class="md-nav__link">
    Demo of Health Check
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#namespace_1" class="md-nav__link">
    Namespace
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#horizontal-pod-autoscaling-hpa" class="md-nav__link">
    Horizontal Pod Autoscaling (HPA)
  </a>
  
    <nav class="md-nav" aria-label="Horizontal Pod Autoscaling (HPA)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#deploy-a-service-podinfo" class="md-nav__link">
    Deploy a Service podinfo
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#config-hpa" class="md-nav__link">
    Config HPA
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stress-testing" class="md-nav__link">
    Stress Testing
  </a>
  
    <nav class="md-nav" aria-label="Stress Testing">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#install-ab" class="md-nav__link">
    Install ab
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#concurrency-stres-test" class="md-nav__link">
    Concurrency Stres Test
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#service" class="md-nav__link">
    Service
  </a>
  
    <nav class="md-nav" aria-label="Service">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#clusterip" class="md-nav__link">
    ClusterIP
  </a>
  
    <nav class="md-nav" aria-label="ClusterIP">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#create-service" class="md-nav__link">
    Create Service
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#expose-service_1" class="md-nav__link">
    Expose Service
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nodeport" class="md-nav__link">
    NodePort
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#special-service" class="md-nav__link">
    Special Service
  </a>
  
    <nav class="md-nav" aria-label="Special Service">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#headless-service" class="md-nav__link">
    Headless Service
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ingress" class="md-nav__link">
    Ingress
  </a>
  
    <nav class="md-nav" aria-label="Ingress">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#deploy-ingress-controller" class="md-nav__link">
    Deploy Ingress Controller
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-deployments" class="md-nav__link">
    Create Deployments
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-service_1" class="md-nav__link">
    Create Service
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-ingress" class="md-nav__link">
    Create Ingress
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test-accessiblity" class="md-nav__link">
    Test Accessiblity
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#storage" class="md-nav__link">
    Storage
  </a>
  
    <nav class="md-nav" aria-label="Storage">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#emptydir" class="md-nav__link">
    emptyDir
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hostpath" class="md-nav__link">
    hostPath
  </a>
  
    <nav class="md-nav" aria-label="hostPath">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#verify-mysql-availability" class="md-nav__link">
    Verify MySQL Availability
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pv-and-pvc" class="md-nav__link">
    PV and PVC
  </a>
  
    <nav class="md-nav" aria-label="PV and PVC">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#set-up-nfs-server" class="md-nav__link">
    Set up NFS Server
  </a>
  
    <nav class="md-nav" aria-label="Set up NFS Server">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#install-nfs-kernel-server" class="md-nav__link">
    Install nfs-kernel-server
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configure-share-folder" class="md-nav__link">
    Configure Share Folder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#install-nfs-client" class="md-nav__link">
    Install NFS Client
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#verify-nfs-server" class="md-nav__link">
    Verify NFS Server
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mount-nfs" class="md-nav__link">
    Mount NFS
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-pv" class="md-nav__link">
    Create PV
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-pvc" class="md-nav__link">
    Create PVC
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#consume-pvc" class="md-nav__link">
    Consume PVC
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#storageclass" class="md-nav__link">
    StorageClass
  </a>
  
    <nav class="md-nav" aria-label="StorageClass">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#configure-rbac-authorization" class="md-nav__link">
    Configure RBAC Authorization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#install-nfs-provisioner" class="md-nav__link">
    Install nfs-provisioner
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-nfs-storageclass" class="md-nav__link">
    Create NFS StorageClass
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#verify" class="md-nav__link">
    Verify
  </a>
  
    <nav class="md-nav" aria-label="Verify">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#create-pvc_1" class="md-nav__link">
    Create PVC
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#consume-pvc_1" class="md-nav__link">
    Consume PVC
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configuration" class="md-nav__link">
    Configuration
  </a>
  
    <nav class="md-nav" aria-label="Configuration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#configmap" class="md-nav__link">
    ConfigMap
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#secret" class="md-nav__link">
    Secret
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#additional-keys" class="md-nav__link">
    Additional Keys
  </a>
  
    <nav class="md-nav" aria-label="Additional Keys">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#various-way-to-create-configmap" class="md-nav__link">
    Various way to create ConfigMap
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set-environment-variable-via-configmap" class="md-nav__link">
    Set environment variable via ConfigMap
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#scheduling" class="md-nav__link">
    Scheduling
  </a>
  
    <nav class="md-nav" aria-label="Scheduling">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#nodeselector" class="md-nav__link">
    nodeSelector
  </a>
  
    <nav class="md-nav" aria-label="nodeSelector">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#label-node_1" class="md-nav__link">
    Label Node
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configure-nodeselector-for-pod" class="md-nav__link">
    Configure nodeSelector for Pod
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nodename" class="md-nav__link">
    nodeName
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#affinity" class="md-nav__link">
    Affinity
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#taints-tolerations" class="md-nav__link">
    Taints &amp; Tolerations
  </a>
  
    <nav class="md-nav" aria-label="Taints &amp; Tolerations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#concept" class="md-nav__link">
    Concept
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set-taints" class="md-nav__link">
    Set Taints
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set-tolerations" class="md-nav__link">
    Set Tolerations
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#remove-taints" class="md-nav__link">
    Remove Taints
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resourcequota" class="md-nav__link">
    ResourceQuota
  </a>
  
    <nav class="md-nav" aria-label="ResourceQuota">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#create-namespace" class="md-nav__link">
    Create Namespace
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-resourcequota" class="md-nav__link">
    Create ResourceQuota
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#verify-test" class="md-nav__link">
    Verify &amp; Test
  </a>
  
    <nav class="md-nav" aria-label="Verify &amp; Test">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#test-nodeport" class="md-nav__link">
    Test NodePort
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test-pvc" class="md-nav__link">
    Test PVC
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#limitrange" class="md-nav__link">
    LimitRange
  </a>
  
    <nav class="md-nav" aria-label="LimitRange">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#create-namespace_1" class="md-nav__link">
    Create Namespace
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set-limitrange" class="md-nav__link">
    Set LimitRange
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test-via-pod" class="md-nav__link">
    Test via Pod
  </a>
  
    <nav class="md-nav" aria-label="Test via Pod">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#scenario-1-pod-without-specified-limits" class="md-nav__link">
    Scenario 1: Pod without specified limits
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scenario-2-pod-with-cpu-limit-without-cpu-request" class="md-nav__link">
    Scenario 2: Pod with CPU limit, without CPU Request
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scenario-3-pod-with-cpu-request-onlyl-without-cpu-limits" class="md-nav__link">
    Scenario 3: Pod with CPU Request onlyl, without CPU Limits
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#troubleshooting_1" class="md-nav__link">
    Troubleshooting
  </a>
  
    <nav class="md-nav" aria-label="Troubleshooting">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#event_1" class="md-nav__link">
    Event
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logs" class="md-nav__link">
    Logs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#monitoring-indicators" class="md-nav__link">
    Monitoring Indicators
  </a>
  
    <nav class="md-nav" aria-label="Monitoring Indicators">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#nodes" class="md-nav__link">
    Nodes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#node-eviction" class="md-nav__link">
    Node Eviction
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rbac" class="md-nav__link">
    RBAC
  </a>
  
    <nav class="md-nav" aria-label="RBAC">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#install-cfssl" class="md-nav__link">
    Install cfssl
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-user" class="md-nav__link">
    Create user
  </a>
  
    <nav class="md-nav" aria-label="Create user">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#create-file-ca-configjson" class="md-nav__link">
    Create file ca-config.json
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-csr-file-for-signature" class="md-nav__link">
    Create csr file for signature
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-file-kubeconfig" class="md-nav__link">
    Create file kubeconfig
  </a>
  
    <nav class="md-nav" aria-label="Create file kubeconfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#set-up-cluster" class="md-nav__link">
    Set up cluster
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set-up-user" class="md-nav__link">
    Set up user
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set-up-context" class="md-nav__link">
    Set up Context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#verify_1" class="md-nav__link">
    Verify
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#role-rolebinding" class="md-nav__link">
    Role &amp; RoleBinding
  </a>
  
    <nav class="md-nav" aria-label="Role &amp; RoleBinding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#create-role-and-rolebinding" class="md-nav__link">
    Create Role and RoleBinding
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#verify-authorization" class="md-nav__link">
    Verify Authorization
  </a>
  
    <nav class="md-nav" aria-label="Verify Authorization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#check-pod-status" class="md-nav__link">
    Check Pod Status
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#check-node-status" class="md-nav__link">
    Check Node Status
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#delete-pod" class="md-nav__link">
    Delete Pod
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#check-pods-in-other-namespace" class="md-nav__link">
    Check Pods in other Namespace
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clusterrole-clusterrolebinding" class="md-nav__link">
    ClusterRole &amp; ClusterRoleBinding
  </a>
  
    <nav class="md-nav" aria-label="ClusterRole &amp; ClusterRoleBinding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#create-clusterrole-and-clusterrolebinding" class="md-nav__link">
    Create ClusterRole and ClusterRoleBinding
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#verify-authorization_1" class="md-nav__link">
    Verify Authorization
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#network-policy" class="md-nav__link">
    Network Policy
  </a>
  
    <nav class="md-nav" aria-label="Network Policy">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#inbound-rules" class="md-nav__link">
    Inbound Rules
  </a>
  
    <nav class="md-nav" aria-label="Inbound Rules">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#create-workload-for-test" class="md-nav__link">
    Create workload for test.
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deny-for-all-ingress" class="md-nav__link">
    Deny For All Ingress
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#allow-for-specific-ingress" class="md-nav__link">
    Allow For Specific Ingress
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#verify-networkpolicy" class="md-nav__link">
    Verify NetworkPolicy
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inbound-across-namespace" class="md-nav__link">
    Inbound Across Namespace
  </a>
  
    <nav class="md-nav" aria-label="Inbound Across Namespace">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#create-workload-and-namespace-for-test" class="md-nav__link">
    Create workload and namespace for test
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-allow-ingress" class="md-nav__link">
    Create Allow Ingress
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#verify-policy" class="md-nav__link">
    Verify Policy
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="kubernetes-tutourials-ubuntualiyun">Kubernetes Tutourials: Ubuntu@Aliyun</h1>
<h2 id="deployment">Deployment</h2>
<h3 id="preparation">Preparation</h3>
<p>Register Aliyun account via <a href="https://home.console.aliyun.com/home/dashboard/ProductAndService">Alibaba Cloud home console</a>.</p>
<p>Request three Elastic Computer Service(ECS) instances with below sizing:</p>
<ul>
<li>System: 2vCPU+4GiB</li>
<li>OS: Ubuntu  20.04 x86_64</li>
<li>Instance Type: ecs.sn1.medium </li>
<li>Instance Name: cka001, cka002, cka003</li>
<li>Network: both public IPs and private IPs</li>
<li>Maximum Bandwidth: 100Mbps (Peak Value)</li>
<li>Cloud disk: 40GiB</li>
<li>Billing Method: Preemptible instance (spot price)</li>
</ul>
<p>Generate SSH key pairs with name <code>cka-key-pair</code> in local directcory <code>/opt</code>.</p>
<p>Change access control to <code>400</code> per security required by command <code>sudo chmod 400 cka-key-pair.pem</code>.
cka003
Access remote cka servers via command <code>ssh -i cka-key-pair.pem root@&lt;your public ip address&gt;</code></p>
<h3 id="initialize-vms">Initialize VMs</h3>
<h4 id="configure-etchosts-file">Configure /etc/hosts file</h4>
<p>Add private IPs in the <code>/etc/hosts</code> file in all VMs.</p>
<h4 id="disable-firewall">Disable firewall</h4>
<p>Disable firewall by command <code>ufw disable</code> in all VMs.</p>
<h3 id="turn-off-swap">Turn off swap</h3>
<p>Turn off swap by command <code>swapoff -a</code> in all VMs.</p>
<p>Disable swap on Ubuntu.</p>
<pre><code>sudo ufw disable
</code></pre>
<p>Check status of swap on Ubuntu.</p>
<pre><code>sudo ufw status verbose
</code></pre>
<h3 id="set-timezone-and-locale">Set timezone and locale</h3>
<p>Set timezone and local for all VMs. For ECS with Ubuntu 20.04 version created by Aliyun, this step is not needed.</p>
<pre><code># ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
# sudo echo 'LANG=&quot;en_US.UTF-8&quot;' &gt;&gt; /etc/profile
# source /etc/profile
</code></pre>
<p>Something like this:</p>
<pre><code>root@cka001:~# ll /etc/localtime
lrwxrwxrwx 1 root root 33 May 24 18:14 /etc/localtime -&gt; /usr/share/zoneinfo/Asia/Shanghai
</code></pre>
<h3 id="kernel-setting">Kernel setting</h3>
<p>Perform below kernel setting in all VMs.</p>
<p>Create file <code>/etc/modules-load.d/containerd.conf</code> to set up containerd configure file.
It's to load two modules <code>overlay</code> and <code>br_netfilter</code>.</p>
<p>Service <code>containerd</code> depends on <code>overlay</code> filesystem. Sometimes referred to as union-filesystems. An <a href="https://developer.aliyun.com/article/660712">overlay-filesystem</a> tries to present a filesystem which is the result over overlaying one filesystem on top of the other. </p>
<p>The <code>br_netfilter</code> module is required to enable transparent masquerading and to facilitate Virtual Extensible LAN (VxLAN) traffic for communication between Kubernetes pods across the cluster. </p>
<pre><code># cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
</code></pre>
<p>Load <code>overlay</code> and <code>br_netfilter</code> modules.</p>
<pre><code># sudo modprobe overlay
# sudo modprobe br_netfilter
</code></pre>
<p>Create file <code>99-kubernetes-cri.conf</code> to set up configure file for Kubernetes CRI.</p>
<p>Set <code>net/bridge/bridge-nf-call-iptables=1</code> to ensure simple configurations (like Docker with a bridge) work correctly with the iptables proxy. <a href="https://cloud.tencent.com/developer/article/1828060">Why <code>net/bridge/bridge-nf-call-iptables=1</code> need to be enable by Kubernetes</a>.</p>
<p>IP forwarding is also known as routing. When it comes to Linux, it may also be called Kernel IP forwarding because it uses the kernel variable <code>net.ipv4.ip_forward</code> to enable or disable the IP forwarding feature. The default preset value is <code>ip_forward=0</code>. Hence, the Linux IP forwarding feature is disabled by default.</p>
<pre><code># cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
</code></pre>
<p>The <code>sysctl</code> command reads the information from the <code>/proc/sys</code> directory. <code>/proc/sys</code> is a virtual directory that contains file objects that can be used to view and set the current kernel parameters.</p>
<p>By commadn <code>sysctl -w net.ipv4.ip_forward=1</code>, the change takes effect immediately, but it is not persistent. After a system reboot, the default value is loaded. Write the settings to <code>/etc/sysctl.conf</code> is to set a parameter permanently, you’ll need to  or another configuration file in the /etc/sysctl.d directory:</p>
<pre><code>sudo sysctl --system
</code></pre>
<h3 id="install-containerd">Install Containerd</h3>
<p>Install Containerd sevice fro all VMs.</p>
<p>Backup source file.</p>
<pre><code># sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak
</code></pre>
<p>Add proper repo sources. For ECS with Ubuntu 20.04 version created by Aliyun, this step is not needed.</p>
<pre><code>cat &gt; /etc/apt/sources.list &lt;&lt; EOF
deb http://mirrors.cloud.aliyuncs.com/ubuntu/ focal main restricted
deb-src http://mirrors.cloud.aliyuncs.com/ubuntu/ focal main restricted
deb http://mirrors.cloud.aliyuncs.com/ubuntu/ focal-updates main restricted
deb-src http://mirrors.cloud.aliyuncs.com/ubuntu/ focal-updates main restricted
deb http://mirrors.cloud.aliyuncs.com/ubuntu/ focal universe
deb-src http://mirrors.cloud.aliyuncs.com/ubuntu/ focal universe
deb http://mirrors.cloud.aliyuncs.com/ubuntu/ focal-updates universe
deb-src http://mirrors.cloud.aliyuncs.com/ubuntu/ focal-updates universe
deb http://mirrors.cloud.aliyuncs.com/ubuntu/ focal multiverse
deb-src http://mirrors.cloud.aliyuncs.com/ubuntu/ focal multiverse
deb http://mirrors.cloud.aliyuncs.com/ubuntu/ focal-updates multiverse
deb-src http://mirrors.cloud.aliyuncs.com/ubuntu/ focal-updates multiverse
deb http://mirrors.cloud.aliyuncs.com/ubuntu/ focal-backports main restricted universe multiverse
deb-src http://mirrors.cloud.aliyuncs.com/ubuntu/ focal-backports main restricted universe multivers
deb http://mirrors.cloud.aliyuncs.com/ubuntu focal-security main restricted
deb-src http://mirrors.cloud.aliyuncs.com/ubuntu focal-security main restricted
deb http://mirrors.cloud.aliyuncs.com/ubuntu focal-security universe
deb-src http://mirrors.cloud.aliyuncs.com/ubuntu focal-security universe
# deb http://mirrors.cloud.aliyuncs.com/ubuntu focal-security multiverse
# deb-src http://mirrors.cloud.aliyuncs.com/ubuntu focal-security multiverse
EOF
</code></pre>
<p>Install Containered.</p>
<pre><code># sudo apt-get update &amp;&amp; sudo apt-get install -y containerd
</code></pre>
<p>Configure Containerd. Modify file <code>/etc/containerd/config.toml</code>.</p>
<pre><code># sudo mkdir -p /etc/containerd
# containerd config default | sudo tee /etc/containerd/config.toml
# vi /etc/containerd/config.toml
</code></pre>
<p>Update <code>sandbox_image</code> with new value <code>"registry.aliyuncs.com/google_containers/pause:3.6"</code>.
Update <code>SystemdCgroup</code> with new value <code>true</code>.</p>
<pre><code>[plugins]
  [plugins.&quot;io.containerd.gc.v1.scheduler&quot;]

  [plugins.&quot;io.containerd.grpc.v1.cri&quot;]
    sandbox_image = &quot;registry.aliyuncs.com/google_containers/pause:3.6&quot;

    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.cni]
    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd]
      [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.default_runtime]
        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.default_runtime.options]
      [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes]
        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc]

          [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc.options]
            SystemdCgroup = true
</code></pre>
<p>Restart Containerd service.</p>
<pre><code># sudo systemctl restart containerd
# sudo systemctl status containerd
</code></pre>
<h3 id="install-nerdctl">Install nerdctl</h3>
<p>Install nerdctl sevice fro all VMs.</p>
<p>The goal of <a href="https://github.com/containerd/nerdctl"><code>nerdctl</code></a> is to facilitate experimenting the cutting-edge features of containerd that are not present in Docker.</p>
<pre><code># wget https://github.com/containerd/nerdctl/releases/download/v0.21.0/nerdctl-0.21.0-linux-amd64.tar.gz
# tar -zxvf nerdctl-0.21.0-linux-amd64.tar.gz
# cp nerdctl /usr/bin/
</code></pre>
<p>Verify nerdctl.</p>
<pre><code># nerdctl --help
</code></pre>
<p>To list local Kubernetes containers.</p>
<pre><code># nerdctl -n k8s.io ps
</code></pre>
<h3 id="install-kubeadm">Install kubeadm</h3>
<p>Update <code>apt-transport-https</code>,  <code>ca-certificates</code>, and <code>curl</code>.</p>
<pre><code># apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https ca-certificates curl
</code></pre>
<p>Install gpg certificate. Just choose one of below command and execute.</p>
<pre><code># Tested in 20.04 release.
# curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -

# Tested in 22.04 release
# sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg
</code></pre>
<p>Add Kubernetes repo. Just choose one of below command and execute.</p>
<pre><code># Tested in 20.04 release
# cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF

# Tested in 22.04 release
# echo &quot;deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main&quot; | sudo tee /etc/apt/sources.list.d/kubernetes.list
</code></pre>
<p>Update  and install dependencied packages.</p>
<pre><code># apt-get update
# apt-get install ebtables
# apt-get install libxtables12=1.8.4-3ubuntu2
# apt-get upgrade iptables
</code></pre>
<p>Check available versions of kubeadm.</p>
<pre><code># apt policy kubeadm
</code></pre>
<p>Install <code>1.23.8-00</code> version of kubeadm and will upgrade to <code>1.24.2</code> later.</p>
<pre><code># sudo apt-get -y install kubelet=1.23.8-00 kubeadm=1.23.8-00 kubectl=1.23.8-00 --allow-downgrades
</code></pre>
<h3 id="setup-master-node">Setup Master Node</h3>
<p>Set up Control Plane on VM playing master node.</p>
<p>Check kubeadm default parameters for initialization.</p>
<pre><code># kubeadm config print init-defaults
</code></pre>
<p>Dry rune and run. Save the output, which will be used later on work nodes.</p>
<pre><code># kubeadm init --dry-run --pod-network-cidr=10.244.0.0/16 --image-repository=registry.aliyuncs.com/google_containers --kubernetes-version=v1.23.8

# kubeadm init --pod-network-cidr=10.244.0.0/16 --image-repository=registry.aliyuncs.com/google_containers --kubernetes-version=v1.23.8
</code></pre>
<p>Set <code>kubeconfig</code> file for current user (here it's <code>root</code>).</p>
<pre><code># mkdir -p $HOME/.kube
# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
# sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre>
<p>Set <code>kubectl</code> <a href="https://github.com/scop/bash-completion">auto-completion</a> following the <a href="https://kubernetes.io/docs/tasks/tools/included/optional-kubectl-configs-bash-linux/">guideline</a>.</p>
<pre><code># apt install -y bash-completion
# source /usr/share/bash-completion/bash_completion
# source &lt;(kubectl completion bash)
# echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~/.bashrc
</code></pre>
<p>If we set an alias for kubectl, we can extend shell completion to work with that alias:</p>
<pre><code>echo 'alias k=kubectl' &gt;&gt;~/.bashrc
echo 'complete -o default -F __start_kubectl k' &gt;&gt;~/.bashrc
</code></pre>
<h3 id="setup-work-nodes">Setup Work Nodes</h3>
<p>Perform on all VMs playing work nodes.</p>
<pre><code># kubeadm join &lt;your master node eth0 ip&gt;:6443 --token &lt;token generated by kubeadm init&gt; --discovery-token-ca-cert-hash &lt;hash key generated by kubeadm init&gt;
</code></pre>
<p>Verify status on master node.</p>
<pre><code>root@cka001:~# kubectl get node
NAME     STATUS   ROLES                  AGE     VERSION
cka001   Ready    control-plane,master   24m     v1.23.8
cka002   Ready    &lt;none&gt;                 9m39s   v1.23.8
cka003   Ready    &lt;none&gt;                 9m27s   v1.23.8
</code></pre>
<h3 id="install-flannel">Install Flannel</h3>
<p><a href="https://github.com/flannel-io/flannel">Flannel</a> is a simple and easy way to configure a layer 3 network fabric designed for Kubernetes.</p>
<p>Deploy Flannel on master node.
In the kube-flannel.yml we can get the default network setting of Flannel, which is same with <code>--pod-network-cidr=10.244.0.0/16</code> we defined before when we initiated <code>kubeadm</code>.</p>
<pre><code>  net-conf.json: |
    {
      &quot;Network&quot;: &quot;10.244.0.0/16&quot;,
      &quot;Backend&quot;: {
        &quot;Type&quot;: &quot;vxlan&quot;
      }
    }
</code></pre>
<pre><code>root@cka001:~# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Warning: policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
podsecuritypolicy.policy/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created
</code></pre>
<h3 id="check-cluster-status">Check Cluster Status</h3>
<p>Perform <code>kubectl cluster-info</code> command on master node we will get below information.</p>
<ul>
<li>Kubernetes control plane is running at https://<mster node ip>:6443</li>
<li>CoreDNS is running at https://<mster node ip>:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</li>
</ul>
<pre><code># kubectl cluster-info
# kubectl get nodes -owide
# kubectl get pod -A
</code></pre>
<h3 id="reset-cluster">Reset cluster</h3>
<p>CAUTION: below steps will destroy current cluster. </p>
<p>Delete all nodes in the cluster.</p>
<pre><code># kubeadm reset
</code></pre>
<p>Clean up rule of <code>iptables</code>.</p>
<pre><code># iptables -F &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t mangle -F &amp;&amp; iptables -X
</code></pre>
<p>Clean up rule of <code>IPVS</code> if using <code>IPVS</code>.</p>
<pre><code># ipvsadm --clear
</code></pre>
<h3 id="troubleshooting">Troubleshooting</h3>
<p><strong>Issue</strong>: 
The connection to the server <master>:6443 was refused - did you specify the right host or port?</p>
<p><strong>Try</strong>:</p>
<p><a href="https://discuss.kubernetes.io/t/the-connection-to-the-server-host-6443-was-refused-did-you-specify-the-right-host-or-port/552/15">Reference</a></p>
<p>Check environment setting.</p>
<pre><code>env | grep -i kub
</code></pre>
<p>Check container status.</p>
<pre><code>sudo systemctl status containerd.service 
</code></pre>
<p>Check kubelet service.</p>
<pre><code>sudo systemctl status kubelet.service 
</code></pre>
<p>Check port listening status.</p>
<pre><code>netstat -pnlt | grep 6443
</code></pre>
<p>Check firewall status.</p>
<pre><code>sudo systemctl status firewalld.service
</code></pre>
<p>Check log.</p>
<pre><code>journalctl -xeu kubelet
</code></pre>
<h2 id="snapshot-of-deployment">Snapshot of deployment</h2>
<p>Till now, the initial deployment is completed sucessfully.</p>
<h3 id="container-layer">Container Layer</h3>
<p>We are using Containerd service to manage our images and containers via command <code>nerdctl</code>.</p>
<p>Get current namespaces.</p>
<pre><code>root@cka001:~# nerdctl namespace ls
NAME      CONTAINERS    IMAGES    VOLUMES    LABELS
k8s.io    18            27        0  
</code></pre>
<p>Get containers under the namespace <code>k8s.io</code> by command <code>nerdctl -n k8s.io ps</code>.</p>
<pre><code>root@cka001:~# nerdctl -n k8s.io container ls
CONTAINER ID    IMAGE                                                                      COMMAND                   CREATED         STATUS    PORTS    NAMES
1eb9a51e0406    registry.aliyuncs.com/google_containers/kube-apiserver:v1.23.8             &quot;kube-apiserver --ad…&quot;    28 hours ago    Up                 k8s://kube-system/kube-apiserver-cka001/kube-apiserver                      
1ebee10176c4    registry.aliyuncs.com/google_containers/kube-proxy:v1.23.8                 &quot;/usr/local/bin/kube…&quot;    28 hours ago    Up                 k8s://kube-system/kube-proxy-v7rsr/kube-proxy                               
2c5e1d183fc7    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  28 hours ago    Up                 k8s://kube-system/kube-apiserver-cka001                                     
2dd9743cecad    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  27 hours ago    Up                 k8s://kube-system/kube-flannel-ds-rf54c                                     
39306eef76cd    docker.io/rancher/mirrored-flannelcni-flannel:v0.18.1                      &quot;/opt/bin/flanneld -…&quot;    27 hours ago    Up                 k8s://kube-system/kube-flannel-ds-rf54c/kube-flannel                        
3ca6fdda63a5    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  28 hours ago    Up                 k8s://kube-system/kube-scheduler-cka001                                     
49e07d9b2b98    registry.aliyuncs.com/google_containers/coredns:v1.8.6                     &quot;/coredns -conf /etc…&quot;    27 hours ago    Up                 k8s://kube-system/coredns-6d8c4cb4d-9khd8/coredns                           
555a3bf58832    registry.aliyuncs.com/google_containers/kube-scheduler:v1.23.8             &quot;kube-scheduler --au…&quot;    28 hours ago    Up                 k8s://kube-system/kube-scheduler-cka001/kube-scheduler                      
5812c42bf572    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  28 hours ago    Up                 k8s://kube-system/etcd-cka001                                               
8619e3c979a3    registry.aliyuncs.com/google_containers/coredns:v1.8.6                     &quot;/coredns -conf /etc…&quot;    27 hours ago    Up                 k8s://kube-system/coredns-6d8c4cb4d-qcp2l/coredns                           
a9459900f462    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  27 hours ago    Up                 k8s://kube-system/coredns-6d8c4cb4d-9khd8                                   
bb2b4624bfd5    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  27 hours ago    Up                 k8s://kube-system/coredns-6d8c4cb4d-qcp2l                                   
c9462709baff    registry.aliyuncs.com/google_containers/kube-controller-manager:v1.23.8    &quot;kube-controller-man…&quot;    28 hours ago    Up                 k8s://kube-system/kube-controller-manager-cka001/kube-controller-manager    
e68c3fbc90f9    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  28 hours ago    Up                 k8s://kube-system/kube-proxy-v7rsr                                          
eae550221813    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  28 hours ago    Up                 k8s://kube-system/kube-controller-manager-cka001                            
ff6626664c43    registry.aliyuncs.com/google_containers/etcd:3.5.1-0                       &quot;etcd --advertise-cl…&quot;    28 hours ago    Up                 k8s://kube-system/etcd-cka001/etcd     
</code></pre>
<p>Some management and commands options of <code>nertctl</code>.</p>
<pre><code>root@cka001:~# nertctl --help
root@cka001:~# nerdctl image ls -a
root@cka001:~# nerdctl volume ls
root@cka001:~# nerdctl stats
</code></pre>
<p>Get below network list with command <code>nerdctl network ls</code> in Containerd layer.</p>
<pre><code>root@cka001:~# nerdctl network ls
NETWORK ID    NAME      FILE
              cbr0      /etc/cni/net.d/10-flannel.conflist
0             bridge    /etc/cni/net.d/nerdctl-bridge.conflist
              host      
              none  
</code></pre>
<p>Get network interface in host <code>cka001</code> with command <code>ip addr list</code>.</p>
<pre><code>lo               : inet 127.0.0.1/8 qlen 1000
eth0             : inet 172.16.18.161/24 brd 172.16.18.255 qlen 1000
flannel.1        : inet 10.244.0.0/32
cni0             : inet 10.244.0.1/24 brd 10.244.0.255 qlen 1000
vethb0a35696@if3 : noqueue master cni0
veth72791f64@if3 : noqueue master cni0
</code></pre>
<p>With <code>kubeadm init</code> to initiate cluster, we need understand below three options about network.</p>
<ul>
<li><code>--pod-network-cidr</code>: <ul>
<li>Specify range of IP addresses for the pod network. If set, the control plane will automatically allocate CIDRs for every node.</li>
<li>Be noted that <code>10.244.0.0/16</code> is default range of flannel. If it's changed here, please do change the same when deploy <code>Flannel</code>.</li>
</ul>
</li>
<li><code>--apiserver-bind-port</code>: <ul>
<li>Port for the API Server to bind to. (default 6443)</li>
</ul>
</li>
<li><code>--service-cidr</code>: <ul>
<li>Use alternative range of IP address for service VIPs. (default "10.96.0.0/12")</li>
</ul>
</li>
</ul>
<p>Note: </p>
<ul>
<li>service VIPs (a.k.a. Cluster IP), specified by option <code>--service-cidr</code>.</li>
<li>podCIDR (a.k.a. endpoint IP)，specified by option <code>--pod-network-cidr</code>.</li>
</ul>
<p>There are 4 distinct networking problems to address:</p>
<ul>
<li>Highly-coupled container-to-container communications: this is solved by Pods (podCIDR) and localhost communications.</li>
<li>Pod-to-Pod communications: <ul>
<li>a.k.a. container-to-container. </li>
<li>Example with Flannel, the flow is: Pod --&gt; veth pair --&gt; cni0 --&gt; flannel.1 --&gt; host eth0 --&gt; host eth0 --&gt; flannel.1 --&gt; cni0 --&gt; veth pair --&gt; Pod.</li>
</ul>
</li>
<li>Pod-to-Service communications:<ul>
<li>Flow: Pod --&gt; Kernel --&gt; Servive iptables --&gt; service --&gt; Pod iptables --&gt; Pod</li>
</ul>
</li>
<li>External-to-Service communications: <ul>
<li>LoadBalancer: SLB --&gt; NodePort --&gt; Service --&gt; Pod</li>
</ul>
</li>
</ul>
<p><code>kube-proxy</code> is responsible for iptables, not traffic. </p>
<h3 id="kubernetes-layer">Kubernetes Layer</h3>
<p>Kubernetes is beyond container layer above. </p>
<p>In Kubernetes layer, we have three nodes, <code>cka001</code>, <code>cka002</code>, and <code>cka003</code>.</p>
<pre><code>root@cka001:~# kubectl get node
NAME     STATUS   ROLES                  AGE   VERSION
cka001   Ready    control-plane,master   27h   v1.23.8
cka002   Ready    &lt;none&gt;                 27h   v1.23.8
cka003   Ready    &lt;none&gt;                 27h   v1.23.8
</code></pre>
<p>We have four initial namespaces across three nodes.</p>
<pre><code>root@cka001:~# kubectl get namespace -A
NAME              STATUS   AGE
default           Active   27h
kube-node-lease   Active   27h
kube-public       Active   27h
kube-system       Active   27h
</code></pre>
<p>We have some initial pods. </p>
<pre><code>root@cka001:~# kubectl get pod -A -o wide
NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES
kube-system   coredns-6d8c4cb4d-9khd8          1/1     Running   0          27h   &lt;cni0 IP&gt;       cka001   &lt;none&gt;           &lt;none&gt;
kube-system   coredns-6d8c4cb4d-qcp2l          1/1     Running   0          27h   &lt;cni0 IP&gt;       cka001   &lt;none&gt;           &lt;none&gt;
kube-system   etcd-cka001                      1/1     Running   0          27h   &lt;eth0 IP&gt;       cka001   &lt;none&gt;           &lt;none&gt;
kube-system   kube-apiserver-cka001            1/1     Running   0          27h   &lt;eth0 IP&gt;       cka001   &lt;none&gt;           &lt;none&gt;
kube-system   kube-controller-manager-cka001   1/1     Running   0          27h   &lt;eth0 IP&gt;       cka001   &lt;none&gt;           &lt;none&gt;
kube-system   kube-flannel-ds-hfvf7            1/1     Running   0          27h   &lt;eth0 IP&gt;       cka003   &lt;none&gt;           &lt;none&gt;
kube-system   kube-flannel-ds-m5mdl            1/1     Running   0          27h   &lt;eth0 IP&gt;       cka002   &lt;none&gt;           &lt;none&gt;
kube-system   kube-flannel-ds-rf54c            1/1     Running   0          27h   &lt;eth0 IP&gt;       cka001   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-bj75j                 1/1     Running   0          27h   &lt;eth0 IP&gt;       cka002   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-gxjj4                 1/1     Running   0          27h   &lt;eth0 IP&gt;       cka003   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-v7rsr                 1/1     Running   0          27h   &lt;eth0 IP&gt;       cka001   &lt;none&gt;           &lt;none&gt;
kube-system   kube-scheduler-cka001            1/1     Running   0          27h   &lt;eth0 IP&gt;       cka001   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Summary below shows the relationship between containers and pods. 
Good references about container pause: <a href="https://zhuanlan.zhihu.com/p/464712164">article</a> and <a href="https://cloud.tencent.com/developer/article/1583919">artical</a>.</p>
<ul>
<li>Master node:<ul>
<li>CoreDNS: 2 pods, 2 containers of each pod<ul>
<li>From image <code>coredns:v1.8.6</code>:<ul>
<li>k8s://kube-system/coredns-6d8c4cb4d-9khd8/coredns</li>
<li>k8s://kube-system/coredns-6d8c4cb4d-qcp2l/coredns</li>
</ul>
</li>
<li>By image <code>pause:3.6</code><ul>
<li>k8s://kube-system/coredns-6d8c4cb4d-9khd8</li>
<li>k8s://kube-system/coredns-6d8c4cb4d-qcp2l</li>
</ul>
</li>
</ul>
</li>
<li>etcd: 1 pod, 2 containers<ul>
<li>By image <code>etcd:3.5.1-0</code><ul>
<li>k8s://kube-system/etcd-cka001/etcd</li>
</ul>
</li>
<li>By image <code>pause:3.6</code><ul>
<li>k8s://kube-system/etcd-cka001</li>
</ul>
</li>
</ul>
</li>
<li>apiserver: 1 pod, 2 containers<ul>
<li>By image <code>kube-apiserver:v1.23.8</code><ul>
<li>k8s://kube-system/kube-apiserver-cka001/kube-apiserver</li>
</ul>
</li>
<li>By image <code>pause:3.6</code><ul>
<li>k8s://kube-system/kube-apiserver-cka001</li>
</ul>
</li>
</ul>
</li>
<li>controller-manager: 1 pod, 2 containers<ul>
<li>By image <code>kube-controller-manager:v1.23.8</code><ul>
<li>k8s://kube-system/kube-controller-manager-cka001/kube-controller-manager</li>
</ul>
</li>
<li>By image <code>pause:3.6</code><ul>
<li>k8s://kube-system/kube-controller-manager-cka001</li>
</ul>
</li>
</ul>
</li>
<li>scheduler: 1 pod, 2 containers<ul>
<li>By image <code>kube-scheduler:v1.23.8</code><ul>
<li>k8s://kube-system/kube-scheduler-cka001/kube-scheduler</li>
</ul>
</li>
<li>By image <code>pause:3.6</code><ul>
<li>k8s://kube-system/kube-scheduler-cka001</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>All nodes:<ul>
<li>Flannel DS: 1 pod of each, 2 containers of each pod<ul>
<li>By image <code>mirrored-flannelcni-flannel:v0.18.1</code><ul>
<li>k8s://kube-system/kube-flannel-ds-rf54c/kube-flannel</li>
</ul>
</li>
<li>By image <code>pause:3.6</code><ul>
<li>k8s://kube-system/kube-flannel-ds-rf54c</li>
</ul>
</li>
</ul>
</li>
<li>Proxy: 1 pod of each, 2 containers of each pod<ul>
<li>By image <code>kube-proxy:v1.23.8</code><ul>
<li>k8s://kube-system/kube-proxy-v7rsr/kube-proxy</li>
</ul>
</li>
<li>By image <code>pause:3.6</code><ul>
<li>k8s://kube-system/kube-proxy-v7rsr</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Let's check current configuration context of Kubernetes we just initialized. </p>
<ul>
<li>Contenxt name is <code>kubernetes-admin@kubernetes</code>.</li>
<li>Cluster name is <code>kubernetes</code>.</li>
<li>User is <code>kubernetes-admin</code>.</li>
<li>No namespace explicitly defined.</li>
</ul>
<pre><code>root@cka001:~# kubectl config get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
*         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin 
</code></pre>
<p>Create a new namespace <code>jh-namespace</code>.</p>
<pre><code>root@cka001:~# kubectl create namespace jh-namespace
</code></pre>
<p>Update current context <code>kubernetes-admin@kubernetes</code> with new namespace <code>jh-namespace</code> as default namespace. </p>
<pre><code>root@cka001:~# kubectl config set-context kubernetes-admin@kubernetes --cluster=kubernetes --namespace=jh-namespace --user=kubernetes-admin 
</code></pre>
<p>Now default namespace is shown in current configuration context. </p>
<pre><code>root@cka001:~# kubectl config get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
*         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   jh-namespace
</code></pre>
<p>Let's execute command <code>kubectl apply -f 02-sample-pod.yaml</code> to create a pod <code>my-first-pod</code> on namespace <code>jh-namespace</code> with below content of file <code>02-sample-pod.yaml</code>.</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: my-first-pod
spec:
  containers:
  - name: nginx
    image: nginx:mainline
    ports:
    - containerPort: 80
</code></pre>
<p>By command <code>kubectl get pod -o wide</code> we get the pod status.</p>
<p>The pod's ip is allocated by <code>cni0</code>. Node is assigned by <code>Scheduler</code>. </p>
<p>We can also find related containers of pod <code>my-first-pod</code> via command <code>nerdctl -n k8s.io container ls</code> on <code>cka003</code>.</p>
<pre><code>root@cka001:~# kubectl get pod -o wide
NAME           READY   STATUS    RESTARTS   AGE   IP           NODE     NOMINATED NODE   READINESS GATES
my-first-pod   1/1     Running   0          19s   10.244.2.2   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<h3 id="case-study">Case Study</h3>
<p>Scenario: stop kubelet service on worker node <code>cka003</code>.</p>
<p>Question:</p>
<ul>
<li>What's the status of each node?</li>
<li>What's containers changed via command <code>nerdctl</code>?</li>
<li>What's pods status via command <code>kubectl get pod -owide -A</code>? </li>
</ul>
<p>Demo:</p>
<p>Execute command <code>systemctl stop kubelet.service</code> on <code>cka003</code>.</p>
<p>Execute command <code>kubectl get node</code> on either <code>cka001</code> or <code>cka003</code>, the status of <code>cka003</code> is <code>NotReady</code>.</p>
<p>Execute command <code>nerdctl -n k8s.io container ls</code> on <code>cka003</code> and we can observe all containers are still up and running, including the pod <code>my-first-pod</code>.</p>
<p>Execute command <code>systemctl start kubelet.service</code> on <code>cka003</code>.</p>
<p>Conclusion:</p>
<ul>
<li>The node status is changed to <code>NotReady</code> from <code>Ready</code>.</li>
<li>For those DaemonSet pods, like <code>flannel</code>、<code>kube-proxy</code>, are exclusively running on each node. They won't be terminated after <code>kubelet</code> is down.</li>
<li>The status of pod <code>my-first-pod</code> keeps showing <code>Terminating</code> on each node because status can not be synced to other nodes via <code>apiserver</code> from <code>cka003</code> because <code>kubelet</code> is down.</li>
<li>The status of pod is marked by <code>controller</code> and recycled by <code>kubelet</code>.</li>
<li>When we start kubelet service on <code>cka003</code>, the pod <code>my-first-pod</code> will be termiated completely on <code>cka003</code>.</li>
</ul>
<p>In addition, let's create a deployment with 3 replicas. Two are running on <code>cka003</code> and one is running on <code>cka002</code>.</p>
<pre><code>root@cka001:~# kubectl get pod -o wide -w
NAME                               READY   STATUS    RESTARTS   AGE    IP           NODE     NOMINATED NODE   READINESS GATES
nginx-deployment-9d745469b-2xdk4   1/1     Running   0          2m8s   10.244.2.3   cka003   &lt;none&gt;           &lt;none&gt;
nginx-deployment-9d745469b-4gvmr   1/1     Running   0          2m8s   10.244.2.4   cka003   &lt;none&gt;           &lt;none&gt;
nginx-deployment-9d745469b-5j927   1/1     Running   0          2m8s   10.244.1.3   cka002   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>After we stop kubelet service on <code>cka003</code>, the two running on <code>cka003</code> are terminated and another two are created and running on <code>cka002</code> automatically. </p>
<h2 id="kubectl">kubectl</h2>
<p>Three approach to operate Kubernetes cluster:</p>
<ul>
<li>via <a href="https://kubernetes.io/docs/reference/kubernetes-api/">API</a></li>
<li>via kubectl</li>
<li>via Dashboard</li>
</ul>
<h3 id="config-file">Config File</h3>
<p>Kubernetes provides a command line tool <code>kubectl</code> for communicating with a Kubernetes cluster's control plane, using the Kubernetes API.</p>
<p>kubectl controls the Kubernetes <em>cluster manager</em>.</p>
<p>For configuration, kubectl looks for a file named config in the <code>$HOME/.kube</code> directory, which is a copy of file <code>/etc/kubernetes/admin.conf</code> generated by <code>kubeadm init</code>. </p>
<p>We can specify other kubeconfig files by setting the <code>KUBECONFIG</code> environment variable or by setting the <code>--kubeconfig flag</code>.  If the <code>KUBECONFIG</code> environment variable doesn't exist, kubectl uses the default kubeconfig file, <code>$HOME/.kube/config</code>.</p>
<p>A <em>context</em> element in a kubeconfig file is used to group access parameters under a convenient name. Each context has three parameters: cluster, namespace, and user. By default, the kubectl command-line tool uses parameters from the current context to communicate with the cluster.</p>
<p>A sample of <code>.kube/config</code>.</p>
<pre><code>apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: &lt;certificate string&gt;
    server: https://&lt;eth0 ip&gt;:6443
  name: &lt;cluster name&gt;
contexts:
- context:
    cluster: &lt;cluster name&gt;
    namespace: &lt;namespace name&gt;
    user: &lt;user name&gt;
  name: &lt;context user&gt;@&lt;context name&gt;
current-context: &lt;context name&gt;
kind: Config
preferences: {}
users:
- name: &lt;user name&gt;
  user:
    client-certificate-data: &lt;certificate string&gt;
    client-key-data: &lt;certificate string&gt;
</code></pre>
<p>To get the current context:</p>
<pre><code>root@cka001:~# kubectl config get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
*         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   jh-namespace
</code></pre>
<p>To set a context with new update, e.g, update default namespace, etc..</p>
<pre><code>kubectl config set-context &lt;context name&gt; --cluster=&lt;cluster name&gt; --namespace=&lt;namespace name&gt; --user=&lt;user name&gt; 
</code></pre>
<p>To use a new context.</p>
<pre><code>kubectl config use-contexts &lt;context name&gt;
</code></pre>
<p>Reference of <a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubectl</a> and <a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands">commandline</a>. </p>
<h3 id="bash-autocomplete">Bash Autocomplete</h3>
<p>Use TAB for bash auto-completion.</p>
<p>Ubuntu：</p>
<pre><code>apt install -y bash-completion
source /usr/share/bash-completion/bash_completion
echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~/.bashrc
source &lt;(kubectl completion bash)
</code></pre>
<h3 id="common-usage">Common Usage</h3>
<p>Get cluster status.</p>
<pre><code># kubectl cluster-info
# kubectl cluster-info dump
</code></pre>
<p>Get health status of control plane.</p>
<pre><code># kubectl get componentstatuses
# kubectl get cs
</code></pre>
<p>Get node status.</p>
<pre><code>kubectl get nodes
kubectl get nodes -o wide
</code></pre>
<p>Update or get node lable.</p>
<pre><code># Update node label
kubectl label node cka002 node=demonode

# Get node info with label info
kubectl get nodes –show-labels

# Search node by label
kubectl get node -l node=demonode
</code></pre>
<p>Create a deployment, option <code>--image</code> specifies a image，option <code>--port</code> specifies port for external access. A pod is also created when deployment is created.</p>
<pre><code>kubectl create deployment myapp --image=nginx --replicas=1 --port=80
kubectl get deployment myapp -o wide
</code></pre>
<p>Get detail information of pod.</p>
<pre><code>kubectl describe pod &lt;pod name&gt;
</code></pre>
<p>Get detail information of deployment.</p>
<pre><code>kubectl describe deployment &lt;deployment&gt;
</code></pre>
<p>Get resources under a namespace or all namespace.</p>
<pre><code>kubectl get namespace
kubectl get pod -n &lt;namespace name&gt;
kubectl get pod --all-namespaces
kubectl get pod -A

kubectl get deployment -n &lt;namespace name&gt;
kubectl get deployment --all-namespaces
kubectl get deployment -A
</code></pre>
<p>By default, pod can only be internally accessed within cluster. 
We can map pod port to node port for external access by exposing a pod, e.g., browser <code>http://&lt;node_ip&gt;:&lt;port_number&gt;</code>.</p>
<pre><code># Expose myapp as service to node port 80.
kubectl expose deployment myapp --type=NodePort --port=80

# Get service
kubectl get service
kubectl get svc -o wide
</code></pre>
<p>Scale out by replicaset. We set three replicasets to scale out deployment <code>myapp</code>. The number of deployment <code>myapp</code> is now three.</p>
<pre><code># Scale out deployment
kubectl scale deployment myapp --replicas=3

# Get status of deployment
kubectl get deployment myapp

# Get status of replicaset
kubectl get replicaset
</code></pre>
<p>Rolling update.</p>
<p>Command usage: <code>kubectl set image (-f FILENAME | TYPE NAME) CONTAINER_NAME_1=CONTAINER_IMAGE_1 ... CONTAINER_NAME_N=CONTAINER_IMAGE_N</code>.</p>
<p>With the command <code>kubectl get deployment</code>, we will get deployment name <code>myapp</code> and related container name <code>nginx</code>.</p>
<pre><code>kubectl get deployment myapp -o wide
</code></pre>
<p>With the command <code>kubectl set image</code> to update image nginx from <code>nginx</code> to <code>nginx:1.19</code> and log the change under deployment's annotations with option <code>--record</code>.</p>
<pre><code>kubectl set image deployment myapp nginx=nginx:1.19 --record
</code></pre>
<p>By the command <code>kubectl set image</code>, all pods are running under new replicaset <code>myapp-b997fb85f</code> with new image version <code>nginx:1.19</code>.</p>
<pre><code>root@cka001:~# kubectl get replicaset -o wide -l app=myapp
NAME              DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES       SELECTOR
myapp-948688ff6   0         0         0       80m     nginx        nginx        app=myapp,pod-template-hash=948688ff6
myapp-b997fb85f   3         3         3       6m29s   nginx        nginx:1.19   app=myapp,pod-template-hash=b997fb85f
</code></pre>
<p>We can get the change history under <code>metadata.annotations</code> by command <code>kubectl get deployment -o yaml</code>.</p>
<pre><code>root@cka001:~# kubectl get deployment myapp -o yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: &quot;2&quot;
    kubernetes.io/change-cause: kubectl set image deployment myapp nginx=nginx:1.19
      --record=true
  creationTimestamp: &quot;2022-06-28T06:33:14Z&quot;
</code></pre>
<p>We can also get the change history by command <code>kubectl rollout history</code>, and show details with specific revision <code>--revision=&lt;revision_number&gt;</code>.</p>
<pre><code>root@cka001:~# kubectl rollout history deployment/myapp
deployment.apps/myapp 
REVISION  CHANGE-CAUSE
1         kubectl set image deployment/myapp myapp=nginx:1.19 --record=true
2         kubectl set image deployment myapp nginx=nginx:1.19 --record=true

root@cka001:~# kubectl rollout history deployment/myapp --revision=1
deployment.apps/myapp with revision #1
Pod Template:
  Labels:       app=myapp
        pod-template-hash=948688ff6
  Annotations:  kubernetes.io/change-cause: kubectl set image deployment/myapp myapp=nginx:1.19 --record=true
  Containers:
   nginx:
    Image:      nginx
    Port:       80/TCP
    Host Port:  0/TCP
    Environment:        &lt;none&gt;
    Mounts:     &lt;none&gt;
  Volumes:      &lt;none&gt;


root@cka001:~# kubectl rollout history deployment/myapp --revision=2
deployment.apps/myapp with revision #2
Pod Template:
  Labels:       app=myapp
        pod-template-hash=b997fb85f
  Annotations:  kubernetes.io/change-cause: kubectl set image deployment myapp nginx=nginx:1.19 --record=true
  Containers:
   nginx:
    Image:      nginx:1.19
    Port:       80/TCP
    Host Port:  0/TCP
    Environment:        &lt;none&gt;
    Mounts:     &lt;none&gt;
  Volumes:      &lt;none&gt;
</code></pre>
<p>Roll back to previous revision with command <code>kubectl rollout undo</code>, or roll back to specific revision with option <code>--to-revision=&lt;revision_number&gt;</code>.</p>
<pre><code># kubectl rollout undo deployment/myapp --to-revision=1
</code></pre>
<p>Get system event information.</p>
<pre><code>kubectl describe pod &lt;pod_name&gt; -n &lt;namespace_name&gt;
</code></pre>
<p>Get the logs for a container in a pod or specified resource. If the pod has only one container, the container name is optional.</p>
<h2 id="kubernetes-api-and-resource">Kubernetes API and Resource</h2>
<h3 id="demo-static-pod">Demo: Static Pod</h3>
<p>Create yaml file in directory <code>/etc/kubernetes/manifests/</code>.
<code>kubectl</code> will automatically check yaml file in <code>/etc/kubernetes/manifests/</code> and create the static pod once it's detected.</p>
<pre><code>root@cka001:~# kubectl run nginx --image=nginx:mainline --dry-run=client -n jh-namespace -oyaml &gt; /etc/kubernetes/manifests/my-nginx.yaml

root@cka001:~# kubectl get pod
NAME           READY   STATUS    RESTARTS   AGE
nginx-cka001   1/1     Running   0          6s
</code></pre>
<p>Delete the yaml file <code>/etc/kubernetes/manifests/my-nginx.yaml</code>, the static pod will be deleted automatically.</p>
<pre><code>root@cka001:~# rm /etc/kubernetes/manifests/my-nginx.yaml 
</code></pre>
<h3 id="demo-init-containers">Demo: Init containers</h3>
<p>This example defines a simple Pod that has two init containers in <code>02-init-pod.yaml</code>. 
The first waits for myservice, and the second waits for mydb. 
Once both init containers complete, the Pod runs the app container from its spec section.</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', &quot;until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done&quot;]
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', &quot;until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done&quot;]
</code></pre>
<p>Create the Pod <code>myapp-pod</code>.</p>
<pre><code>kubectl apply -f 02-init-pod.yaml
</code></pre>
<p>Check Pod status.</p>
<pre><code># kubectl get pod myapp-pod
NAME        READY   STATUS     RESTARTS   AGE
myapp-pod   0/1     Init:0/2   0          12m
</code></pre>
<p>Inspect Pods.</p>
<pre><code>kubectl logs myapp-pod -c init-myservice # Inspect the first init container
kubectl logs myapp-pod -c init-mydb      # Inspect the second init container
</code></pre>
<p>At this point, those init containers will be waiting to discover Services named mydb and myservice.</p>
<p>Here's a configuration <code>04-myservice.yaml</code> we can use to make those Services appear :</p>
<pre><code>---
apiVersion: v1
kind: Service
metadata:
  name: myservice
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
---
apiVersion: v1
kind: Service
metadata:
  name: mydb
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9377
</code></pre>
<p>To create the <code>mydb</code> and <code>myservice</code> services:</p>
<pre><code>kubectl apply -f 04-myservice.yaml
</code></pre>
<p>We'll now see that those init containers complete, and that the myapp-pod Pod moves into the Running state:</p>
<pre><code>root@cka001:~# kubectl get -f 04-myservice.yaml
NAME        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
myservice   ClusterIP   10.103.101.99   &lt;none&gt;        80/TCP    40s
mydb        ClusterIP   10.96.79.220    &lt;none&gt;        80/TCP    40s

root@cka001:~# kubectl get pod myapp-pod
NAME        READY   STATUS    RESTARTS   AGE
myapp-pod   1/1     Running   0          13m
</code></pre>
<h3 id="demo-mutil-container-pod">Demo Mutil-Container Pod</h3>
<p>Create the sampel yaml file <code>multi-pod.yaml</code>.</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: multi-container-pod
spec:
  containers:
  - name: container-1-nginx
    image: nginx
    ports:
    - containerPort: 80  
  - name: container-2-alpine
    image: alpine
    command: [&quot;watch&quot;, &quot;wget&quot;, &quot;-qO-&quot;, &quot;localhost&quot;]
</code></pre>
<p>Create the pod.</p>
<pre><code>root@cka001:~# kubectl apply -f multi-pod.yaml

root@cka001:~# kubectl get pod multi-container-pod
NAME                  READY   STATUS    RESTARTS   AGE
multi-container-pod   2/2     Running   0          81s
</code></pre>
<p>Get details of the pod we created via command <code>kubectl describe pod multi-container-pod</code> and we can see below events.</p>
<pre><code>Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  3m14s  default-scheduler  Successfully assigned jh-namespace/multi-container-pod to cka003
  Normal  Pulling    3m14s  kubelet            Pulling image &quot;nginx&quot;
  Normal  Pulled     3m12s  kubelet            Successfully pulled image &quot;nginx&quot; in 2.02130736s
  Normal  Created    3m11s  kubelet            Created container container-1-nginx
  Normal  Started    3m11s  kubelet            Started container container-1-nginx
  Normal  Pulling    3m11s  kubelet            Pulling image &quot;alpine&quot;
  Normal  Pulled     3m3s   kubelet            Successfully pulled image &quot;alpine&quot; in 8.317148653s
  Normal  Created    3m3s   kubelet            Created container container-2-alpine
  Normal  Started    3m3s   kubelet            Started container container-2-alpine
</code></pre>
<p>For multi-container pod, container name is needed if we want to get log of pod via command <code>kubectl logs &lt;pod_name&gt; &lt;container_name&gt;</code>.</p>
<pre><code>root@cka001:~# kubectl logs multi-container-pod
error: a container name must be specified for pod multi-container-pod, choose one of: [container-1-nginx container-2-alpine]

root@cka001:~# kubectl logs multi-container-pod container-1-nginx
......
::1 - - [02/Jul/2022:01:12:29 +0000] &quot;GET / HTTP/1.1&quot; 200 615 &quot;-&quot; &quot;Wget&quot; &quot;-&quot;
</code></pre>
<p>Same if we need specify container name to login into the pod via command <code>kubectl exec -it &lt;pod_name&gt; -c &lt;container_name&gt; -- &lt;commands&gt;</code>.</p>
<pre><code>root@cka001:~# kubectl exec -it multi-container-pod -c container-1-nginx -- /bin/bash
root@multi-container-pod:/# ls
bin  boot  dev  docker-entrypoint.d  docker-entrypoint.sh  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
</code></pre>
<h3 id="demo-usage-of-kubectl">Demo: Usage of kubectl</h3>
<h4 id="grant-authorization-to-serviceaccount">Grant Authorization to ServiceAccount</h4>
<p>With Kubernetes 1.23 and lower version, when we create a new namespace, Kubernetes will automatically create a ServiceAccount <code>default</code> and a token <code>default-token-xxxxx</code>.</p>
<p>We can say that the ServiceAccount <code>default</code> is an account under the namespace.</p>
<p>Here is an example of new namespace <code>jh-namespace</code> I created.</p>
<ul>
<li>ServiceAcccount: <code>default</code></li>
<li>Token: <code>default-token-8vrsc</code></li>
</ul>
<pre><code>root@cka001:~# kubectl get sa -n jh-namespace
NAME      SECRETS   AGE
default   1         26h

root@cka001:~# kubectl get secrets -n jh-namespace
NAME                  TYPE                                  DATA   AGE
default-token-8vrsc   kubernetes.io/service-account-token   3      26h
</code></pre>
<p>There is a cluster rule <code>admin</code>, and no related rolebinding.</p>
<pre><code>root@cka001:~# kubectl get clusterrole admin -n jh-namespace
NAME    CREATED AT
admin   2022-06-25T06:24:44Z

root@cka001:~# kubectl get role -n jh-namespace
No resources found in jh-namespace namespace.

root@cka001:~# kubectl get rolebinding -n jh-namespace
No resources found in jh-namespace namespace.
</code></pre>
<p>Get token of the service account <code>default</code>.</p>
<pre><code>TOKEN=$(kubectl describe secret $(kubectl get secrets | grep default | cut -f1 -d ' ') | grep -E '^token' | cut -f2 -d':' | tr -d ' ')
echo $TOKEN
</code></pre>
<p>Get API Service address.</p>
<pre><code>APISERVER=$(kubectl config view | grep https | cut -f 2- -d &quot;:&quot; | tr -d &quot; &quot;)
echo $APISERVER
</code></pre>
<p>Get pod resources in namespace <code>jh-namespace</code> via API server with JSON layout.</p>
<pre><code>curl $APISERVER/api/v1/namespaces/jh-namespace/pods --header &quot;Authorization: Bearer $TOKEN&quot; --insecure
</code></pre>
<p>We will receive below error message. The serviceaccount <code>default</code> does not have authorization to access pod.</p>
<pre><code>&quot;message&quot;: &quot;pods is forbidden: User \&quot;system:serviceaccount:jh-namespace:default\&quot; cannot list resource \&quot;pods\&quot; in API group \&quot;\&quot; in the namespace \&quot;jh-namespace\&quot;&quot;,
</code></pre>
<p>Let's create a rolebinding <code>rolebinding-admin</code> to bind cluster role <code>admin</code> to service account <code>default</code> in namespapce <code>jh-namespace</code>.
Hence service account <code>default</code> is granted adminstrator authorization in namespace <code>jh-namespace</code>.</p>
<pre><code># Usage:
kubectl create rolebinding &lt;rule&gt; --clusterrole=&lt;clusterrule&gt; --serviceaccount=&lt;namespace&gt;:&lt;name&gt; --namespace=&lt;namespace&gt;

# Crate rolebinding:
kubectl create rolebinding rolebinding-admin --clusterrole=admin --serviceaccount=jh-namespace:default --namespace=jh-namespace
</code></pre>
<p>Result looks like below.</p>
<pre><code>root@cka001:~# kubectl get rolebinding -n jh-namespace
NAME                ROLE                AGE
rolebinding-admin   ClusterRole/admin   39s
</code></pre>
<p>Try again, get pod resources in namespace <code>jh-namespace</code> via API server with JSON layout.</p>
<pre><code>curl $APISERVER/api/v1/namespaces/jh-namespace/pods --header &quot;Authorization: Bearer $TOKEN&quot; --insecure
</code></pre>
<h4 id="label-node">Label Node</h4>
<p>Get current label of nodes.</p>
<pre><code>root@cka001:~# kubectl get node --show-labels
NAME     STATUS   ROLES                  AGE   VERSION   LABELS
cka001   Ready    control-plane,master   4d    v1.23.8   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=cka001,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node.kubernetes.io/exclude-from-external-load-balancers=
cka002   Ready    &lt;none&gt;                 4d    v1.23.8   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=cka002,kubernetes.io/os=linux
cka003   Ready    &lt;none&gt;                 4d    v1.23.8   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=cka003,kubernetes.io/os=linux
</code></pre>
<p>Label a node <code>cka003</code>.</p>
<pre><code>root@cka001:~# kubectl label node cka003 node=demonode
</code></pre>
<h4 id="deployment_1">Deployment</h4>
<p>Create a deployment <code>myapp</code>. <code>--port=8080</code> means the port that this container exposes.</p>
<pre><code>kubectl create deployment myapp --image=docker.io/jocatalin/kubernetes-bootcamp:v1 --replicas=1 --port=8080
</code></pre>
<p>Get the status of the deployment.</p>
<pre><code>root@cka001:~# kubectl get deployment -o wide
NAME    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS            IMAGES                                       SELECTOR
myapp   0/1     1            0           19s   kubernetes-bootcamp   docker.io/jocatalin/kubernetes-bootcamp:v1   app=myapp
</code></pre>
<p>Get the details of deployment.</p>
<pre><code>root@cka001:~# kubectl describe deployment myapp
</code></pre>
<p>Get the status of the Pod.</p>
<pre><code>root@cka001:~# kubectl get pod -o wide
NAME                    READY   STATUS    RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
myapp-b5d775f5d-6jtgs   1/1     Running   0          2m36s   10.244.2.12   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Get the details of the Pod.</p>
<pre><code>root@cka001:~# kubectl describe pod myapp-b5d775f5d-6jtgs
</code></pre>
<h4 id="namespace">Namespace</h4>
<p>Get current available namespaces.</p>
<pre><code>root@cka001:~# kubectl get namespace
NAME              STATUS   AGE
default           Active   4d1h
jh-namespace      Active   2d19h
kube-node-lease   Active   4d1h
kube-public       Active   4d1h
kube-system       Active   4d1h
</code></pre>
<p>Get Pod under a specific namespace.</p>
<pre><code>root@cka001:~# kubectl get pod -n kube-system
NAME                             READY   STATUS    RESTARTS   AGE
coredns-6d8c4cb4d-9khd8          1/1     Running   0          4d1h
coredns-6d8c4cb4d-qcp2l          1/1     Running   0          4d1h
etcd-cka001                      1/1     Running   0          4d1h
kube-apiserver-cka001            1/1     Running   0          4d1h
kube-controller-manager-cka001   1/1     Running   0          4d1h
kube-flannel-ds-hfvf7            1/1     Running   0          4d
kube-flannel-ds-m5mdl            1/1     Running   0          4d
kube-flannel-ds-rf54c            1/1     Running   0          4d
kube-proxy-bj75j                 1/1     Running   0          4d
kube-proxy-gxjj4                 1/1     Running   0          4d
kube-proxy-v7rsr                 1/1     Running   0          4d1h
kube-scheduler-cka001            1/1     Running   0          4d1h
</code></pre>
<p>Get Pods in all namespaces.</p>
<pre><code>root@cka001:~# kubectl get pod --all-namespaces
root@cka001:~# kubectl get pod -A
</code></pre>
<h4 id="expose-service">Expose Service</h4>
<p>Get current running Pod we created just now.</p>
<pre><code>root@cka001:~# kubectl get deployment myapp -o wide
NAME    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS            IMAGES                                       SELECTOR
myapp   1/1     1            1           44m   kubernetes-bootcamp   docker.io/jocatalin/kubernetes-bootcamp:v1   app=myapp

root@cka001:~# kubectl get pod -o wide
NAME                    READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
myapp-b5d775f5d-6jtgs   1/1     Running   0          25m   10.244.2.12   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Send http request to the Pod.</p>
<pre><code>root@cka001:~# curl 10.244.2.12:8080
Hello Kubernetes bootcamp! | Running on: myapp-b5d775f5d-6jtgs | v=1
</code></pre>
<p>To make pod be accessed outside, we need expose port <code>8080</code> to a node port. A related service will be created. </p>
<pre><code>root@cka001:~# kubectl expose deployment myapp --type=NodePort --port=8080
service/myapp exposed
</code></pre>
<p>Get details of service <code>myapp</code>.</p>
<pre><code>root@cka001:~# kubectl get svc myapp -o wide
NAME    TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE     SELECTOR
myapp   NodePort   10.108.93.159   &lt;none&gt;        8080:30520/TCP   5h14m   app=myapp

root@cka001:~# kubectl get svc myapp -o yaml
root@cka001:~# kubectl describe svc myapp
</code></pre>
<p>Get details of related endpoint <code>myapp</code>.</p>
<pre><code>root@cka001:~# kubectl get endpoints myapp -o wide
NAME    ENDPOINTS          AGE
myapp   10.244.2.12:8080   5h21m

root@cka001:~# kubectl describe ep myapp
Name:         myapp
Namespace:    jh-namespace
Labels:       app=myapp
Annotations:  endpoints.kubernetes.io/last-change-trigger-time: 2022-06-29T08:03:17Z
Subsets:
  Addresses:          10.244.2.12
  NotReadyAddresses:  &lt;none&gt;
  Ports:
    Name     Port  Protocol
    ----     ----  --------
    &lt;unset&gt;  8080  TCP

Events:  &lt;none&gt;
</code></pre>
<p>Get details of Pod of <code>myapp</code>.</p>
<pre><code>root@cka001:~# kubectl get pod -owide
NAME                    READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
myapp-b5d775f5d-6jtgs   1/1     Running   0          70m   10.244.2.12   cka003   &lt;none&gt;           &lt;none&gt;

root@cka001:~# kubectl get node -o wide
NAME     STATUS   ROLES                  AGE    VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
cka001   Ready    control-plane,master   4d2h   v1.23.8   172.16.18.161   &lt;none&gt;        Ubuntu 20.04.4 LTS   5.4.0-113-generic   containerd://1.5.9
cka002   Ready    &lt;none&gt;                 4d1h   v1.23.8   172.16.18.160   &lt;none&gt;        Ubuntu 20.04.4 LTS   5.4.0-113-generic   containerd://1.5.9
cka003   Ready    &lt;none&gt;                 4d1h   v1.23.8   172.16.18.159   &lt;none&gt;        Ubuntu 20.04.4 LTS   5.4.0-113-generic   containerd://1.5.9
</code></pre>
<p>Send http request to the service and node sucessfully. Pod port <code>8080</code> is mapped to node port <code>30520</code>.</p>
<pre><code>root@cka001:~# curl http://10.108.93.159:8080
Hello Kubernetes bootcamp! | Running on: myapp-b5d775f5d-6jtgs | v=1

root@cka001:~# curl http://172.16.18.159:30520
Hello Kubernetes bootcamp! | Running on: myapp-b5d775f5d-6jtgs | v=1
</code></pre>
<h4 id="scalling">Scalling</h4>
<p>Deployment <code>myapp</code> is now having 1 replica.</p>
<pre><code>root@cka001:~# kubectl get deployment myapp -o wide
NAME    READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS            IMAGES                                       SELECTOR
myapp   1/1     1            1           6h12m   kubernetes-bootcamp   docker.io/jocatalin/kubernetes-bootcamp:v1   app=myapp
</code></pre>
<p>Scale to 2 replicas.</p>
<pre><code>root@cka001:~# kubectl scale deployment myapp --replicas=2
deployment.apps/myapp scaled

root@cka001:~# kubectl get deployment myapp -o wide
NAME    READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS            IMAGES                                       SELECTOR
myapp   2/2     2            2           6h14m   kubernetes-bootcamp   docker.io/jocatalin/kubernetes-bootcamp:v1   app=myapp
</code></pre>
<p>Scale to 1 replicas. We can see interim phase that one Pos is been terminating.</p>
<pre><code>root@cka001:~# kubectl get deployment myapp -o wide
NAME    READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS            IMAGES                                       SELECTOR
myapp   1/1     1            1           6h17m   kubernetes-bootcamp   docker.io/jocatalin/kubernetes-bootcamp:v1   app=myapp

root@cka001:~# kubectl get pod
NAME                    READY   STATUS        RESTARTS   AGE
myapp-b5d775f5d-6jtgs   1/1     Running       0          6h17m
myapp-b5d775f5d-mpshb   1/1     Terminating   0          3m28s
</code></pre>
<h4 id="rolling">Rolling</h4>
<p>Get current deployment image version.</p>
<pre><code>root@cka001:~# kubectl get deployment -o wide
NAME    READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS            IMAGES                                       SELECTOR
myapp   1/1     1            1           6h21m   kubernetes-bootcamp   docker.io/jocatalin/kubernetes-bootcamp:v1   app=myapp
</code></pre>
<p>Update image to new versions.</p>
<pre><code>kubectl set image deployment/myapp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v2 --record
kubectl set image deployment/myapp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v3 --record
kubectl set image deployment/myapp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v4 --record
kubectl set image deployment/myapp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v5 --record
</code></pre>
<p>We can observe that Pod's IP is changed to new one, and running on another node.
New Pod is in <code>ImagePullBackOff</code> status due to network issue to access <code>docker.io/jocatalin/kubernetes-bootcamp</code>.</p>
<pre><code>root@cka001:~# kubectl get pod -o wide
NAME                     READY   STATUS             RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
myapp-75ccb85dd6-hzc82   0/1     ImagePullBackOff   0          2m15s   10.244.1.13   cka002   &lt;none&gt;           &lt;none&gt;
myapp-b5d775f5d-6jtgs    1/1     Running            0          6h24m   10.244.2.12   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Let's verify if the service is still available after rolling update. Send http request to the node sucessfully.</p>
<pre><code>root@cka001:~# curl http://172.16.18.160:30520
Hello Kubernetes bootcamp! | Running on: myapp-b5d775f5d-6jtgs | v=1
</code></pre>
<p>Get rolling update history.</p>
<pre><code>root@cka001:~# kubectl rollout history deployment/myapp
deployment.apps/myapp 
REVISION  CHANGE-CAUSE
1         &lt;none&gt;
2         kubectl set image deployment/myapp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v2 --record=true
3         kubectl set image deployment/myapp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v3 --record=true
4         kubectl set image deployment/myapp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v4 --record=true
5         kubectl set image deployment/myapp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v5 --record=true
</code></pre>
<p>Reverse to revision 3. Copied revision <code>3</code> to <code>6</code> as current revision.</p>
<pre><code>root@cka001:~# kubectl rollout undo deployment/myapp --to-revision=3
deployment.apps/myapp rolled back

root@cka001:~# kubectl rollout history deployment/myapp
deployment.apps/myapp 
REVISION  CHANGE-CAUSE
1         &lt;none&gt;
2         kubectl set image deployment/myapp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v2 --record=true
4         kubectl set image deployment/myapp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v4 --record=true
5         kubectl set image deployment/myapp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v5 --record=true
6         kubectl set image deployment/myapp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v3 --record=true
</code></pre>
<h4 id="event">Event</h4>
<p>Get detail event info of related Pod.</p>
<pre><code>root@cka001:~# kubectl describe pod myapp-78bdb65cd8-bnjbj
</code></pre>
<p>Result looks like below.</p>
<pre><code>Events:
  Type     Reason     Age                 From               Message
  ----     ------     ----                ----               -------
  Normal   Scheduled  15m                 default-scheduler  Successfully assigned jh-namespace/myapp-78bdb65cd8-bnjbj to cka002
  Normal   Pulling    14m (x4 over 15m)   kubelet            Pulling image &quot;docker.io/jocatalin/kubernetes-bootcamp:v3&quot;
  Warning  Failed     14m (x4 over 15m)   kubelet            Failed to pull image &quot;docker.io/jocatalin/kubernetes-bootcamp:v3&quot;: rpc error: code = NotFound desc = failed to pull and unpack image &quot;docker.io/jocatalin/kubernetes-bootcamp:v3&quot;: failed to resolve reference &quot;docker.io/jocatalin/kubernetes-bootcamp:v3&quot;: docker.io/jocatalin/kubernetes-bootcamp:v3: not found
  Warning  Failed     14m (x4 over 15m)   kubelet            Error: ErrImagePull
  Warning  Failed     14m (x6 over 15m)   kubelet            Error: ImagePullBackOff
  Normal   BackOff    44s (x65 over 15m)  kubelet            Back-off pulling image &quot;docker.io/jocatalin/kubernetes-bootcamp:v3&quot;
</code></pre>
<p>Get detail event info of entire cluster.</p>
<pre><code>root@cka001:~# kubectl get event
</code></pre>
<h4 id="logging">Logging</h4>
<p>Get log info of Pod.</p>
<pre><code>kubectl logs -f &lt;pod_name&gt;
kubectl logs -f &lt;pod_name&gt; -c &lt;container_name&gt; 
</code></pre>
<pre><code>root@cka001:~# kubectl logs -f myapp-78bdb65cd8-bnjbj
Error from server (BadRequest): container &quot;kubernetes-bootcamp&quot; in pod &quot;myapp-78bdb65cd8-bnjbj&quot; is waiting to start: trying and failing to pull image
</code></pre>
<p>Get log info of K8s components. </p>
<pre><code>root@cka001:~# kubectl logs kube-apiserver-cka001 -n kube-system
root@cka001:~# kubectl logs kube-controller-manager-cka001 -n kube-system
root@cka001:~# kubectl logs kube-scheduler-cka001 -n kube-system
root@cka001:~# kubectl logs etcd-cka001 -n kube-system
root@cka001:~# systemctl status kubelet
root@cka001:~# journalctl -fu kubelet
root@cka001:~# kubectl logs kube-proxy-bj75j -n kube-system
</code></pre>
<h3 id="demo-workload-resources">Demo: Workload Resources</h3>
<h4 id="deployment_2">Deployment</h4>
<p>Create deployment via command <code>kubectl create</code>.</p>
<pre><code>root@cka001:~# kubectl create deployment deploy-http-app1 --image=nginx:1.19
</code></pre>
<p>Create deployment via yaml file and apply it.</p>
<pre><code>root@cka001:~# cat &gt; deploy-http-app2.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-http-app2
  labels:
    app: deploy-http-app2
spec:
  selector:
    matchLabels:
      app: deploy-http-app2
  replicas: 1
  template:
    metadata:
      labels:
        app: deploy-http-app2
    spec:
      containers:
      - name: nginx
        image: nginx:1.19
EOF


root@cka001:~# kubectl apply -f deploy-http-app2.yaml
</code></pre>
<p>Get Deployment Pod created just now.</p>
<pre><code>root@cka001:~# kubectl get pod
NAME                                READY   STATUS    RESTARTS   AGE
deploy-http-app1-7cbc9b645d-zztg9   1/1     Running   0          116s
deploy-http-app2-5f5f7765c9-7hcmt   1/1     Running   0          46s
</code></pre>
<p>Use below commands to check details of deployment pod we creatd just now.</p>
<pre><code># kubectl describe deployment
# kubectl get deployment -oyaml
# kubectl describe pod
# kubectl get pod -oyaml
</code></pre>
<h4 id="statefulset">StatefulSet</h4>
<p>Create StatefulSet with yaml file and apply it.</p>
<pre><code>root@cka001:~# cat &gt; stateufulset-web.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: &quot;nginx&quot;
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
          name: web
EOF

root@cka001:~# kubectl apply -f stateufulset-web.yaml
</code></pre>
<p>Get details of StatefulSet Pod created just now.</p>
<pre><code>root@cka001:~# kubectl get pod | grep web
NAME                                READY   STATUS    RESTARTS   AGE
web-0                               1/1     Running   0          2m1s
web-1                               1/1     Running   0          117s

root@cka001:~# kubectl get sts -o wide
NAME   READY   AGE   CONTAINERS   IMAGES
web    2/2     88s   nginx        nginx
</code></pre>
<p>Use command <code>kubectl edit sts web</code> to update an existing StatefulSet.
ONLY these fields can be updated: <code>replicas</code>、<code>image</code>、<code>rolling updates</code>、<code>labels</code>、<code>resource request/limit</code> and <code>annotations</code>.</p>
<p>Note: 
Copy of StatefulSet Pod will not be created automatically in other node when it's dead in current node.  </p>
<h4 id="daemonset">DaemonSet</h4>
<p>Create DaemonSet with yaml file and apply it.</p>
<pre><code>root@cka001:~# cat &gt; daemonset-busybox.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: daemonset-busybox
  labels:
    app: daemonset-busybox
spec:
  selector:
    matchLabels:
      app: daemonset-busybox
  template:
    metadata:
      labels:
        app: daemonset-busybox
    spec:
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: busybox
        image: busybox:1.28
        args:
        - sleep
        - &quot;10000&quot;
EOF


root@cka001:~# kubectl apply -f daemonset-busybox.yaml
</code></pre>
<p>Get status of DaemonSet Pod. Note, it's deployed on each node.</p>
<pre><code>root@cka001:~# kubectl get daemonset
NAME                DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset-busybox   3         3         3       3            3           &lt;none&gt;          5m33s

root@cka001:~# kubectl get pod -o wide | grep daemonset-busybox
NAME                                READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
daemonset-busybox-kb2kp             1/1     Running   0          75s   10.244.0.6    cka001   &lt;none&gt;           &lt;none&gt;
daemonset-busybox-lnspq             1/1     Running   0          75s   10.244.2.16   cka003   &lt;none&gt;           &lt;none&gt;
daemonset-busybox-r6sc7             1/1     Running   0          75s   10.244.1.17   cka002   &lt;none&gt;           &lt;none&gt;
</code></pre>
<h4 id="job">Job</h4>
<p>Create Job with yaml file and apply it.</p>
<pre><code>root@cka001:~# cat &gt; job-pi.yaml &lt;&lt;EOF
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    spec:
      containers:
      - name: pi
        image: perl:5.34
        command: [&quot;perl&quot;,  &quot;-Mbignum=bpi&quot;, &quot;-wle&quot;, &quot;print bpi(2000)&quot;]
      restartPolicy: Never
  backoffLimit: 4
EOF

root@cka001:~# kubectl apply -f job-pi.yaml
</code></pre>
<p>Get details of Job.</p>
<pre><code>root@cka001:~# kubectl get jobs
</code></pre>
<p>Get details of Job Pod. The status <code>Completed</code> means the job was done successfully.</p>
<pre><code>root@cka001:~# kubectl get pod pi-s28pr
</code></pre>
<p>Get log info of the Job Pod.</p>
<pre><code>root@cka001:~# kubectl logs pi-s28pr
3.141592653589793..............
</code></pre>
<h4 id="cronjob">Cronjob</h4>
<p>Create Cronjob with yaml file and apply it.</p>
<pre><code>root@cka001:~# cat &gt; cronjob-hello.yaml &lt;&lt;EOF
apiVersion: batch/v1
kind: CronJob
metadata:
 name: hello
spec:
  schedule: &quot;*/1 * * * *&quot;
  jobTemplate:
   spec:
    template:
     spec:
      containers:
      - name: hello
        image: busybox
        args:
        - /bin/sh
        - -c
        - date ; echo Hello from the kubernetes cluster
      restartPolicy: OnFailure
EOF


root@cka001:~# kubectl apply -f cronjob-hello.yaml
</code></pre>
<p>Get detail of Cronjob</p>
<pre><code>root@cka001:~# kubectl get cronjobs
</code></pre>
<p>Monitor Jobs. Every 1 minute a new job will be created. </p>
<pre><code>root@cka001:~# kubectl get jobs -w
</code></pre>
<h2 id="label-and-annotation">Label and Annotation</h2>
<h3 id="demo-label-and-annotation">Demo: Label and Annotation</h3>
<h4 id="label">Label</h4>
<p>Set Label <code>disktype=ssd</code> for node <code>cka003</code>.</p>
<pre><code>root@cka001:~# kubectl label node cka002 disktype=ssd
</code></pre>
<p>Get Label info</p>
<pre><code>root@cka001:~# kubectl get node --show-labels
root@cka001:~# kubectl describe node cka003
root@cka001:~# kubectl get node cka003 -oyaml
</code></pre>
<p>Overwrite Label with <code>disktype=hdd</code> for node <code>cka003</code>.</p>
<pre><code>root@cka001:~# kubectl label node cka003 disktype=hdd --overwrite
</code></pre>
<p>Remove Label for node <code>cka003</code></p>
<pre><code>root@cka001:~# kubectl label node cka003 disktype-
</code></pre>
<h4 id="annotation">Annotation</h4>
<p>Create Nginx deployment</p>
<pre><code>root@cka001:~# kubectl create deploy nginx --image=nginx:mainline
</code></pre>
<p>Get Annotation info.</p>
<pre><code>root@cka001:~# kubectl describe deployment/nginx

Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=nginx
</code></pre>
<p>Add new Annotation.</p>
<pre><code>root@cka001:~# kubectl annotate deployment nginx owner=jh

Annotations:            deployment.kubernetes.io/revision: 1
                        owner: jh
Selector:               app=nginx
</code></pre>
<p>Update/Overwrite Annotation.</p>
<pre><code>root@cka001:~# kubectl annotate deployment/nginx owner=qwer --overwrite

Annotations:            deployment.kubernetes.io/revision: 1
                        owner: qwer
Selector:               app=nginx
</code></pre>
<p>Remove Annotation</p>
<pre><code>root@cka001:~# kubectl annotate deployment/nginx owner-

Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=nginx
</code></pre>
<h2 id="health-check">Health Check</h2>
<h3 id="status-of-pod-and-container">Status of Pod and Container</h3>
<p>Create a yaml file <code>multi-pods.yaml</code>. </p>
<pre><code>cat &gt; multi-pods.yaml  &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: multi-pods
  name: multi-pods
spec:
  containers:
  - image: nginx
    name: nginx
  - image: busybox
    name: busybox
  dnsPolicy: ClusterFirst
  restartPolicy: Always
EOF
</code></pre>
<p>Apply the yaml file to create a Pod <code>multi-pods</code> with two containers <code>nginx</code> and <code>busybox</code>. </p>
<pre><code>kubectl apply -f multi-pods.yaml
</code></pre>
<p>Minotor the status with option <code>--watch</code>. The status of Pod was changed from <code>ContainerCreating</code> to <code>NotReady</code> to <code>CrashLoopBackOff</code>.</p>
<pre><code>root@cka001:~# kubectl get pod multi-pods --watch
NAME         READY   STATUS              RESTARTS   AGE
multi-pods   0/2     ContainerCreating   0          49s
multi-pods   1/2     NotReady            1          99s
multi-pods   1/2     CrashLoopBackOff    2          110s
</code></pre>
<p>Get details of the Pod <code>multi-pods</code>, focus on Container's state under segment <code>Containers</code> and Conditions of Pod under segment <code>Conditions</code>.</p>
<pre><code>root@cka001:~# kubectl describe pod multi-pods
......
Containers:
  nginx:
    ......
    State:          Running
      Started:      Sun, 03 Jul 2022 12:52:42 +0800
    Ready:          True
    Restart Count:  0
    ......
  busybox:
    ......
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sun, 03 Jul 2022 12:58:43 +0800
      Finished:     Sun, 03 Jul 2022 12:58:43 +0800
    Ready:          False
    Restart Count:  6
    ......
Conditions:
  Type              Status
  Initialized       True     # Set to True when initCounter completed successfully.
  Ready             False    # Set to True when ContainersReady is True.
  ContainersReady   False    # Set to True when all containers are ready.
  PodScheduled      True     # Set to True when Pos schedule completed successfully.
...... 
</code></pre>
<h3 id="livenessprobe">LivenessProbe</h3>
<p>Detail description of the demo can be found on the <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">Kubernetes document</a>.</p>
<p>Create a yaml file <code>liveness.yaml</code> with <code>livenessProbe</code> setting and apply it.</p>
<pre><code>root@cka001:~# cat &gt; liveness.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - name: liveness
    image: busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
EOF

root@cka001:~# kubectl apply -f liveness.yaml
</code></pre>
<p>Let's see what happened in the Pod <code>liveness-exec</code>.</p>
<ul>
<li>Create a folder <code>/tmp/healthy</code>.</li>
<li>Execute the the command <code>cat /tmp/healthy</code> and return successful code.</li>
<li>After <code>35</code> seconds, execute command <code>rm -rf /tmp/healthy</code> to delete the folder. The probe <code>livenessProbe</code> detects the failure and return error message.</li>
<li>The kubelet kills the container and restarts it. The folder is created again <code>touch /tmp/healthy</code>.</li>
</ul>
<p>By command <code>kubectl describe pod liveness-exec</code>, wec can see below event message. Once failure detected, image will be pulled again and the folder <code>/tmp/healthy</code> is in place again.</p>
<pre><code>Events:
  Type     Reason     Age                   From               Message
  ----     ------     ----                  ----               -------
  Normal   Scheduled  4m21s                 default-scheduler  Successfully assigned jh-namespace/liveness-exec to cka002
  Normal   Pulled     4m19s                 kubelet            Successfully pulled image &quot;busybox&quot; in 1.906981795s
  Normal   Pulled     3m4s                  kubelet            Successfully pulled image &quot;busybox&quot; in 1.967545593s
  Normal   Created    109s (x3 over 4m19s)  kubelet            Created container liveness
  Normal   Started    109s (x3 over 4m19s)  kubelet            Started container liveness
  Normal   Pulled     109s                  kubelet            Successfully pulled image &quot;busybox&quot; in 2.051565102s
  Warning  Unhealthy  66s (x9 over 3m46s)   kubelet            Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory
  Normal   Killing    66s (x3 over 3m36s)   kubelet            Container liveness failed liveness probe, will be restarted
  Normal   Pulling    36s (x4 over 4m21s)   kubelet            Pulling image &quot;busybox&quot;
</code></pre>
<h3 id="readinessprobe">ReadinessProbe</h3>
<p>Readiness probes are configured similarly to liveness probes. The only difference is that you use the readinessProbe field instead of the livenessProbe field.</p>
<p>Create a yaml file <code>readiness.yaml</code> with <code>readinessProbe</code> setting and apply it.</p>
<pre><code>cat &gt; readiness.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: readiness
spec:
    containers:
    - name: readiness
      image: busybox
      args:
      - /bin/sh
      - -c
      - touch /tmp/healthy; sleep 5;rm -rf /tmp/healthy; sleep 600
      readinessProbe:
        exec:
          command:
          - cat
          - /tmp/healthy
        initialDelaySeconds: 10
        periodSeconds: 5
EOF

kubectl apply -f readiness.yaml
</code></pre>
<p>The ready status of the Pod is 0/1, that is, the Pod is not up successfully.</p>
<pre><code>root@cka001:~# kubectl get pod readiness --watch
NAME        READY   STATUS    RESTARTS   AGE
readiness   0/1     Running   0          15s
</code></pre>
<p>Execute command <code>kubectl describe pod readiness</code> to check status of Pod. We will see failure message <code>Readiness probe failed</code>.</p>
<pre><code>Events:
  Type     Reason     Age               From               Message
  ----     ------     ----              ----               -------
  Normal   Scheduled  35s               default-scheduler  Successfully assigned jh-namespace/readiness to cka003
  Normal   Pulling    35s               kubelet            Pulling image &quot;busybox&quot;
  Normal   Pulled     32s               kubelet            Successfully pulled image &quot;busybox&quot; in 2.420171698s
  Normal   Created    32s               kubelet            Created container readiness
  Normal   Started    32s               kubelet            Started container readiness
  Warning  Unhealthy  5s (x4 over 20s)  kubelet            Readiness probe failed: cat: can't open '/tmp/healthy': No such file or directory
</code></pre>
<p>Liveness probes do not wait for readiness probes to succeed. If you want to wait before executing a liveness probe you should use initialDelaySeconds or a startupProbe.</p>
<h3 id="demo-of-health-check">Demo of Health Check</h3>
<p>Set up yaml file of health check for Nginx based Deployment + Service and apply it.</p>
<pre><code>cat &gt; nginx-healthcheck.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-healthcheck
spec:
  replicas: 2
  selector:
    matchLabels:
      name: nginx-healthcheck
  template:
    metadata:
      labels:
        name: nginx-healthcheck
    spec:
      containers:
        - name: nginx-healthcheck
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 80  
          livenessProbe:
            initialDelaySeconds: 5
            periodSeconds: 5
            tcpSocket:
              port: 80
            timeoutSeconds: 5   
          readinessProbe:
            httpGet:
              path: /
              port: 80
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-healthcheck
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
  type: NodePort
  selector:
    name: nginx-healthcheck
EOF


kubectl apply -f nginx-healthcheck.yaml
</code></pre>
<p>Check nginx-healthcheck Pod.</p>
<pre><code>kubectl get pod -owide
</code></pre>
<p>Get below result.</p>
<pre><code>NAME                                 READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
nginx-healthcheck-79fc55d944-9jbvj   1/1     Running   0          50s   10.244.2.82   cka003   &lt;none&gt;           &lt;none&gt;
nginx-healthcheck-79fc55d944-rwx7n   1/1     Running   0          50s   10.244.1.11   cka002   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Access Pod IP via <code>curl</code> command, e.g., above example.</p>
<pre><code>curl 10.244.2.82
curl 10.244.1.11
</code></pre>
<p>We will see a successful <code>index.html</code> conten of Nginx below with above example.</p>
<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>Check details of Service craeted in above example.</p>
<pre><code>kubectl describe svc nginx-healthcheck
</code></pre>
<p>We will see below output. There are two Pods information listed in <code>Endpoints</code>.</p>
<pre><code>Name:                     nginx-healthcheck
Namespace:                jh-namespace
Labels:                   &lt;none&gt;
Annotations:              &lt;none&gt;
Selector:                 name=nginx-healthcheck
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.98.196.231
IPs:                      10.98.196.231
Port:                     &lt;unset&gt;  80/TCP
TargetPort:               80/TCP
NodePort:                 &lt;unset&gt;  31505/TCP
Endpoints:                10.244.1.11:80,10.244.2.82:80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   &lt;none&gt;
</code></pre>
<p>We can also get information of Endpoints.</p>
<pre><code>kubectl get endpoints nginx-healthcheck
</code></pre>
<p>Get below result.</p>
<pre><code>NAME                ENDPOINTS                       AGE
nginx-healthcheck   10.244.1.11:80,10.244.2.82:80   8m35s
</code></pre>
<p>Till now, two <code>nginx-healthcheck</code> Pods are working and providing service as expected. </p>
<p>Let's simulate an error by deleting and <code>index.html</code> file in on of <code>nginx-healthcheck</code> Pod and see what's readinessProbe will do.</p>
<p>First, execute <code>kubectl exec -it &lt;your_pod_name&gt; -- bash</code> to log into <code>nginx-healthcheck</code> Pod, and delete the <code>index.html</code> file.</p>
<pre><code>kubectl exec -it nginx-healthcheck-79fc55d944-9jbvj -- bash
cd /usr/share/nginx/html/
rm -rf index.html
exit
</code></pre>
<p>After that, let's check the status of above Pod that <code>index.html</code> file was deleted.</p>
<pre><code>kubectl describe pod nginx-healthcheck-79fc55d944-9jbvj
</code></pre>
<p>We can now see <code>Readiness probe failed</code> error event message.</p>
<pre><code>Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  29m                default-scheduler  Successfully assigned jh-namespace/nginx-healthcheck-79fc55d944-9jbvj to cka003
  Normal   Pulled     29m                kubelet            Container image &quot;nginx:latest&quot; already present on machine
  Normal   Created    29m                kubelet            Created container nginx-healthcheck
  Normal   Started    29m                kubelet            Started container nginx-healthcheck
  Warning  Unhealthy  1s (x16 over 71s)  kubelet            Readiness probe failed: HTTP probe failed with statuscode: 403
</code></pre>
<p>Let's check another Pod. </p>
<pre><code>kubectl describe pod nginx-healthcheck-79fc55d944-rwx7n
</code></pre>
<p>There is no error info.</p>
<pre><code>Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  31m   default-scheduler  Successfully assigned jh-namespace/nginx-healthcheck-79fc55d944-rwx7n to cka002
  Normal  Pulled     31m   kubelet            Container image &quot;nginx:latest&quot; already present on machine
  Normal  Created    31m   kubelet            Created container nginx-healthcheck
  Normal  Started    31m   kubelet            Started container nginx-healthcheck
</code></pre>
<p>Now, access Pod IP via <code>curl</code> command and see what the result of each Pod.</p>
<pre><code>curl 10.244.2.82
curl 10.244.1.11
</code></pre>
<p>We will receive error while access the first Pod <code>curl 10.244.2.82</code>. The second Pos works well <code>curl 10.244.1.11</code>. </p>
<pre><code>&lt;html&gt;
&lt;head&gt;&lt;title&gt;403 Forbidden&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;&lt;h1&gt;403 Forbidden&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx/1.23.0&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>Let's check current status of Nginx Service after one of Pods runs into failure. </p>
<pre><code>kubectl describe svc nginx-healthcheck
</code></pre>
<p>In below output, there is only one Pod information listed in Endpoint.</p>
<pre><code>Name:                     nginx-healthcheck
Namespace:                jh-namespace
Labels:                   &lt;none&gt;
Annotations:              &lt;none&gt;
Selector:                 name=nginx-healthcheck
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.98.196.231
IPs:                      10.98.196.231
Port:                     &lt;unset&gt;  80/TCP
TargetPort:               80/TCP
NodePort:                 &lt;unset&gt;  31505/TCP
Endpoints:                10.244.1.11:80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   &lt;none&gt;
</code></pre>
<p>Same result we can get by checking information of Endpoints, which is only Pod is running.</p>
<pre><code>kubectl get endpoints nginx-healthcheck 
</code></pre>
<p>Output:</p>
<pre><code>NAME                ENDPOINTS        AGE
nginx-healthcheck   10.244.1.11:80   40m
</code></pre>
<p>Conclusion: 
By delete the index.html file, the Pod is in unhealth status and is removed from endpoint list. 
One one health Pod can provide normal service.</p>
<p>Let's re-create the <code>index.html</code> file again in the Pod. </p>
<pre><code>kubectl exec -it nginx-healthcheck-79fc55d944-9jbvj -- bash

cd /usr/share/nginx/html/

cat &gt; index.html &lt;&lt; EOF 
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
EOF

exit
</code></pre>
<p>We now can see that two Pods are back to Endpoints to provide service now.</p>
<pre><code>kubectl describe svc nginx-healthcheck

kubectl get endpoints nginx-healthcheck
</code></pre>
<p>Re-access Pod IP via <code>curl</code> command and we can see both are back to normal status.</p>
<pre><code>curl 10.244.2.82
curl 10.244.1.11
</code></pre>
<p>Verify the Pod status again. </p>
<pre><code>kubectl describe pod nginx-healthcheck-79fc55d944-9jbvj
</code></pre>
<h2 id="namespace_1">Namespace</h2>
<p>Get list of Namespace</p>
<pre><code>kubectl get namespace
</code></pre>
<p>Get list of Namespace with Label information.</p>
<pre><code>kubectl get ns --show-labels
</code></pre>
<p>Create a Namespace</p>
<pre><code>kubectl create namespace cka
</code></pre>
<p>Label the new created Namespace <code>cka</code>.</p>
<pre><code>kubectl label ns cka cka=true
</code></pre>
<p>Create Nginx Deployment in Namespace <code>cka</code>. </p>
<pre><code>kubectl create deploy nginx --image=nginx --namespace cka
</code></pre>
<p>Check Deployments and Pods running in namespace <code>cka</code>.</p>
<pre><code>kubectl get deploy,pod -n cka
</code></pre>
<p>Result is below.</p>
<pre><code>NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx   1/1     1            1           2m14s

NAME                         READY   STATUS    RESTARTS   AGE
pod/nginx-85b98978db-bmkhf   1/1     Running   0          2m14s
</code></pre>
<p>Delete namespace <code>cka</code>. All resources in the namespaces will be gone.</p>
<pre><code>kubectl delete ns cka
</code></pre>
<h2 id="horizontal-pod-autoscaling-hpa">Horizontal Pod Autoscaling (HPA)</h2>
<ul>
<li>Install Metrics Server component</li>
</ul>
<p>Download yaml file for Metrics Server component</p>
<pre><code>wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
</code></pre>
<p>Replace Google image by Aliyun image <code>image: registry.aliyuncs.com/google_containers/metrics-server:v0.6.1</code>.</p>
<pre><code>sed -i 's/k8s\.gcr\.io\/metrics-server\/metrics-server\:v0\.6\.1/registry\.aliyuncs\.com\/google_containers\/metrics-server\:v0\.6\.1/g' components.yaml
</code></pre>
<p>Change <code>arg</code> of <code>metrics-server</code> by adding <code>--kubelet-insecure-tls</code> to disable tls certificate validation. </p>
<pre><code>vi components.yaml
</code></pre>
<p>Updated <code>arg</code> of <code>metrics-server</code> is below.</p>
<pre><code>  template:
    metadata:
      labels:
        k8s-app: metrics-server
    spec:
      containers:
      - args:
        - --cert-dir=/tmp
        - --secure-port=4443
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --kubelet-use-node-status-port
        - --metric-resolution=15s
        - --kubelet-insecure-tls
        image: registry.aliyuncs.com/google_containers/metrics-server:v0.6.1

</code></pre>
<p>Appy the yaml file <code>components.yaml</code> to deploy <code>metrics-server</code>.</p>
<pre><code>kubectl apply -f components.yaml
</code></pre>
<p>Below resources were crested. </p>
<pre><code>serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
</code></pre>
<p>Verify if <code>metrics-server</code> Pod is running as expected.</p>
<pre><code>kubectl get pod -n kube-system -owide | grep metrics-server
</code></pre>
<p>Get current usage of CPU, memory of each node.</p>
<pre><code>kubectl top node
</code></pre>
<p>Result:</p>
<pre><code>NAME     CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
cka001   148m         7%     1746Mi          45%
cka002   41m          2%     1326Mi          34%       
cka003   39m          1%     1383Mi          36%
</code></pre>
<h3 id="deploy-a-service-podinfo">Deploy a Service <code>podinfo</code></h3>
<p>Create and apply the yaml file <code>podinfo.yaml</code> to deploy Deployment and Service <code>podinfo</code> for further stress testing.</p>
<pre><code>cat &gt; podinfo.yaml &lt;&lt; EOF
apiVersion: v1
kind: Service
metadata:
  name: podinfo
  labels:
    app: podinfo
spec:
  type: NodePort
  ports:
    - port: 9898
      targetPort: 9898
      nodePort: 31198
      protocol: TCP
  selector:
    app: podinfo
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: podinfo
  labels:
    app: podinfo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: podinfo
  template:
    metadata:
      labels:
        app: podinfo
    spec:
      containers:
      - name: podinfod
        image: stefanprodan/podinfo:0.0.1
        imagePullPolicy: Always
        command:
          - ./podinfo
          - -port=9898
          - -logtostderr=true
          - -v=2
        ports:
        - containerPort: 9898
          protocol: TCP
        resources:
          requests:
            memory: &quot;32Mi&quot;
            cpu: &quot;10m&quot;
          limits:
            memory: &quot;256Mi&quot;
            cpu: &quot;100m&quot;
EOF


kubectl apply -f podinfo.yaml
</code></pre>
<h3 id="config-hpa">Config HPA</h3>
<p>Create and apply yaml file <code>hpa.yaml</code> for HPA by setting CPU threshold <code>50%</code> to trigger auto-scalling with minimal <code>2</code> and maximal <code>10</code> Replicas.</p>
<pre><code>cat &gt; hpa.yaml &lt;&lt;EOF
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: nginx
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: podinfo
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 50
EOF


kubectl apply -f hpa.yaml
</code></pre>
<p>Get status of HPA.</p>
<pre><code>kubectl get hpa
</code></pre>
<p>Result:</p>
<pre><code>NAME    REFERENCE            TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
nginx   Deployment/podinfo   10%/50%   2         10        2          26s
</code></pre>
<h3 id="stress-testing">Stress Testing</h3>
<p>Here we will use <code>ab</code> tool to simulate 1000 concurrency.</p>
<p>The <code>ab</code> command is a command line load testing and benchmarking tool for web servers that allows you to simulate high traffic to a website. </p>
<p>The short definition form apache.org is: The acronym <code>ab</code> stands for Apache Bench where bench is short for benchmarking.</p>
<h4 id="install-ab">Install ab</h4>
<p>Execute below command to install <code>ab</code> tool.</p>
<pre><code>apt install apache2-utils -y
</code></pre>
<p>Most common options of <code>ab</code> are <code>-n</code> and <code>-c</code>：</p>
<pre><code>-n requests     Number of requests to perform
-c concurrency  Number of multiple requests to make at a time
-t timelimit    Seconds to max. to spend on benchmarking. This implies -n 50000
-p postfile     File containing data to POST. Remember also to set -T      
-T content-type Content-type header to use for POST/PUT data, eg. 'application/x-www-form-urlencoded'. Default is 'text/plain'
-k              Use HTTP KeepAlive feature
</code></pre>
<p>Example: </p>
<pre><code>ab -n 1000 -c 100 http://www.baidu.com/
</code></pre>
<h4 id="concurrency-stres-test">Concurrency Stres Test</h4>
<p>Simulate 1000 concurrency request to current node running command <code>ab</code>.</p>
<pre><code>ab -c 1000 -t 60 http://127.0.0.1:31198/
</code></pre>
<p>By command <code>kubectl get hpa -w</code> we can see that CPU workload has been increasing.</p>
<pre><code>NAME    REFERENCE            TARGETS    MINPODS   MAXPODS   REPLICAS   AGE
nginx   Deployment/podinfo   388%/50%   2         10        10         32m
</code></pre>
<p>And see auto-scalling automically triggered via commands <code>kubectl get pod</code> and <code>kubectl get deployment</code>.</p>
<pre><code>NAME                                 READY   STATUS    RESTARTS   AGE
nginx-healthcheck-79fc55d944-9jbvj   1/1     Running   0          153m
nginx-healthcheck-79fc55d944-rwx7n   1/1     Running   0          153m
podinfo-668b5b9b5b-4rxwr             1/1     Running   0          51m
podinfo-668b5b9b5b-6vm5k             1/1     Running   0          6m
podinfo-668b5b9b5b-7p74p             1/1     Running   0          5m45s
podinfo-668b5b9b5b-8929m             1/1     Running   0          5m45s
podinfo-668b5b9b5b-9fr28             1/1     Running   0          51m
podinfo-668b5b9b5b-dz74z             1/1     Running   0          6m
podinfo-668b5b9b5b-fzszt             1/1     Running   0          5m30s
podinfo-668b5b9b5b-gb2qq             1/1     Running   0          5m45s
podinfo-668b5b9b5b-tbdvj             1/1     Running   0          5m30s
podinfo-668b5b9b5b-z6dlh             1/1     Running   0          5m45s
</code></pre>
<p>Please be noted the scale up is a phased process rather than a sudden event to scale to max. 
And it'll be scaled down to a balanced status when CPU workload is down.</p>
<pre><code>kubectl get hpa -w
</code></pre>
<p>After several hours, we can see below result with above command.</p>
<pre><code>NAME    REFERENCE            TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
nginx   Deployment/podinfo   0%/50%    2         10        2          8h 
</code></pre>
<h2 id="service">Service</h2>
<h3 id="clusterip">ClusterIP</h3>
<h4 id="create-service">Create Service</h4>
<p>Create a Deployment <code>http-app</code>.
Create a Service with same name and link with Development by Label Selector. 
Service type is <code>ClusterIP</code>, which is default type and accessable internally. </p>
<p>Create yaml file <code>svc-clusterip.yaml</code> and apply it to create Deployment and Service <code>http-app</code>.</p>
<pre><code>cat &gt; svc-clusterip.yaml &lt;&lt;EOF
apiVersion: v1
kind: Service
metadata:
  name: httpd-app
spec:
  type: ClusterIP
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  selector:
    app: httpd
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd-app
spec:
  selector:
    matchLabels:
      app: httpd
  replicas: 2
  template:
    metadata:
      labels:
        app: httpd
    spec:
      containers:
      - name: httpd
        image: httpd
        ports:
        - containerPort: 80
EOF

kubectl apply -f svc-clusterip.yaml
</code></pre>
<p>Execute command <code>kubectl get deployment,service,pod -o wide</code> to check resources we created. </p>
<pre><code>NAME                        READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES   SELECTOR
deployment.apps/httpd-app   2/2     2            2           3m1s   httpd        httpd    app=httpd

NAME                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE    SELECTOR
service/httpd-app   ClusterIP   10.100.67.181   &lt;none&gt;        80/TCP    3m1s   app=httpd

NAME                             READY   STATUS    RESTARTS   AGE    IP            NODE     NOMINATED NODE   READINESS GATES
pod/httpd-app-6496d888c9-mg2jt   1/1     Running   0          3m1s   10.244.2.97   cka003   &lt;none&gt;           &lt;none&gt;
pod/httpd-app-6496d888c9-pdgq8   1/1     Running   0          3m1s   10.244.1.19   cka002   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Verify the access from node <code>cka001</code> to Pod IPs.</p>
<pre><code>curl 10.244.2.97
curl 10.244.1.19
</code></pre>
<p>And receive below successful information.</p>
<pre><code>&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;
</code></pre>
<p>Verify the access via ClusterIP with Port.</p>
<pre><code>curl 10.100.67.181:80
</code></pre>
<p>And receive below successful information.</p>
<pre><code>&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;
</code></pre>
<h4 id="expose-service_1">Expose Service</h4>
<p>Create and attach to a temporary Pod <code>Busybox</code> and use <code>nslookup</code> to verify DNS resolution. The option <code>--rm</code> means delete the Pod after exit.</p>
<pre><code>kubectl run -it nslookup --rm --image=busybox:1.28
</code></pre>
<p>After attach to the Pod, run command <code>nslookup httpd-app</code>. The IP address <code>10.100.67.181</code> of name <code>httpd-app</code> we received is the ClusterIP of Service <code>httpd-app</code>.</p>
<pre><code>/ # nslookup httpd-app
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      httpd-app
Address 1: 10.100.67.181 httpd-app.jh-namespace.svc.cluster.local
</code></pre>
<p>We can check the IP of temporary Pod <code>Busybox</code> in a new terminal by executing command <code>kubectl get pod -o wide</code>. The Pod <code>Busybox</code> has different IP <code>10.244.2.98</code>.</p>
<pre><code>root@cka001:~# kubectl get pod nslookup
NAME                         READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
nslookup                     1/1     Running   0          12m   10.244.2.98   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<h3 id="nodeport">NodePort</h3>
<p>Create and apply yaml file <code>svc-nodeport.yaml</code> to create a Service <code>httpd-app</code>.</p>
<pre><code>cat &gt; svc-nodeport.yaml &lt;&lt;EOF
apiVersion: v1
kind: Service
metadata:
  name: httpd-app
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 80
    nodePort: 30080
  selector:
     app: httpd
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd-app
spec:
  selector:
    matchLabels:
      app: httpd
  replicas: 2
  template:
    metadata:
      labels:
        app: httpd
    spec:
      containers:
      - name: httpd
        image: httpd
        ports:
        - containerPort: 80
EOF


kubectl apply -f svc-nodeport.yaml
</code></pre>
<p>We will receive below output. The command <code>kubectl apply -f &lt;yaml_file&gt;</code> will update configuration to existing resources.
Here the Service <code>httpd-app</code> is changed from <code>ClusterIP</code> to <code>NodePort</code> type. No change to the Deployment <code>httpd-app</code>.</p>
<pre><code>service/httpd-app configured
deployment.apps/httpd-app unchanged
</code></pre>
<p>Check the Service <code>httpd-app</code> via <code>kubectl get svc</code>. 
IP is the same.
Type is changed to NodePort.
Port numbers is changed from <code>80/TCP</code> to <code>80:30080/TCP</code>.</p>
<pre><code>NAME        TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
httpd-app   NodePort   10.100.67.181   &lt;none&gt;        80:30080/TCP   78m
</code></pre>
<p>Test the connection to the Service <code>httpd-app</code> via command <code>curl &lt;your_node_ip&gt;:30080</code>. It's node IP, not cluster IP, nor Pod IP.
We will receive below successful information.</p>
<pre><code>&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;
</code></pre>
<h3 id="special-service">Special Service</h3>
<h4 id="headless-service">Headless Service</h4>
<p>Create and apply yaml file <code>svc-headless.yaml</code> to create a <code>Headless Service</code>.</p>
<pre><code>cat &gt; svc-headless.yaml &lt;&lt;EOF
apiVersion: v1
kind: Service
metadata:
  name: web
  labels:
    app: web
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: web
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: &quot;web&quot;
  replicas: 2
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: web
        image: nginx
        ports:
        - containerPort: 80
          name: web
EOF

kubectl apply -f svc-headless.yaml
</code></pre>
<p>Check Pos by command <code>kubectl get pod -owide -l app=web</code>.</p>
<pre><code>NAME    READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
web-0   1/1     Running   0          85s   10.244.2.99   cka003   &lt;none&gt;           &lt;none&gt;
web-1   1/1     Running   0          82s   10.244.1.20   cka002   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Get details of the Service by command <code>kubectl describe svc -l app=web</code>.</p>
<pre><code>Name:              web
Namespace:         jh-namespace
Labels:            app=web
Annotations:       &lt;none&gt;
Selector:          app=web
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                None
IPs:               None
Port:              web  80/TCP
TargetPort:        80/TCP
Endpoints:         10.244.1.20:80,10.244.2.99:80
Session Affinity:  None
Events:            &lt;none&gt;
</code></pre>
<p>启动一个Busybox Pod，使用 nslookup 来 测试 DNS 解析</p>
<p>Attach to the temporary Pod <code>Busybox</code> and use <code>nslookup</code> to verify DNS resolution.</p>
<pre><code>kubectl run -it nslookup --rm --image=busybox:1.28
</code></pre>
<p>With <code>nslookup</code> command for Headless Service <code>web</code>, we received two IP of Pods, not ClusterIP due to Headless Service. </p>
<pre><code>/ # nslookup web
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web
Address 1: 10.244.2.99 web-0.web.jh-namespace.svc.cluster.local
Address 2: 10.244.1.20 web-1.web.jh-namespace.svc.cluster.local
</code></pre>
<p>We can also use <code>nslookup</code> for <code>web-0.web</code> and <code>web-0.web</code>. Every Pod of Headless Service has own Service Name for DNS lookup.</p>
<pre><code>/ # nslookup web-0.web
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-0.web
Address 1: 10.244.2.99 web-0.web.jh-namespace.svc.cluster.local
</code></pre>
<p>Clean up all resources created before.</p>
<h2 id="ingress">Ingress</h2>
<h3 id="deploy-ingress-controller">Deploy Ingress Controller</h3>
<p>Get Ingress Controller yaml file.</p>
<pre><code>wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.2.1/deploy/static/provider/cloud/deploy.yaml
</code></pre>
<p>Replace grc.io to Aliyun.</p>
<ul>
<li><code>k8s.gcr.io/ingress-nginx/kube-webhook-certgen</code> to <code>registry.aliyuncs.com/google_containers/kube-webhook-certgen</code>.</li>
<li><code>k8s.gcr.io/ingress-nginx/controller</code> to <code>registry.aliyuncs.com/google_containers/nginx-ingress-controller</code>.</li>
</ul>
<pre><code>sed -i 's/k8s.gcr.io\/ingress-nginx\/kube-webhook-certgen/registry.aliyuncs.com\/google\_containers\/kube-webhook-certgen/g' deploy.yaml
sed -i 's/k8s.gcr.io\/ingress-nginx\/controller/registry.aliyuncs.com\/google\_containers\/nginx-ingress-controller/g' deploy.yaml
</code></pre>
<p>Apply the yaml file <code>deploy.yaml</code> to create Ingress Nginx.</p>
<pre><code>kubectl apply -f deploy.yaml
</code></pre>
<p>Check the status of Pod.
Please be noted that a new namespace <code>ingress-nginx</code> was created and Ingress Nginx resources are running under the new namespace.</p>
<pre><code>kubectl get pod -n ingress-nginx
</code></pre>
<p>The result is below.</p>
<pre><code>NAME                                        READY   STATUS      RESTARTS   AGE
ingress-nginx-admission-create-dcsww        0/1     Completed   0          3m32s
ingress-nginx-admission-patch-hslwf         0/1     Completed   0          3m32s
ingress-nginx-controller-556fbd6d6f-trl9r   1/1     Running     0          3m32s
</code></pre>
<h3 id="create-deployments">Create Deployments</h3>
<p>Create two deployment <code>nginx-app-1</code> and <code>nginx-app-2</code>.</p>
<pre><code>cat &gt; nginx-app.yaml &lt;&lt; EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-app-1
spec:
  selector:
    matchLabels:
      app: nginx-app-1
  replicas: 1 
  template:
    metadata:
      labels:
        app: nginx-app-1
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
          - name: html
            mountPath: /usr/share/nginx/html
      volumes:
       - name: html
         hostPath:
           path: /root/html-1
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-app-2
spec:
  selector:
    matchLabels:
      app: nginx-app-2
  replicas: 1 
  template:
    metadata:
      labels:
        app: nginx-app-2
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
          - name: html
            mountPath: /usr/share/nginx/html
      volumes:
       - name: html
         hostPath:
           path: /root/html-2
EOF


kubectl apply -f nginx-app.yaml
</code></pre>
<p>Get status of Pods by executing <code>kubectl get pod -o wide</code>. Two Pods are running on node <code>cka003</code> with two different Pod IPs.</p>
<pre><code>NAME                           READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
nginx-app-1-695b7b647d-l76bh   1/1     Running   0          34s   10.244.2.104   cka003   &lt;none&gt;           &lt;none&gt;
nginx-app-2-7f6bf6f4d4-lvbz8   1/1     Running   0          34s   10.244.2.105   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Access to two Pod via curl. We get <code>403</code> error.</p>
<pre><code>curl 10.244.2.104
</code></pre>
<p>Log onto node <code>cka003</code> and create <code>index.html</code> file in path <code>/root/html-1/</code>. The directory <code>/root/html-1/</code> is already in place after <code>nginx-app-1</code> and <code>nginx-app-2</code> created.</p>
<pre><code>echo 'This is test 1 !!' &gt; /root/html-1/index.html
echo 'This is test 2 !!' &gt; /root/html-2/index.html
</code></pre>
<p>Check Pods status again by executing <code>kubectl get pod -o wide</code>.</p>
<pre><code>NAME                           READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
nginx-app-1-695b7b647d-l76bh   1/1     Running   0          6m11s   10.244.2.104   cka003   &lt;none&gt;           &lt;none&gt;
nginx-app-2-7f6bf6f4d4-lvbz8   1/1     Running   0          6m11s   10.244.2.105   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Access to two Pod via curl. </p>
<pre><code>curl 10.244.2.104
curl 10.244.2.105
</code></pre>
<p>We get correct information now.</p>
<pre><code>This is test 1 !!
This is test 2 !!
</code></pre>
<h3 id="create-service_1">Create Service</h3>
<p>Create Service <code>nginx-app-1</code> and <code>nginx-app-2</code> and map to related deployment <code>nginx-app-1</code> and <code>nginx-app-2</code>.</p>
<pre><code>cat &gt; nginx-svc.yaml &lt;&lt; EOF
apiVersion: v1
kind: Service
metadata:
 name: nginx-app-1
spec:
 ports:
 - protocol: TCP
   port: 80
   targetPort: 80
 selector:
   app: nginx-app-1
---
kind: Service
apiVersion: v1
metadata:
 name: nginx-app-2
spec:
 ports:
 - protocol: TCP
   port: 80
   targetPort: 80
 selector:
   app: nginx-app-2
EOF


kubectl apply -f nginx-svc.yaml
</code></pre>
<p>Check the status by executing <code>kubectl get svc -o wide</code>.</p>
<pre><code>NAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE   SELECTOR
nginx-app-1   ClusterIP   10.111.95.99   &lt;none&gt;        80/TCP    22s   app=nginx-app-1
nginx-app-2   ClusterIP   10.96.15.218   &lt;none&gt;        80/TCP    22s   app=nginx-app-2
</code></pre>
<p>Access to two Service via curl. </p>
<pre><code>curl 10.111.95.99
curl 10.96.15.218
</code></pre>
<p>We get correct information.</p>
<pre><code>This is test 1 !!
This is test 2 !!
</code></pre>
<h3 id="create-ingress">Create Ingress</h3>
<p>Create Ingress resource via file <code>nginx-app-ingress.yaml</code>. 
Map to two Services <code>nginx-app-1</code> and <code>nginx-app-1</code> we created..
Change the namespace if needed.</p>
<pre><code>cat &gt; nginx-app-ingress.yaml &lt;&lt; EOF
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-app
  namespace: jh-namespace
spec:
  ingressClassName: &quot;nginx&quot;
  rules:
  - host: app1.com
    http:
      paths:
      - pathType: Prefix
        path: &quot;/&quot;
        backend:
          service:
            name: nginx-app-1
            port: 
              number: 80
  - host: app2.com
    http:
      paths:
      - pathType: Prefix
        path: &quot;/&quot;
        backend:
          service:
            name: nginx-app-2
            port:
              number: 80
EOF


kubectl apply -f nginx-app-ingress.yaml
</code></pre>
<p>Get Ingress status by executing command <code>kubectl get ingress</code>.</p>
<pre><code>NAME        CLASS   HOSTS               ADDRESS   PORTS   AGE
nginx-app   nginx   app1.com,app2.com             80      64s
</code></pre>
<h3 id="test-accessiblity">Test Accessiblity</h3>
<p>By executing <code>kubectl get pod -n ingress-nginx -o wide</code>, we know Ingress Controllers are running on node <code>cka002</code>.</p>
<pre><code>NAME                                        READY   STATUS      RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
ingress-nginx-admission-create-dcsww        0/1     Completed   0          33m   10.244.2.102   cka003   &lt;none&gt;           &lt;none&gt;
ingress-nginx-admission-patch-hslwf         0/1     Completed   0          33m   10.244.2.103   cka003   &lt;none&gt;           &lt;none&gt;
ingress-nginx-controller-556fbd6d6f-trl9r   1/1     Running     0          33m   10.244.1.22    cka002   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>By executing <code>kubectl get node -o wide</code>, we know node <code>cka002</code>'s IP is <code>172.16.18.159</code>.</p>
<pre><code>NAME     STATUS   ROLES                  AGE   VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
cka001   Ready    control-plane,master   13d   v1.23.8   172.16.18.161   &lt;none&gt;        Ubuntu 20.04.4 LTS   5.4.0-113-generic   containerd://1.5.9
cka002   Ready    &lt;none&gt;                 13d   v1.23.8   172.16.18.160   &lt;none&gt;        Ubuntu 20.04.4 LTS   5.4.0-113-generic   containerd://1.5.9
cka003   Ready    &lt;none&gt;                 13d   v1.23.8   172.16.18.159   &lt;none&gt;        Ubuntu 20.04.4 LTS   5.4.0-113-generic   containerd://1.5.9
</code></pre>
<p>Add below into <code>/etc/hosts file</code> in one of node. Put node IP below. In above example, IP of node <code>cka002</code> is <code>172.16.18.160</code>, which is running <code>ingress-nginx-controller</code>.</p>
<pre><code>cat &gt;&gt; /etc/hosts &lt;&lt; EOF
172.16.18.160   app1.com
172.16.18.160   app2.com
EOF
</code></pre>
<p>By executing <code>kubectl -n ingress-nginx get svc</code> to get NodePort of Ingress Controller. </p>
<pre><code>NAME                                 TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx-controller             LoadBalancer   10.110.117.253   &lt;pending&gt;     80:31640/TCP,443:31338/TCP   40m
ingress-nginx-controller-admission   ClusterIP      10.107.32.104    &lt;none&gt;        443/TCP                      40m
</code></pre>
<p>Two files <code>index.html</code> are in two Pods, the web services are exposed to outside via node IP. 
Hence we can see below result. The <code>ingress-nginx-controller</code> plays a central entry point for outside access, and provide two ports for different backend services from Pods.</p>
<pre><code>curl app1.com:31640
This is test 1 !!
</code></pre>
<pre><code>curl app2.com:31338
This is test 2 !!
</code></pre>
<h2 id="storage">Storage</h2>
<h3 id="emptydir">emptyDir</h3>
<p>Create a Pod with <code>emptyDir</code> type Volume.</p>
<pre><code>cat &gt; pod-emptydir.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
 name: hello-producer
spec:
 containers:
 - image: busybox
   name: producer
   volumeMounts:
   - mountPath: /producer_dir
     name: shared-volume
   args:
   - /bin/sh
   - -c
   - echo &quot;hello world&quot; &gt; /producer_dir/hello; sleep 30000
 volumes:
 - name: shared-volume
   emptyDir: {}
EOF


kubectl apply -f pod-emptydir.yaml
</code></pre>
<p>Check which node the Pod <code>hello-producer</code> is running. </p>
<pre><code>kubectl get pod hello-producer -owide
</code></pre>
<p>The Pod is running on node <code>cka003</code>.</p>
<pre><code>NAME             READY   STATUS    RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES
hello-producer   1/1     Running   0          106s   10.244.2.106   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Log onto <code>cka003</code> because the Pod <code>hello-producer</code> is running on the node.</p>
<p>Set up the environment <code>CONTAINER_RUNTIME_ENDPOINT</code> for command <code>crictl</code>. Suggest to do the same for all nodes.</p>
<pre><code>export CONTAINER_RUNTIME_ENDPOINT=unix:///run/containerd/containerd.sock
</code></pre>
<p>Run command <code>crictl ps</code> to get the container ID of Pod <code>hello-producer</code>.</p>
<pre><code>crictl ps |grep hello-producer
</code></pre>
<p>The ID of container <code>producer</code> is <code>05f5e1bb6a1bb</code>.</p>
<pre><code>CONTAINER           IMAGE               CREATED             STATE               NAME                ATTEMPT             POD ID              POD
05f5e1bb6a1bb       62aedd01bd852       15 minutes ago      Running             producer            0                   995cbc23eb763       hello-producer
</code></pre>
<p>Run command <code>crictl inspect</code> to get the path of mounted <code>shared-volume</code>, which is <code>emptyDir</code>.</p>
<pre><code>crictl inspect 05f5e1bb6a1bb | grep source | grep empty
</code></pre>
<p>The result is below.</p>
<pre><code>&quot;source&quot;: &quot;/var/lib/kubelet/pods/272ba5fa-e213-4c79-ab57-d4c91f4371ba/volumes/kubernetes.io~empty-dir/shared-volume&quot;,
</code></pre>
<p>Change the path to the path of mounted <code>shared-volume</code> we get above.</p>
<pre><code>cd /var/lib/kubelet/pods/272ba5fa-e213-4c79-ab57-d4c91f4371ba/volumes/kubernetes.io~empty-dir/shared-volume
cat hello
</code></pre>
<p>We will see the content of file <code>hello</code>. </p>
<p>The path <code>/producer_dir</code> inside the Pod is mounted to the local host path. 
The file <code>/producer_dir/hello</code> we created inside the Pod is actually in the host local path.</p>
<p>Let's delete the container <code>producer</code>, the file <code>hello</code> is still there.</p>
<pre><code>crictl ps
crictl stop &lt;your_container_id&gt;
crictl rm &lt;your_container_id&gt;
ls /var/lib/kubelet/pods/272ba5fa-e213-4c79-ab57-d4c91f4371ba/volumes/kubernetes.io~empty-dir/shared-volume
</code></pre>
<p>Let's delete the Pod <code>hello-producer</code>, the file <code>hello</code> was gone with error <code>No such file or directory</code>.</p>
<pre><code>kubectl delete pod hello-producer
ls /var/lib/kubelet/pods/272ba5fa-e213-4c79-ab57-d4c91f4371ba/volumes/kubernetes.io~empty-dir/shared-volume
</code></pre>
<h3 id="hostpath">hostPath</h3>
<p>Apply below yaml file to create a MySQL Pod and mount a <code>hostPath</code>.
It'll mount host directory <code>/tmp/mysql</code> to Pod directory <code>/var/lib/mysql</code>.
Check locally if directory <code>/tmp/mysql</code> is in place. If not, create it using <code>mkdir /tmp/mysql</code>.</p>
<pre><code>cat &gt; mysql-hostpath.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
 name: mysql
spec:
 selector:
   matchLabels:
     app: mysql
 template:
   metadata:
     labels:
       app: mysql
   spec:
     containers:
     - image: mysql:8.0
       name: mysql
       env:
       - name: MYSQL_ROOT_PASSWORD
         value: password
       ports:
       - containerPort: 3306
         name: mysql
       volumeMounts:
       - name: mysql-vol
         mountPath: /var/lib/mysql
     volumes:
     - hostPath:
            path: /tmp/mysql
       name: mysql-vol
EOF

kubectl apply -f mysql-hostpath.yaml
</code></pre>
<h4 id="verify-mysql-availability">Verify MySQL Availability</h4>
<p>Check status of MySQL Pod. Need document the Pod name and node it's running on.</p>
<pre><code>kubectl get pod -l app=mysql -o wide
</code></pre>
<p>Attach into the MySQL Pod on the running node.</p>
<pre><code>kubectl exec -it &lt;your_pod_name&gt; -- bash
</code></pre>
<p>Within the Pod, go to directory <code>/var/lib/mysql</code>, all files in the directory are same with all files in host directory <code>/tmp/mysql</code>.</p>
<p>Connect to the database in the Pod.</p>
<pre><code>mysql -h 127.0.0.1 -uroot -ppassword
</code></pre>
<p>Operate the database.</p>
<pre><code>mysql&gt; show databases;
mysql&gt; connect mysql;
mysql&gt; show tables;
</code></pre>
<h3 id="pv-and-pvc">PV and PVC</h3>
<p>Here we will use NFS as backend storage to demo how to deploy PV and PVC.</p>
<h4 id="set-up-nfs-server">Set up NFS Server</h4>
<h5 id="install-nfs-kernel-server">Install nfs-kernel-server</h5>
<p>Log onto <code>cka002</code>.</p>
<p>Choose one Worker <code>cka002</code> to build NFS server. </p>
<pre><code>sudo apt-get install -y nfs-kernel-server
</code></pre>
<h5 id="configure-share-folder">Configure Share Folder</h5>
<p>Create share folder.  </p>
<pre><code>mkdir /nfsdata
</code></pre>
<p>Append one line in file <code>/etc/exports</code>.</p>
<pre><code>cat &gt;&gt; /etc/exports &lt;&lt; EOF
/nfsdata *(rw,sync,no_root_squash)
EOF
</code></pre>
<p>There are many different NFS sharing options, including these:</p>
<ul>
<li><code>*</code>: accessable to all IPs, or specific IPs.</li>
<li><code>rw</code>: Share as read-write. Keep in mind that normal Linux permissions still apply. (Note that this is a default option.)</li>
<li><code>ro</code>: Share as read-only.</li>
<li><code>sync</code>: File data changes are made to disk immediately, which has an impact on performance, but is less likely to result in data loss. On som* `distributions this is the default.</li>
<li><code>async</code>: The opposite of sync; file data changes are made initially to memory. This speeds up performance but is more likely to result in data loss. O* `some distributions this is the default.</li>
<li><code>root_squash</code>: Map the root user and group account from the NFS client to the anonymous accounts, typically either the nobody account or the nfsnobod* `account. See the next section, “User ID Mapping,” for more details. (Note that this is a default option.)</li>
<li><code>no_root_squash</code>: Map the root user and group account from the NFS client to the local root and group accounts.</li>
</ul>
<p>We will use password-free remote mount based on <code>nfs</code> and <code>rpcbind</code> services between Linux servers, not based on <code>smb</code> service. 
The two servers must first grant credit, install and set up nfs and rpcbind services on the server side, set the common directory, start the service, and mount it on the client</p>
<p>Start <code>rpcbind</code> service.</p>
<pre><code>sudo systemctl enable rpcbind
sudo systemctl restart rpcbind
</code></pre>
<p>Start <code>nfs</code> service.</p>
<pre><code>sudo systemctl enable nfs-server
sudo systemctl start nfs-server
</code></pre>
<p>Once <code>/etc/exports</code> is changed, we need run below command to make change effected.</p>
<pre><code>exportfs -ra
</code></pre>
<p>Check whether sharefolder is configured. </p>
<pre><code>showmount -e
</code></pre>
<p>And see below output.</p>
<pre><code>Export list for cka002:
/nfsdata *
</code></pre>
<h5 id="install-nfs-client">Install NFS Client</h5>
<p>Install NFS client on all nodes.</p>
<pre><code>sudo apt-get install -y nfs-common
</code></pre>
<h5 id="verify-nfs-server">Verify NFS Server</h5>
<p>Log onto any nodes to verify NFS service and sharefolder list.</p>
<p>Log onto <code>cka001</code> and check sharefolder status on <code>cka002</code>.</p>
<pre><code>showmount -e cka002
</code></pre>
<p>Below result will be shown if no issues.</p>
<pre><code>Export list for cka002:
/nfsdata *
</code></pre>
<h5 id="mount-nfs">Mount NFS</h5>
<p>Execute below command to mount remote NFS folder on any other non-NFS-server node, e.g., <code>cka001</code> or <code>cka003</code>.</p>
<pre><code>mkdir /remote-nfs-dir
mount -t nfs cka002:/nfsdata /remote-nfs-dir/
</code></pre>
<p>Use command <code>df -h</code> to verify mount point. Below is the sample output.</p>
<pre><code>Filesystem       Size  Used Avail Use% Mounted on
cka002:/nfsdata   40G  6.8G   31G  18% /remote-nfs-dir
</code></pre>
<h4 id="create-pv">Create PV</h4>
<p>Create a PV with below yaml file <code>mysql-pv.yaml</code>. Replace the NFS Server IP with actual IP that NFS server is running on.</p>
<pre><code>cat &gt; mysql-pv.yaml &lt;&lt;EOF
apiVersion: v1
kind: PersistentVolume
metadata:
 name: mysql-pv
spec:
 accessModes:
     - ReadWriteOnce
 capacity:
   storage: 1Gi
 persistentVolumeReclaimPolicy: Retain
 storageClassName: nfs
 nfs:
   path: /nfsdata/
   server: 172.16.18.160
EOF

kubectl apply -f mysql-pv.yaml
</code></pre>
<p>Check the PV.</p>
<pre><code>kubectl get pv
</code></pre>
<p>The result:</p>
<pre><code>NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
mysql-pv   1Gi        RWO            Retain           Available           nfs                     9s
</code></pre>
<h4 id="create-pvc">Create PVC</h4>
<p>Create a PVC with below yaml file <code>mysql-pvc.yaml</code>, specify storage size, access mode, and storage class. 
The PVC will be binded with PV automatically viw storage class name. </p>
<pre><code>cat &gt; mysql-pvc.yaml &lt;&lt;EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: nfs
EOF

kubectl apply -f mysql-pvc.yaml
</code></pre>
<h4 id="consume-pvc">Consume PVC</h4>
<p>Create a Deployment to consume the PVC created.</p>
<pre><code>cat &gt; mysql.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
 name: mysql
spec:
 selector:
   matchLabels:
     app: mysql
 template:
   metadata:
     labels:
       app: mysql
   spec:
     containers:
     - image: mysql:8.0
       name: mysql
       env:
       - name: MYSQL_ROOT_PASSWORD
         value: password
       ports:
       - containerPort: 3306
         name: mysql
       volumeMounts:
       - name: mysql-persistent-storage
         mountPath: /var/lib/mysql
         subPath: mysqldata
     volumes:
     - name: mysql-persistent-storage
       persistentVolumeClaim:
        claimName: mysql-pvc
EOF


kubectl apply -f mysql.yaml
</code></pre>
<p>Now we can see MySQL files were moved to directory <code>/nfsdata</code> on <code>cka002</code></p>
<h3 id="storageclass">StorageClass</h3>
<h4 id="configure-rbac-authorization">Configure RBAC Authorization</h4>
<p>RBAC authorization uses the rbac.authorization.k8s.io API group to drive authorization decisions, allowing you to dynamically configure policies through the Kubernetes API.</p>
<ul>
<li>ServiceAccount: <code>nfs-client-provisioner</code></li>
<li>
<p>namespace: <code>jh-namespace</code></p>
</li>
<li>
<p>ClusterRole: <code>nfs-client-provisioner-runner</code></p>
</li>
<li>
<p>ClusterRoleBinding: <code>run-nfs-client-provisioner</code>, roleRefer: above ClusterRole, link to above ServiceAccount.</p>
</li>
<li>
<p>Role: <code>leader-locking-nfs-client-provisioner</code></p>
</li>
<li>RoleBinding: <code>leader-locking-nfs-client-provisioner</code>, roleRefer: above Role, link to above ServiceAccount.</li>
</ul>
<p>Create RBAC Authorization.</p>
<pre><code>cat &gt; nfs-provisioner-rbac.yaml &lt;&lt;EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: jh-namespace
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [&quot;&quot;]
    resources: [&quot;nodes&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;persistentvolumes&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;persistentvolumeclaims&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]
  - apiGroups: [&quot;storage.k8s.io&quot;]
    resources: [&quot;storageclasses&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;events&quot;]
    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: jh-namespace
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: jh-namespace
rules:
  - apiGroups: [&quot;&quot;]
    resources: [&quot;endpoints&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: jh-namespace
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: jh-namespace
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io
EOF


kubectl apply -f nfs-provisioner-rbac.yaml
</code></pre>
<h4 id="install-nfs-provisioner">Install <code>nfs-provisioner</code></h4>
<p>Create NFS Provisioner with below yaml file. </p>
<pre><code>cat &gt; nfs-provisioner-deployment.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nfs-client-provisioner
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: liyinlin/nfs-subdir-external-provisioner:v4.0.2
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: k8s-sigs.io/nfs-subdir-external-provisioner
            - name: NFS_SERVER
              value: 172.16.18.160
            - name: NFS_PATH
              value: /nfsdata
      volumes:
        - name: nfs-client-root
          nfs:
            server: 172.16.18.160
            path: /nfsdata
EOF

kubectl apply -f nfs-provisioner-deployment.yaml
</code></pre>
<h4 id="create-nfs-storageclass">Create NFS StorageClass</h4>
<p>Create yaml file <code>nfs-storageclass.yaml</code>.</p>
<pre><code>vi nfs-storageclass.yaml
</code></pre>
<p>And add below info to create NFS StorageClass.</p>
<pre><code>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-client
  annotations:
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
parameters:
  pathPattern: &quot;${.PVC.namespace}/${.PVC.annotations.nfs.io/storage-path}&quot;
  onDelete: delete
</code></pre>
<p>Apply the yaml file.</p>
<pre><code>kubectl apply -f nfs-storageclass.yaml
</code></pre>
<h4 id="verify">Verify</h4>
<h5 id="create-pvc_1">Create PVC</h5>
<p>Create PVC </p>
<pre><code>cat &gt; storageclass-pvc.yaml &lt;&lt;EOF
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: nfs-pvc-from-sc
spec:
  storageClassName: nfs-client
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
EOF

kubectl apply -f storageclass-pvc.yaml
</code></pre>
<p>Check the PVC status we ceated.</p>
<pre><code>kubectl get pvc nfs-pvc-from-sc
</code></pre>
<pre><code>NAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
nfs-pvc-from-sc   Bound    pvc-25eaa043-911e-46c7-b17e-f65256f52725   1Gi        RWX            nfs-client     39h
</code></pre>
<h5 id="consume-pvc_1">Consume PVC</h5>
<p>Create Pod to consume the PVC&gt;</p>
<pre><code>cat &gt; mysql-with-sc-pvc.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-with-sc-pvc
spec:
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - image: mysql:8.0
        name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: password
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: mysql-pv
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-pv
        persistentVolumeClaim:
          claimName: nfs-pvc-from-sc
EOF


kubectl apply -f mysql-with-sc-pvc.yaml
</code></pre>
<p>Check the Deployment status.</p>
<pre><code>kubectl get deployment mysql-with-sc-pvc -o wide
</code></pre>
<pre><code>NAME                READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES      SELECTOR
mysql-with-sc-pvc   1/1     1            1           39h   mysql        mysql:8.0   app=mysql
</code></pre>
<p>Check related Pods status. Be noted that the Pod <code>mysql-with-sc-pvc-7c97d875f8-dwfkc</code> is running on <code>cka003</code>.</p>
<pre><code>kubectl get pod -o wide -l app=mysql
</code></pre>
<pre><code>NAME                                 READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
mysql-774db46945-sztrp               1/1     Running   0          40h   10.244.1.23    cka002   &lt;none&gt;           &lt;none&gt;
mysql-with-sc-pvc-7c97d875f8-dwfkc   1/1     Running   0          38h   10.244.2.110   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Let's check directory <code>/nfsdata/</code> on NFS server. </p>
<pre><code>ll /nfsdata/
</code></pre>
<p>Two folders were created. Same content of <code>/remote-nfs-dir/</code> on other nodes.</p>
<pre><code>drwxrwxrwx  6 systemd-coredump root 4096 Jul 10 23:08 jh-namespace/
drwxr-xr-x  6 systemd-coredump root 4096 Jul 10 21:23 mysqldata/
</code></pre>
<p>One sub-folder's name is namespace under directory <code>/nfsdata/</code> and it is mounted to Pod.
By default, namespace name will be used at mount point. 
If we want to use customized folder for that purpose, we need claim an annotation <code>nfs.io/storage-path</code>, e.g., <code>test-path</code> in below example.</p>
<pre><code>cat &gt; test-claim.yaml &lt;&lt;EOF
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-claim
  namespace: kube-system
  annotations:
    nfs.io/storage-path: &quot;test-path&quot;
spec:
  storageClassName: nfs-client
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
EOF

kubectl apply -f test-claim.yaml
</code></pre>
<p>In above case, the PVC was created in <code>kube-system</code> Namespace, hence we can see directory <code>test-path</code> is under directory<code>kube-system</code>. 
Overall directory structure of folder <code>/nfsdata/</code> looks like below.</p>
<pre><code>tree /nfsdata/
</code></pre>
<pre><code>/nfsdata/
├── jh-namespace
│   ├── mysql.sock -&gt; /var/run/mysqld/mysqld.sock
│   ├── sys
│   │   └── sys_config.ibd
│   ├── undo_001
│   └── undo_002
├── kube-system
│   └── test-path
└── mysqldata
</code></pre>
<p>Please be noted that above rule is following <code>nfs-subdir-external-provisioner</code> implementation. It's may be different with other <code>provisioner</code>.</p>
<p>Detail about <code>nfs-subdir-external-provisioner</code> project is <a href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner">here</a></p>
<hr />
<h3 id="configuration">Configuration</h3>
<h4 id="configmap">ConfigMap</h4>
<p>Create a yaml file <code>configmap.yaml</code> for ConfigMap.</p>
<pre><code>vi configmap.yaml
</code></pre>
<p>Paste below content.</p>
<pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    cattle.io/creator: norman
  name: nginx
  namespace: jh-namespace
data:
  nginx.conf: |-
    user  nginx;
    worker_processes  2;

    error_log  /var/log/nginx/error.log warn;
    pid        /var/run/nginx.pid;


    events {
        worker_connections  1024;
    }


    http {
        include       /etc/nginx/mime.types;
        default_type  application/octet-stream;

        log_format  main  '$remote_addr - $remote_user [$time_local] &quot;$request&quot; '
                          '$status $body_bytes_sent &quot;$http_referer&quot; '
                          '&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;';

        access_log  /var/log/nginx/access.log  main;

        sendfile        on;
        #tcp_nopush     on;

        keepalive_timeout  65;

        #gzip  on;

        include /etc/nginx/conf.d/*.conf;
    }
</code></pre>
<p>Apply the ConfigMap.</p>
<pre><code>kubectl apply -f configmap.yaml
</code></pre>
<p>Create Pod <code>nginx-with-cm</code>.</p>
<pre><code>cat &gt; nginx-with-cm.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: nginx-with-cm
spec:
 containers:
 - name: nginx
   image: nginx
   volumeMounts:
   - name: foo
     mountPath: &quot;/etc/nginx/nginx.conf&quot;
     subPath:  nginx.conf
 volumes:
 - name: foo
   configMap:
     name: nginx
EOF

kubectl apply -f nginx-with-cm.yaml
</code></pre>
<p>Be noted:</p>
<ul>
<li>By default to mount ConfigMap, Kubernetes will overwrite all content of the mount point. We can use <code>volumeMounts.subPath</code> to specify that only overwrite the file <code>nginx.conf</code>.</li>
<li>Is we use <code>volumeMounts.subPath</code> to mount a Container Volume, Kubernetes won't do hot update to reflect real-time update.</li>
</ul>
<p>Verify if the <code>nginx.conf</code> mounted from outside is in the Container by comparing with above file.</p>
<pre><code>kubectl exec -it nginx-with-cm -- sh 
cat /etc/nginx/nginx.conf
</code></pre>
<h4 id="secret">Secret</h4>
<p>Encode password with base64  </p>
<pre><code>echo -n admin | base64  
YWRtaW4=

echo -n 123456 | base64
MTIzNDU2
</code></pre>
<p>Create Secret.</p>
<pre><code>cat &gt; secret.yaml &lt;&lt;EOF
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
data:
  username: YWRtaW4=
  password: MTIzNDU2
EOF

kubectl apply -f secret.yaml
</code></pre>
<p>Using Volume to mount (injection) Secret to a Pod.</p>
<pre><code>cat &gt; busybox-with-secret.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: busybox-with-secret
spec:
 containers:
 - name: mypod
   image: busybox
   args:
    - /bin/sh
    - -c
    - sleep 1000000;
   volumeMounts:
   - name: foo
     mountPath: &quot;/tmp/secret&quot;
 volumes:
 - name: foo
   secret:
    secretName: mysecret
EOF

kubectl apply -f busybox-with-secret.yaml
</code></pre>
<p>Let's attach to the Pod <code>busybox-with-secret</code> to verify if two data elements of <code>mysecret</code> are successfully mounted to the path <code>/tmp/secret</code> within the Pod.</p>
<pre><code>kubectl exec -it busybox-with-secret -- sh
</code></pre>
<p>By executing below command, we can see two data elements are in the directory <code>/tmp/secret</code> as file type. </p>
<pre><code>/ # ls -l /tmp/secret/
lrwxrwxrwx    1 root     root            15 Jul 12 16:13 password -&gt; ..data/password
lrwxrwxrwx    1 root     root            15 Jul 12 16:13 username -&gt; ..data/username
</code></pre>
<p>And we can get the content of each element, which are predefined before.</p>
<pre><code>/ # cat /tmp/secret/username
admin

/ # cat /tmp/secret/password
123456
</code></pre>
<h4 id="additional-keys">Additional Keys</h4>
<h5 id="various-way-to-create-configmap">Various way to create ConfigMap</h5>
<p>ConfigMap can be created by file, directory, or value. </p>
<p>Let's create a ConfigMap <code>colors</code> includes:</p>
<ul>
<li>Four files with four color names.</li>
<li>One file with favorite color name.</li>
</ul>
<pre><code>mkdir configmap
cd configmap
mkdir primary

echo c &gt; primary/cyan
echo m &gt; primary/magenta
echo y &gt; primary/yellow
echo k &gt; primary/black
echo &quot;known as key&quot; &gt;&gt; primary/black
echo blue &gt; favorite
</code></pre>
<p>Final structure looks like below via command <code>tree configmap</code>.</p>
<pre><code>configmap
├── favorite
└── primary
    ├── black
    ├── cyan
    ├── magenta
    └── yellow
</code></pre>
<p>Create ConfigMap referring above files we created. Make sure we are now in the path <code>~/configmap</code>.</p>
<pre><code>kubectl create configmap colors \
--from-literal=text=black  \
--from-file=./favorite  \
--from-file=./primary/
</code></pre>
<p>Check content of the ConfigMap <code>colors</code>.</p>
<pre><code>kubectl get configmap colors -o yaml
</code></pre>
<pre><code>apiVersion: v1
data:
  black: |
    k
    known as key
  cyan: |
    c
  favorite: |
    blue
  magenta: |
    m
  text: black
  yellow: |
    y
kind: ConfigMap
metadata:
  creationTimestamp: &quot;2022-07-12T16:38:27Z&quot;
  name: colors
  namespace: jh-namespace
  resourceVersion: &quot;2377258&quot;
  uid: d5ab133f-5e4d-41d4-bc9e-2bbb22a872a1
</code></pre>
<h5 id="set-environment-variable-via-configmap">Set environment variable via ConfigMap</h5>
<p>Here we will create a Pod <code>pod-configmap-env</code> and set the environment variable <code>ilike</code> and assign value of <code>favorite</code> from ConfigMap <code>colors</code>.</p>
<pre><code>cat &gt; pod-configmap-env.yaml &lt;&lt; EOF
apiVersion: v1
kind: Pod
metadata:
  name: pod-configmap-env
spec:
  containers:
  - name: nginx
    image: nginx
    env:
    - name: ilike
      valueFrom:
        configMapKeyRef:
          name: colors
          key: favorite
EOF

kubectl apply -f pod-configmap-env.yaml
</code></pre>
<p>Attach to the Pod <code>pod-configmap-env</code>.</p>
<pre><code>kubectl exec -it pod-configmap-env -- bash
</code></pre>
<p>Verify the value of env variable <code>ilike</code> is <code>blue</code>, which is the value of <code>favorite</code> of ConfigMap <code>colors</code>.</p>
<pre><code>root@pod-configmap-env:/# echo $ilike
blue
</code></pre>
<p>We can also use all key-value of ConfigMap to set up environment variables of Pod.</p>
<pre><code>cat &gt; pod-configmap-env-2.yaml &lt;&lt; EOF
apiVersion: v1
kind: Pod
metadata:
 name: pod-configmap-env-2
spec:
 containers:
 - name: nginx
   image: nginx
   envFrom:
    - configMapRef:
        name: colors
EOF

kubectl apply -f pod-configmap-env-2.yaml
</code></pre>
<p>Attach to the Pod <code>pod-configmap-env-2</code>.</p>
<pre><code>kubectl exec -it pod-configmap-env-2 -- bash
</code></pre>
<p>Verify the value of env variables based on key-values we defined in ConfigMap <code>colors</code>.</p>
<pre><code>root@pod-configmap-env-2:/# echo $black
k known as key
root@pod-configmap-env-2:/# echo $cyan
c
root@pod-configmap-env-2:/# echo $favorite
blue
</code></pre>
<h2 id="scheduling">Scheduling</h2>
<h3 id="nodeselector">nodeSelector</h3>
<p>Let's assume the scenario below.</p>
<ul>
<li>We have a group of high performance servers.</li>
<li>Some applications require high performance computing.</li>
<li>These applicaiton need to be scheduled and running on those high performance servers.</li>
</ul>
<p>We can leverage Kubernetes attributes node <code>label</code> and <code>nodeSelector</code> to group resources as a whole for scheduling to meet above requirement.</p>
<h4 id="label-node_1">Label Node</h4>
<p>Here I will label <code>cka002</code> with <code>Configuration=hight</code>.</p>
<pre><code>kubectl label node cka002 configuration=hight
</code></pre>
<p>Verify. We wil see the label <code>configuration=hight</code> on <code>cka002</code>.</p>
<pre><code>kubectl get node --show-labels
</code></pre>
<h4 id="configure-nodeselector-for-pod">Configure nodeSelector for Pod</h4>
<p>Create a Pod and use <code>nodeSelector</code> to schedule the Pod running on specified node.</p>
<pre><code>cat &gt; mysql-nodeselector.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
 name: mysql-nodeselector
spec:
 selector:
   matchLabels:
     app: mysql
 template:
   metadata:
     labels:
       app: mysql
   spec:
     containers:
     - image: mysql:8.0
       name: mysql
       env:
       - name: MYSQL_ROOT_PASSWORD
         value: password
       ports:
       - containerPort: 3306
         name: mysql
     nodeSelector:
       configuration: hight
EOF

kubectl apply -f mysql-nodeselector.yaml
</code></pre>
<p>Let's check with node the Pod <code>mysql-nodeselector</code> is running.</p>
<pre><code>kubectl get pod -l app=mysql -o wide |  grep mysql-nodeselector
</code></pre>
<p>With below result, Pod <code>mysql-nodeselector</code> is running on <code>cka002</code> node.</p>
<pre><code>NAME                                  READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
mysql-nodeselector-6b7d9c875d-227t6   1/1     Running   0          50s     10.244.1.26    cka002   &lt;none&gt;           &lt;none&gt;
</code></pre>
<h3 id="nodename">nodeName</h3>
<p>Be noted, <code>nodeName</code> has hightest priority as it's not scheduled by <code>Scheduler</code>.</p>
<p>Create a Pod <code>nginx-nodename</code> with <code>nodeName=cka003</code>.</p>
<pre><code>cat &gt; nginx-nodename.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: nginx-nodename
spec:
  containers:
  - name: nginx
    image: nginx
  nodeName: cka003
EOF

kubectl apply -f nginx-nodename.yaml
</code></pre>
<p>Verify if Pod <code>nginx-nodename</code> is running on `ckc003 node.</p>
<pre><code>kubectl get pod -owide |grep nginx-nodename
</code></pre>
<pre><code>NAME                                      READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
nginx-nodename                            1/1     Running   0          6s      10.244.2.113   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<h3 id="affinity">Affinity</h3>
<p>In Kubernetes cluster, some Pods have frequent interaction with other Pods. With that situation, it's suggested to schedule these Pods running on same node. 
For example, Two Pods Nginx and Mysql, we need deploy them on one node if they frequently communicate.</p>
<p>We can use <code>podAffinity</code> to select Pods based on their relationship. </p>
<p>There are two scheduling type of <code>podAffinity</code>.</p>
<ul>
<li><code>requiredDuringSchedulingIgnoredDuringExecution</code>(硬亲和)</li>
<li><code>preferredDuringSchedulingIgnoredDuringExecution</code>(软亲和)</li>
</ul>
<p><code>topologyKey</code> could be set by below types:</p>
<ul>
<li><code>kubernetes.io/hostname</code> ＃NodeName</li>
<li><code>failure-domain.beta.kubernetes.io/zone</code> ＃Zone </li>
<li><code>failure-domain.beta.kubernetes.io/region</code> # Region </li>
</ul>
<p>We can set node Label to classify Name/Zone/Region of node, which can be used by <code>podAffinity</code>.</p>
<p>Create a Pod Nginx.</p>
<pre><code>cat &gt; pod_nginx.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx
EOF


kubectl apply -f pod_nginx.yaml
</code></pre>
<p>Create a Pod MySql.</p>
<pre><code>cat &gt; pod_mysql.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  containers:
  - name: mysql
    image: mysql
    env:
     - name: &quot;MYSQL_ROOT_PASSWORD&quot;
       value: &quot;123456&quot;
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - nginx
        topologyKey: kubernetes.io/hostname
EOF

kubectl apply -f pod_mysql.yaml
</code></pre>
<p>As we configured <code>podAffinity</code>, so Pod <code>mysql</code> will be scheduled to the same node with Pod <code>nginx</code> by Label <code>app:nginx</code>.</p>
<p>Via the command <code>kubectl get pod -o wide</code> we can get two Pods are running on node <code>cka002</code>.</p>
<pre><code>NAME                                      READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
mysql                                     1/1     Running   0          2m48s   10.244.1.28    cka002   &lt;none&gt;           &lt;none&gt;
nginx                                     1/1     Running   0          9m53s   10.244.1.27    cka002   &lt;none&gt;           &lt;none&gt;
</code></pre>
<h3 id="taints-tolerations">Taints &amp; Tolerations</h3>
<h4 id="concept">Concept</h4>
<p>Node affinity is a property of Pods that attracts them to a set of nodes (either as a preference or a hard requirement). 
Taints are the opposite -- they allow a node to repel a set of pods.</p>
<p>Tolerations are applied to pods. 
Tolerations allow the scheduler to schedule pods with matching taints. 
Tolerations allow scheduling but don't guarantee scheduling: the scheduler also evaluates other parameters as part of its function.</p>
<p>Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. 
One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints.</p>
<p>In the production environment, we generally configure Taints for Control Plane nodes (in fact, most K8s installation tools automatically add Taints to Control Plane nodes), because Control Plane only runs Kubernetes system components. 
If it is used to run application Pods, it is easy to consume resources. In the end, the Control Plane node will crash. 
If we need to add additional system components to the Control Plane node later, we can configure Tolerations for the corresponding system component Pod to tolerate taints.</p>
<h4 id="set-taints">Set Taints</h4>
<p>Set <code>cka003</code> node to taint node. Set status to <code>NoSchedule</code>, which won't impact existing Pods running on <code>cka003</code>.</p>
<pre><code>kubectl taint nodes cka003 key=value:NoSchedule
</code></pre>
<h4 id="set-tolerations">Set Tolerations</h4>
<p>We can use Tolerations to let Pods schedule to a taint node. </p>
<pre><code>cat &gt; mysql-tolerations.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
 name: mysql-tolerations
spec:
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - image: mysql:8.0
        name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: password
        ports:
        - containerPort: 3306
          name: mysql
      tolerations:
      - key: &quot;key&quot;
        operator: &quot;Equal&quot;
        value: &quot;value&quot;
        effect: &quot;NoSchedule&quot;
EOF

kubectl apply -f mysql-tolerations.yaml
</code></pre>
<p>The Pod of Deployment <code>mysql-tolerations</code> is scheduled and running on node <code>cka003</code> with <code>tolerations</code> setting, which is a taint node.</p>
<pre><code>kubectl get pod -o wide | grep mysql-tolerations
</code></pre>
<pre><code>NAME                                      READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
mysql-tolerations-5c5986944b-cg9bs        1/1     Running   0          57s     10.244.2.114   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<h4 id="remove-taints">Remove Taints</h4>
<pre><code>kubectl taint nodes cka003 key-
</code></pre>
<h2 id="resourcequota">ResourceQuota</h2>
<h3 id="create-namespace">Create Namespace</h3>
<p>Ceate a Namespace</p>
<pre><code>kubectl create ns quota-object-example
</code></pre>
<h3 id="create-resourcequota">Create ResourceQuota</h3>
<p>Create a Namespace ResourceQuota and apply to namespace <code>quota-object-example</code>.
Within the namespace, we can only create 1 PVC, 1 LoadBalancer Service, can not create NodePort Service.</p>
<pre><code>cat &gt; resourcequota.yaml &lt;&lt;EOF
apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-quota-demo
  namespace: quota-object-example
spec:
  hard:
    persistentvolumeclaims: &quot;1&quot;
    services.loadbalancers: &quot;2&quot;
    services.nodeports: &quot;0&quot;
EOF


kubectl apply -f resourcequota.yaml
</code></pre>
<h3 id="verify-test">Verify &amp; Test</h3>
<p>Check Quota status</p>
<pre><code>kubectl get resourcequota object-quota-demo --namespace=quota-object-example --output=yaml
</code></pre>
<p>Key information is below. </p>
<pre><code>spec:
  hard:
    persistentvolumeclaims: &quot;1&quot;
    services.loadbalancers: &quot;2&quot;
    services.nodeports: &quot;0&quot;
status:
  hard:
    persistentvolumeclaims: &quot;1&quot;
    services.loadbalancers: &quot;2&quot;
    services.nodeports: &quot;0&quot;
  used:
    persistentvolumeclaims: &quot;0&quot;
    services.loadbalancers: &quot;0&quot;
    services.nodeports: &quot;0&quot;
</code></pre>
<h4 id="test-nodeport">Test NodePort</h4>
<p>Create a Deployment <code>ns-quota-test</code> on namespace <code>quota-object-example</code>.</p>
<pre><code>kubectl create deployment ns-quota-test --image nginx --namespace=quota-object-example
</code></pre>
<p>Expose the Deployment via NodePort    </p>
<pre><code>kubectl expose deployment ns-quota-test --port=80 --type=NodePort --namespace=quota-object-example
</code></pre>
<p>We receive below error, which is expected because we set Quota <code>services.nodeports: 0</code>.</p>
<pre><code class="language-shell">Error from server (Forbidden): services &quot;ns-quota-test&quot; is forbidden: exceeded quota: object-quota-demo, requested: services.nodeports=1, used: services.nodeports=0, limited: services.nodeports=0
</code></pre>
<h4 id="test-pvc">Test PVC</h4>
<p>Create a PVC <code>pvc-quota-demo</code> on namespace <code>quota-object-example</code>.</p>
<pre><code>cat &gt; test-pvc-quota.yaml &lt;&lt; EOF
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc-quota-demo
  namespace: quota-object-example
spec:
  storageClassName: nfs-client
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
EOF


kubectl apply -f test-pvc-quota.yaml
</code></pre>
<p>Check the Quota status.</p>
<pre><code>kubectl get resourcequota object-quota-demo --namespace=quota-object-example --output=yaml
</code></pre>
<p>Here <code>persistentvolumeclaims</code> is used <code>1</code>, and the quota is also <code>1</code>. If we create PVC again, will receive 403 error.</p>
<pre><code>spec:
  hard:
    persistentvolumeclaims: &quot;1&quot;
    services.loadbalancers: &quot;2&quot;
    services.nodeports: &quot;0&quot;
status:
  hard:
    persistentvolumeclaims: &quot;1&quot;
    services.loadbalancers: &quot;2&quot;
    services.nodeports: &quot;0&quot;
  used:
    persistentvolumeclaims: &quot;1&quot;
    services.loadbalancers: &quot;0&quot;
    services.nodeports: &quot;0&quot;
</code></pre>
<h2 id="limitrange">LimitRange</h2>
<p>A <em>LimitRange</em> provides constraints that can:</p>
<ul>
<li>Enforce minimum and maximum compute resources usage per Pod or Container in a namespace.</li>
<li>Enforce minimum and maximum storage request per PersistentVolumeClaim in a namespace.</li>
<li>Enforce a ratio between request and limit for a resource in a namespace.</li>
<li>Set default request/limit for compute resources in a namespace and automatically inject them to Containers at runtime.</li>
</ul>
<h3 id="create-namespace_1">Create Namespace</h3>
<p>Create a Namespace <code>default-cpu-example</code> for demo.</p>
<pre><code>kubectl create namespace default-cpu-example
</code></pre>
<h3 id="set-limitrange">Set LimitRange</h3>
<p>Create LimitRange by below yaml file to define range of CPU Request and CPU Limit for a Container.
After apply LimitRange resource, the CPU limitation will affect all new created Pods.</p>
<pre><code>cat &gt; cpu-limitrange.yaml &lt;&lt; EOF
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
  namespace: default-cpu-example
spec:
  limits:
  - default:
      cpu: 1
    defaultRequest:
      cpu: 0.5
    type: Container
EOF


kubectl apply -f cpu-limitrange.yaml
</code></pre>
<h3 id="test-via-pod">Test via Pod</h3>
<h4 id="scenario-1-pod-without-specified-limits">Scenario 1: Pod without specified limits</h4>
<p>Create a Pod without any specified limits.</p>
<pre><code>cat &gt; default-cpu-demo-pod.yaml &lt;&lt; EOF
apiVersion: v1
kind: Pod
metadata:
  name: default-cpu-demo
  namespace: default-cpu-example
spec:
  containers:
  - name: default-cpu-demo-ctr
    image: nginx
EOF


kubectl apply -f default-cpu-demo-pod.yaml
</code></pre>
<p>Verify details of the Pod we created. The Pod inherits the both CPU Limits and CPU Requests from namespace as its default.</p>
<pre><code>kubectl get pod default-cpu-demo --output=yaml --namespace=default-cpu-example
</code></pre>
<pre><code>spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: default-cpu-demo-ctr
    resources:
      limits:
        cpu: &quot;1&quot;
      requests:
        cpu: 500m
</code></pre>
<h4 id="scenario-2-pod-with-cpu-limit-without-cpu-request">Scenario 2: Pod with CPU limit, without CPU Request</h4>
<p>Create Pod with specified CPU limits only.  </p>
<pre><code>cat &gt; default-cpu-demo-limit.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: default-cpu-demo-limit
  namespace: default-cpu-example
spec:
  containers:
  - name: default-cpu-demo-limit-ctr
    image: nginx
    resources:
      limits:
        cpu: &quot;1&quot;
EOF

kubectl apply -f default-cpu-demo-limit.yaml
</code></pre>
<p>Verify details of the Pod we created. The Pod inherits the CPU Request from namespace as its default and specifies own CPU Limits.</p>
<pre><code>kubectl get pod default-cpu-demo-limit --output=yaml --namespace=default-cpu-example
</code></pre>
<pre><code>spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: default-cpu-demo-limit-ctr
    resources:
      limits:
        cpu: &quot;1&quot;
      requests:
        cpu: &quot;1&quot;
</code></pre>
<h4 id="scenario-3-pod-with-cpu-request-onlyl-without-cpu-limits">Scenario 3: Pod with CPU Request onlyl, without CPU Limits</h4>
<p>Create Pod with specified CPU Request only. </p>
<pre><code>cat &gt; default-cpu-demo-request.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: default-cpu-demo-request
  namespace: default-cpu-example
spec:
  containers:
  - name: default-cpu-demo-request-ctr
    image: nginx
    resources:
      requests:
        cpu: &quot;0.75&quot;
EOF

kubectl apply -f default-cpu-demo-request.yaml
</code></pre>
<p>Verify details of the Pod we created. The Pod inherits the CPU Limits from namespace as its default and specifies own CPU Requests.</p>
<pre><code>kubectl get pod default-cpu-demo-request --output=yaml --namespace=default-cpu-example
</code></pre>
<pre><code>spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: default-cpu-demo-request-ctr
    resources:
      limits:
        cpu: &quot;1&quot;
      requests:
        cpu: 750m
</code></pre>
<h2 id="troubleshooting_1">Troubleshooting</h2>
<h3 id="event_1">Event</h3>
<p>Usage:</p>
<pre><code>kubectl describe &lt;resource_type&gt; &lt;resource_name&gt; --namespace=&lt;namespace_name&gt;
</code></pre>
<p>Get event information of a Pod</p>
<p>Create a Tomcat Pod.</p>
<pre><code>kubectl run tomcat --image=tomcat
</code></pre>
<p>Check event of above deplyment.</p>
<pre><code>kubectl describe pod/tomcat
</code></pre>
<p>Get below event information.</p>
<pre><code>Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  55s   default-scheduler  Successfully assigned jh-namespace/tomcat to cka002
  Normal  Pulling    54s   kubelet            Pulling image &quot;tomcat&quot;
  Normal  Pulled     21s   kubelet            Successfully pulled image &quot;tomcat&quot; in 33.134162692s
  Normal  Created    19s   kubelet            Created container tomcat
  Normal  Started    19s   kubelet            Started container tomcat
</code></pre>
<p>Get event information for a Namespace.</p>
<pre><code>kubectl get events -n &lt;your_namespace_name&gt;
</code></pre>
<p>Get current default namespace event information.</p>
<pre><code>LAST SEEN   TYPE      REASON           OBJECT                          MESSAGE
70s         Warning   FailedGetScale   horizontalpodautoscaler/nginx   deployments/scale.apps &quot;podinfo&quot; not found
2m16s       Normal    Scheduled        pod/tomcat                      Successfully assigned jh-namespace/tomcat to cka002
2m15s       Normal    Pulling          pod/tomcat                      Pulling image &quot;tomcat&quot;
102s        Normal    Pulled           pod/tomcat                      Successfully pulled image &quot;tomcat&quot; in 33.134162692s
100s        Normal    Created          pod/tomcat                      Created container tomcat
100s        Normal    Started          pod/tomcat                      Started container tomcat
</code></pre>
<p>Get event information for all Namespace.</p>
<pre><code>kubectl get events -A
</code></pre>
<h3 id="logs">Logs</h3>
<p>Usage:</p>
<pre><code>kubectl logs &lt;pod_name&gt; -n &lt;namespace_name&gt;
</code></pre>
<p>Options:</p>
<ul>
<li><code>--tail &lt;n&gt;</code>: display only the most recent <code>&lt;n&gt;</code> lines of output</li>
<li><code>-f</code>: streaming the output</li>
</ul>
<p>Get the most recent 100 lines of output.</p>
<pre><code>kubectl logs -f tomcat --tail 100
</code></pre>
<p>If it's multipPod, use <code>-c</code> to specify Container.</p>
<pre><code>kubectl logs -f tomcat --tail 100 -c tomcat
</code></pre>
<h3 id="monitoring-indicators">Monitoring Indicators</h3>
<h4 id="nodes">Nodes</h4>
<p>Get node monitoring information</p>
<pre><code>kubectl top node
</code></pre>
<p>Output:</p>
<pre><code>NAME     CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
cka001   147m         7%     1940Mi          50%
cka002   62m          3%     2151Mi          56%
cka003   63m          3%     1825Mi          47%
</code></pre>
<p>Get Pod monitoring information</p>
<pre><code>kubectl top pod
</code></pre>
<p>Output:</p>
<pre><code>root@cka001:~# kubectl top pod
NAME                                      CPU(cores)   MEMORY(bytes)   
busybox-with-secret                       0m           0Mi
mysql                                     2m           366Mi
mysql-774db46945-sztrp                    2m           349Mi
mysql-nodeselector-6b7d9c875d-227t6       2m           365Mi
mysql-tolerations-5c5986944b-cg9bs        2m           366Mi
mysql-with-sc-pvc-7c97d875f8-dwfkc        2m           349Mi
nfs-client-provisioner-699db7fd58-bccqs   2m           7Mi
nginx                                     0m           3Mi
nginx-app-1-695b7b647d-l76bh              0m           3Mi
nginx-app-2-7f6bf6f4d4-lvbz8              0m           3Mi
nginx-nodename                            0m           3Mi
nginx-with-cm                             0m           3Mi
pod-configmap-env                         0m           3Mi
pod-configmap-env-2                       0m           3Mi
tomcat                                    1m           58Mi
</code></pre>
<p>Sort output by CPU or Memory using option <code>--sort-by</code>.</p>
<pre><code>kubectl top pod --sort-by=cpu
</code></pre>
<p>Output:</p>
<pre><code>NAME                                      CPU(cores)   MEMORY(bytes)   
nfs-client-provisioner-699db7fd58-bccqs   2m           7Mi
mysql                                     2m           366Mi
mysql-774db46945-sztrp                    2m           349Mi
mysql-nodeselector-6b7d9c875d-227t6       2m           365Mi
mysql-tolerations-5c5986944b-cg9bs        2m           366Mi
mysql-with-sc-pvc-7c97d875f8-dwfkc        2m           349Mi
tomcat                                    1m           58Mi
nginx                                     0m           3Mi
nginx-app-1-695b7b647d-l76bh              0m           3Mi
nginx-app-2-7f6bf6f4d4-lvbz8              0m           3Mi
nginx-nodename                            0m           3Mi
nginx-with-cm                             0m           3Mi
pod-configmap-env                         0m           3Mi
pod-configmap-env-2                       0m           3Mi
busybox-with-secret                       0m           0Mi
</code></pre>
<h3 id="node-eviction">Node Eviction</h3>
<p>Disable scheduling for a Node.</p>
<pre><code>kubectl cordon &lt;node_name&gt;
</code></pre>
<p>Example:</p>
<pre><code>kubectl cordon cka003
</code></pre>
<p>Node status:</p>
<pre><code>NAME     STATUS                     ROLES                  AGE   VERSION
cka001   Ready                      control-plane,master   18d   v1.23.8
cka002   Ready                      &lt;none&gt;                 18d   v1.23.8
cka003   Ready,SchedulingDisabled   &lt;none&gt;                 18d   v1.23.8
</code></pre>
<p>Enable scheduling for a Node.</p>
<pre><code>kubectl uncordon &lt;node_name&gt;
</code></pre>
<p>Example:</p>
<pre><code>kubectl uncordon cka003
</code></pre>
<p>Node status:</p>
<pre><code>NAME     STATUS   ROLES                  AGE   VERSION
cka001   Ready    control-plane,master   18d   v1.23.8
cka002   Ready    &lt;none&gt;                 18d   v1.23.8
cka003   Ready    &lt;none&gt;                 18d   v1.23.8
</code></pre>
<p>Evict Pods on a Node.</p>
<pre><code>kubectl drain &lt;node_name&gt;
kubectl drain &lt;node_name&gt; --ignore-daemonsets
kubectl drain &lt;node_name&gt; --ignore-daemonsets --delete-emptydir-data
</code></pre>
<h2 id="rbac">RBAC</h2>
<p>Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within the organization.</p>
<h3 id="install-cfssl">Install cfssl</h3>
<p>Install <code>cfssl</code> tool</p>
<pre><code>apt install golang-cfssl
</code></pre>
<h3 id="create-user">Create user</h3>
<h4 id="create-file-ca-configjson">Create file <code>ca-config.json</code></h4>
<p>Change to directory <code>/etc/kubernetes/pki</code>.</p>
<pre><code>cd /etc/kubernetes/pki
</code></pre>
<p>Check if file <code>ca-config.json</code> is in place in current directory.</p>
<pre><code>ll ca-config.json
</code></pre>
<p>If not, create it.</p>
<ul>
<li>We can add multiple profiles to specify different expiry date, scenario, parameters, etc.. </li>
<li>Profile will be used to sign certificate.</li>
</ul>
<pre><code>cat &gt; ca-config.json &lt;&lt;EOF
{
  &quot;signing&quot;: {
    &quot;default&quot;: {
      &quot;expiry&quot;: &quot;87600h&quot;
    },
    &quot;profiles&quot;: {
      &quot;kubernetes&quot;: {
        &quot;usages&quot;: [
            &quot;signing&quot;,
            &quot;key encipherment&quot;,
            &quot;server auth&quot;,
            &quot;client auth&quot;
        ],
        &quot;expiry&quot;: &quot;87600h&quot;
      }
    }
  }
}
EOF
</code></pre>
<h4 id="create-csr-file-for-signature">Create csr file for signature</h4>
<p>Stay in the directory <code>/etc/kubernetes/pki</code>.</p>
<p>Create csr file.</p>
<pre><code>cat &gt; test-cka-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;test-cka&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ]
}
EOF
</code></pre>
<p>Generate certifcate and key.</p>
<pre><code>cfssl gencert -ca=ca.crt -ca-key=ca.key -config=ca-config.json -profile=kubernetes test-cka-csr.json | cfssljson -bare test-cka
</code></pre>
<p>Get below files.</p>
<pre><code>ll -tr | grep test-cka
</code></pre>
<pre><code>-rw-r--r-- 1 root root  997 Jul 13 20:11 test-cka.csr
-rw-r--r-- 1 root root  221 Jul 13 20:09 test-cka-csr.json
-rw------- 1 root root 1675 Jul 13 20:11 test-cka-key.pem
-rw-r--r-- 1 root root 1281 Jul 13 20:11 test-cka.pem
</code></pre>
<h4 id="create-file-kubeconfig">Create file kubeconfig</h4>
<p>Export env <code>KUBE_APISERVER</code>. Put master node IP <code>172.16.18.161</code> here.</p>
<pre><code>export KUBE_APISERVER=&quot;https://172.16.18.161:6443&quot;
</code></pre>
<pre><code>NAME     STATUS   ROLES                  AGE   VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
cka001   Ready    control-plane,master   18d   v1.23.8   172.16.18.161   &lt;none&gt;        Ubuntu 20.04.4 LTS   5.4.0-113-generic   containerd://1.5.9
cka002   Ready    &lt;none&gt;                 18d   v1.23.8   172.16.18.160   &lt;none&gt;        Ubuntu 20.04.4 LTS   5.4.0-113-generic   containerd://1.5.9
cka003   Ready    &lt;none&gt;                 18d   v1.23.8   172.16.18.159   &lt;none&gt;        Ubuntu 20.04.4 LTS   5.4.0-113-generic   containerd://1.5.9
</code></pre>
<p>Verify the setting.</p>
<pre><code>echo $KUBE_APISERVER
</code></pre>
<p>Output:</p>
<pre><code>https://172.16.18.161:6443
</code></pre>
<h5 id="set-up-cluster">Set up cluster</h5>
<p>Stay in the directory <code>/etc/kubernetes/pki</code>.</p>
<p>Generate kubeconfig file.</p>
<pre><code>kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true --server=${KUBE_APISERVER} --kubeconfig=test-cka.kubeconfig
</code></pre>
<p>Output:</p>
<pre><code>Cluster &quot;kubernetes&quot; set.
</code></pre>
<p>Now we get the new config file <code>test-cka.kubeconfig</code></p>
<pre><code>ll -tr | grep test-cka
</code></pre>
<p>Output:</p>
<pre><code>-rw-r--r-- 1 root root  221 Jul 13 20:09 test-cka-csr.json
-rw-r--r-- 1 root root 1281 Jul 13 20:11 test-cka.pem
-rw------- 1 root root 1675 Jul 13 20:11 test-cka-key.pem
-rw-r--r-- 1 root root  997 Jul 13 20:11 test-cka.csr
-rw------- 1 root root 1671 Jul 13 20:21 test-cka.kubeconfig
</code></pre>
<p>Get content of file <code>test-cka.kubeconfig</code>.</p>
<pre><code>cat test-cka.kubeconfig
</code></pre>
<pre><code>apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJT......==
    server: https://172.16.18.161:6443
  name: kubernetes
contexts: null
current-context: &quot;&quot;
kind: Config
preferences: {}
users: null
</code></pre>
<h5 id="set-up-user">Set up user</h5>
<p>In file <code>test-cka.kubeconfig</code>, user info is null. </p>
<p>Set up user.</p>
<pre><code>kubectl config set-credentials test-cka --client-certificate=/etc/kubernetes/pki/test-cka.pem --client-key=/etc/kubernetes/pki/test-cka-key.pem --embed-certs=true --kubeconfig=test-cka.kubeconfig
</code></pre>
<p>Output</p>
<pre><code>User &quot;test-cka&quot; set.
</code></pre>
<p>Now file <code>test-cka.kubeconfig</code> was updated and user information was added.</p>
<pre><code>cat test-cka.kubeconfig
</code></pre>
<pre><code>apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0t......Cg==
    server: https://172.16.18.161:6443
  name: kubernetes
contexts: null
current-context: &quot;&quot;
kind: Config
preferences: {}
users:
- name: test-cka
  user:
    client-certificate-data: LS0t...S0K
    client-key-data: LS0t......Cg==
</code></pre>
<p>Now we have a complete kubeconfig file.
When we use it to get node information, receive error below because we did not set up current-context in kubeconfig file.</p>
<pre><code>kubectl --kubeconfig test-cka.kubeconfig get nodes
</code></pre>
<pre><code>The connection to the server localhost:8080 was refused - did you specify the right host or port?
</code></pre>
<p>Current contents is empty.</p>
<pre><code>kubectl --kubeconfig test-cka.kubeconfig config get-contexts
</code></pre>
<pre><code>CURRENT   NAME   CLUSTER   AUTHINFO   NAMESPACE
</code></pre>
<h5 id="set-up-context">Set up Context</h5>
<p>Set up context. </p>
<pre><code>kubectl config set-context kubernetes --cluster=kubernetes --user=test-cka  --kubeconfig=test-cka.kubeconfig
</code></pre>
<p>Output:</p>
<pre><code>Context &quot;kubernetes&quot; created.
</code></pre>
<p>Now we have context now but the <code>CURRENT</code> flag is empty.</p>
<pre><code>kubectl --kubeconfig test-cka.kubeconfig config get-contexts
</code></pre>
<p>Output:</p>
<pre><code>CURRENT   NAME         CLUSTER      AUTHINFO   NAMESPACE
          kubernetes   kubernetes   test-cka
</code></pre>
<p>Set up default context. The context will link clusters and users for multiple clusters environment and we can switch to different cluster.</p>
<pre><code>kubectl config use-context kubernetes --kubeconfig=test-cka.kubeconfig 
</code></pre>
<pre><code>Switched to context &quot;kubernetes&quot;.
</code></pre>
<h5 id="verify_1">Verify</h5>
<p>Now <code>CURRENT</code> is marked with <code>*</code>, that is, current-context is set up.</p>
<pre><code>kubectl --kubeconfig=/etc/kubernetes/pki/test-cka.kubeconfig config get-contexts
</code></pre>
<pre><code>CURRENT   NAME         CLUSTER      AUTHINFO   NAMESPACE
*         kubernetes   kubernetes   test-cka    
</code></pre>
<p>When we get Pods infor we get error because user <code>test-cka</code> does not have authorization in the cluster.</p>
<pre><code>kubectl --kubeconfig=test-cka.kubeconfig get pod 
</code></pre>
<pre><code>Error from server (Forbidden): pods is forbidden: User &quot;test-cka&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;default&quot;
</code></pre>
<pre><code>kubectl --kubeconfig=test-cka.kubeconfig get node
</code></pre>
<pre><code>Error from server (Forbidden): nodes is forbidden: User &quot;test-cka&quot; cannot list resource &quot;nodes&quot; in API group &quot;&quot; at the cluster scope
</code></pre>
<h3 id="role-rolebinding">Role &amp; RoleBinding</h3>
<p>Back to home directory.</p>
<pre><code>cd ~
</code></pre>
<h4 id="create-role-and-rolebinding">Create Role and RoleBinding</h4>
<p>Create a role <code>pod-reader</code> with only <code>get</code>,<code>watch</code>,<code>list</code> authorization for Pod resource in <code>default</code> namespace.</p>
<pre><code>cat &gt; role.yaml &lt;&lt;EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [&quot;&quot;] # Empty &quot;&quot; means core API group
  resources: [&quot;pods&quot;]
  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]
EOF


kubectl apply -f role.yaml
</code></pre>
<p>Bind the role <code>pod-reader</code> to user <code>test-cka</code>.</p>
<pre><code>cat &gt; rolebinding.yaml &lt;&lt; EOF
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: test-cka
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
EOF


kubectl apply -f rolebinding.yaml
</code></pre>
<h4 id="verify-authorization">Verify Authorization</h4>
<h5 id="check-pod-status">Check Pod Status</h5>
<p>Get Pods status via user <code>test-cka</code>.</p>
<pre><code>kubectl --kubeconfig /etc/kubernetes/pki/test-cka.kubeconfig get pod
</code></pre>
<pre><code>No resources found in default namespace.
</code></pre>
<h5 id="check-node-status">Check Node Status</h5>
<p>We receive error to get node status because the role we defined is only for Pod resource.</p>
<pre><code>kubectl --kubeconfig /etc/kubernetes/pki/test-cka.kubeconfig get node
</code></pre>
<pre><code class="language-shell">Error from server (Forbidden): nodes is forbidden: User &quot;test-cka&quot; cannot list resource &quot;nodes&quot; in API group &quot;&quot; at the cluster scope
</code></pre>
<h5 id="delete-pod">Delete Pod</h5>
<p>We receive error when try to delete a Pod because we only have <code>get</code>,<code>watch</code>,<code>list</code> for Pod, no <code>delete</code> authorization.</p>
<pre><code>kubectl --kubeconfig /etc/kubernetes/pki/test-cka.kubeconfig delete pod nslookup
</code></pre>
<pre><code>Error from server (Forbidden): pods &quot;nslookup&quot; is forbidden: User &quot;test-cka&quot; cannot delete resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;default&quot;
</code></pre>
<h5 id="check-pods-in-other-namespace">Check Pods in other Namespace</h5>
<p>We receive error to get Pods status in other namespace because the role we defined is only for <code>default</code> namespace.</p>
<pre><code>kubectl --kubeconfig /etc/kubernetes/pki/test-cka.kubeconfig get pod -n kube-system
</code></pre>
<pre><code>Error from server (Forbidden): pods is forbidden: User &quot;test-cka&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;kube-system&quot;
</code></pre>
<h3 id="clusterrole-clusterrolebinding">ClusterRole &amp; ClusterRoleBinding</h3>
<h4 id="create-clusterrole-and-clusterrolebinding">Create ClusterRole and ClusterRoleBinding</h4>
<p>Create a ClusterRole with authorization <code>get</code>,<code>watch</code>,<code>list</code> for <code>nodes</code> resource.</p>
<pre><code>cat &gt; clusterrole.yaml &lt;&lt;EOF
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nodes-reader
rules:
- apiGroups: [&quot;&quot;]
  resources: [&quot;nodes&quot;]
  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]
EOF


kubectl apply -f clusterrole.yaml
</code></pre>
<p>Bind ClusterRole <code>nodes-reader</code> to user <code>test-cka</code>.</p>
<pre><code>cat &gt; clusterrolebinding.yaml &lt;&lt; EOF
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-nodes-global
subjects:
- kind: User
  name: test-cka
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: nodes-reader
  apiGroup: rbac.authorization.k8s.io
EOF


kubectl apply -f clusterrolebinding.yaml
</code></pre>
<h4 id="verify-authorization_1">Verify Authorization</h4>
<p>Try to get node information, no error received.</p>
<pre><code>kubectl --kubeconfig /etc/kubernetes/pki/test-cka.kubeconfig get node
</code></pre>
<pre><code>NAME     STATUS   ROLES                  AGE   VERSION
cka001   Ready    control-plane,master   18d   v1.23.8
cka002   Ready    &lt;none&gt;                 18d   v1.23.8
cka003   Ready    &lt;none&gt;                 18d   v1.23.8
</code></pre>
<h2 id="network-policy">Network Policy</h2>
<p>Delete Flannel</p>
<pre><code>kubectl delete -f https://raw.githubusercontent.com/coreos/flannel/v0.18.1/Documentation/kube-flannel.yml
</code></pre>
<p>or</p>
<pre><code>kubectl delete -f kube-flannel.yml
</code></pre>
<p>Output:</p>
<pre><code>Warning: policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
podsecuritypolicy.policy &quot;psp.flannel.unprivileged&quot; deleted
clusterrole.rbac.authorization.k8s.io &quot;flannel&quot; deleted
clusterrolebinding.rbac.authorization.k8s.io &quot;flannel&quot; deleted
serviceaccount &quot;flannel&quot; deleted
configmap &quot;kube-flannel-cfg&quot; deleted
daemonset.apps &quot;kube-flannel-ds&quot; deleted
</code></pre>
<p>Clean up iptables for all nodes.</p>
<pre><code>rm -rf /var/run/flannel /opt/cni /etc/cni /var/lib/cni
iptables -F &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t mangle -F &amp;&amp; iptables -X
</code></pre>
<p>Log out and log on to host (e.g., cka001) again. Install Calico.</p>
<pre><code>curl https://docs.projectcalico.org/manifests/calico.yaml -O

kubectl apply -f calico.yaml
</code></pre>
<p>Output:</p>
<pre><code>configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
</code></pre>
<p>Verify status of Calico. </p>
<pre><code>kubectl get pod -n kube-system | grep calico
</code></pre>
<p>Output. Make sure all Pods are running</p>
<pre><code>NAME                                       READY   STATUS        RESTARTS   AGE
calico-kube-controllers-7bc6547ffb-tjfcg   1/1     Running       0          30m
calico-node-7x8jm                          1/1     Running       0          30m
calico-node-cwxj5                          1/1     Running       0          30m
calico-node-rq978                          1/1     Running       0          30m
</code></pre>
<p>If facing any error, check log in the Container.</p>
<pre><code># Get Container ID
crictl ps

# Get log info
crictl logs &lt;your_container_id&gt;
</code></pre>
<p>As we change CNI from Flannel to Calico, we need delete all Pods. All of Pods will be created automatically again. </p>
<pre><code>kubectl delete pod -A --all
</code></pre>
<p>Make sure all Pods are up and running successfully.</p>
<pre><code>kubectl get pod -A
</code></pre>
<h3 id="inbound-rules">Inbound Rules</h3>
<h4 id="create-workload-for-test">Create workload for test.</h4>
<p>Create three Deployments <code>pod-netpol-1</code>,<code>pod-netpol-2</code>,<code>pod-netpol-3</code>.</p>
<pre><code>cat &gt; pod-netpol.yaml &lt;&lt; EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: pod-netpol-1
  name: pod-netpol-1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pod-netpol-1
  template:
    metadata:
      labels:
        app: pod-netpol-1
    spec:
      containers:
      - image: busybox
        name: busybox
        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;sleep 1h&quot;]

---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: pod-netpol-2
  name: pod-netpol-2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pod-netpol-2
  template:
    metadata:
      labels:
        app: pod-netpol-2
    spec:
      containers:
      - image: busybox
        name: busybox
        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;sleep 1h&quot;]

---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: pod-netpol-3
  name: pod-netpol-3
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pod-netpol-3
  template:
    metadata:
      labels:
        app: pod-netpol-3
    spec:
      containers:
      - image: busybox
        name: busybox
        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;sleep 1h&quot;]       
EOF

kubectl apply -f pod-netpol.yaml
</code></pre>
<p>Check Pods IP.</p>
<pre><code>kubectl get pod -owide
</code></pre>
<p>Output:</p>
<pre><code>NAME                                      READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
pod-netpol-1-6494f6bf8b-6nwwf             1/1     Running   0          19s   10.244.102.9   cka003   &lt;none&gt;           &lt;none&gt;
pod-netpol-2-77478d77ff-96hgd             1/1     Running   0          19s   10.244.112.9   cka002   &lt;none&gt;           &lt;none&gt;
pod-netpol-3-68977dcb48-j9fkb             1/1     Running   0          19s   10.244.102.8   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Attach to Pod <code>pod-netpol-1</code></p>
<pre><code>kubectl exec -it pod-netpol-1-6494f6bf8b-6nwwf -- sh
</code></pre>
<p>Execute command <code>ping</code> to check if pod-netpol-2 and pod-netpol-3 are pingable. </p>
<pre><code>/ # ping 10.244.112.9
3 packets transmitted, 3 packets received, 0% packet loss

/ # ping 10.244.102.8
3 packets transmitted, 3 packets received, 0% packet loss
</code></pre>
<h4 id="deny-for-all-ingress">Deny For All Ingress</h4>
<p>Create deny policy for all ingress.</p>
<pre><code>cat &gt; networkpolicy-default-deny-ingress.yaml &lt;&lt; EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Ingress
EOF


kubectl apply -f networkpolicy-default-deny-ingress.yaml
</code></pre>
<p>Attach to Pod <code>pod-netpol-1</code> again</p>
<pre><code>kubectl exec -it pod-netpol-1-6494f6bf8b-6nwwf -- sh
</code></pre>
<p>Execute command <code>ping</code> to check if pod-netpol-2 and pod-netpol-3 are pingable. Both ping are denied as expected.</p>
<pre><code>/ # ping 10.244.112.9
3 packets transmitted, 0 packets received, 100% packet loss

/ # ping 10.244.102.8
3 packets transmitted, 0 packets received, 100% packet loss
</code></pre>
<h4 id="allow-for-specific-ingress">Allow For Specific Ingress</h4>
<p>Create NetworkPlicy to allow ingress from pod-netpol-1 to pod-netpol-2.</p>
<pre><code>cat &gt; allow-pod-netpol-1-to-pod-netpol-2.yaml &lt;&lt;EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-pod-netpol-1-to-pod-netpol-2
spec:
  podSelector:
    matchLabels:
      app: pod-netpol-2
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: pod-netpol-1
EOF

kubectl apply -f allow-pod-netpol-1-to-pod-netpol-2.yaml
</code></pre>
<h4 id="verify-networkpolicy">Verify NetworkPolicy</h4>
<p>Attach to Pod <code>pod-netpol-1</code> again.</p>
<pre><code>kubectl exec -it pod-netpol-1-6494f6bf8b-6nwwf -- sh
</code></pre>
<p>Execute command <code>ping</code> to check if pod-netpol-2 and pod-netpol-3 are pingable. 
As expected, pod-netpol-2 is reachable and pod-netpol-3 is still unreachable. </p>
<pre><code>/ # ping 10.244.112.9
3 packets transmitted, 3 packets received, 0% packet loss

/ # ping 10.244.102.8
3 packets transmitted, 0 packets received, 100% packet loss
</code></pre>
<h3 id="inbound-across-namespace">Inbound Across Namespace</h3>
<h4 id="create-workload-and-namespace-for-test">Create workload and namespace for test</h4>
<p>Create Namespace <code>ns-netpol</code>.
Create Deployment <code>pod-netpol</code>.</p>
<pre><code>kubectl create ns ns-netpol

cat &gt; pod-netpol.yaml &lt;&lt; EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: pod-netpol
  name: pod-netpol
  namespace: ns-netpol
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pod-netpol
  template:
    metadata:
      labels:
        app: pod-netpol
    spec:
      containers:
      - image: busybox
        name: busybox
        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;sleep 1h&quot;]
EOF


kubectl apply -f pod-netpol.yaml
</code></pre>
<p>Check Pod status on new namespace.</p>
<pre><code>kubectl get pod -n ns-netpol
</code></pre>
<p>Output:</p>
<pre><code>NAME                          READY   STATUS    RESTARTS   AGE
pod-netpol-5b67b6b496-zxppp   1/1     Running   0          10s
</code></pre>
<p>Attach into <code>pod-netpol</code> Pod.</p>
<pre><code>kubectl exec -it pod-netpol-5b67b6b496-zxppp -n ns-netpol -- sh
</code></pre>
<p>Try to ping pod-netpol-2 (<code>10.244.112.9</code>) in Namespace <code>jh-namespace</code>. It's unreachable. </p>
<pre><code>ping 10.244.112.9
3 packets transmitted, 0 packets received, 100% packet loss
</code></pre>
<h4 id="create-allow-ingress">Create Allow Ingress</h4>
<p>Create NetworkPolicy to allow access to pod-netpol-2 in namespace <code>jh-namespace</code> from all Pods in namespace <code>pod-netpol</code>.</p>
<pre><code>cat &gt; allow-ns-netpol-to-pod-netpol-2.yaml &lt;&lt;EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ns-netpol-to-pod-netpol-2
spec:
  podSelector:
    matchLabels:
      app: pod-netpol-2
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          allow: to-pod-netpol-2
EOF

kubectl apply -f allow-ns-netpol-to-pod-netpol-2.yaml
</code></pre>
<h4 id="verify-policy">Verify Policy</h4>
<p>Attach into <code>pod-netpol</code> Pod.</p>
<pre><code>kubectl exec -it pod-netpol-5b67b6b496-zxppp -n ns-netpol -- sh
</code></pre>
<p>Try to ping pod-netpol-2 (<code>10.244.112.9</code>) in Namespace <code>jh-namespace</code>. It's still unreachable. </p>
<pre><code>ping 10.244.112.9
3 packets transmitted, 0 packets received, 100% packet loss
</code></pre>
<p>What we allowed ingress is from namespace with label <code>allow: to-pod-netpol-2</code>, but namespace <code>ns-netpol</code> does not have it and we need label it.</p>
<pre><code>kubectl label ns ns-netpol allow=to-pod-netpol-2
</code></pre>
<p>Attach into <code>pod-netpol</code> Pod.</p>
<pre><code>kubectl exec -it pod-netpol-5b67b6b496-zxppp -n ns-netpol -- sh
</code></pre>
<p>Try to ping pod-netpol-2 (<code>10.244.112.9</code>) in Namespace <code>jh-namespace</code>. It's now reachable. </p>
<pre><code>ping 10.244.112.9
3 packets transmitted, 3 packets received, 0% packet loss
</code></pre>
<p>Be noted that we can use namespace default label as well.</p>
                
              
              
                


              
            </article>
          </div>
        </div>
        
      </main>
      
        
<footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        
          Made with
          <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
            Material for MkDocs
          </a>
        
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.fcfe8b6d.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.b1047164.min.js"></script>
      
    
  </body>
</html>