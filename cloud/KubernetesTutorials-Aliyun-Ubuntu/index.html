
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="canonical" href="https://huyuhui001.github.io/mySite/index.html/cloud/KubernetesTutorials-Aliyun-Ubuntu/">
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-7.3.6">
    
    
      
        <title>Kubernetes Tutourials: Ubuntu@Aliyun - MEMO</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.a57b2b03.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.3f5d1f46.min.css">
        
          
          
          <meta name="theme-color" content="#2094f3">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
    
      


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="blue" data-md-color-accent="deep-blue">
  
    
    <script>function __prefix(e){return new URL("../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#kubernetes-tutourials-ubuntualiyun" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="MEMO" class="md-header__button md-logo" aria-label="MEMO" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MEMO
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Kubernetes Tutourials: Ubuntu@Aliyun
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="MEMO" class="md-nav__button md-logo" aria-label="MEMO" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    MEMO
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../linux/" class="md-nav__link">
        Linux
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../python/" class="md-nav__link">
        Python
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        Cloud
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../about/" class="md-nav__link">
        About
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1installation" class="md-nav__link">
    1.Installation
  </a>
  
    <nav class="md-nav" aria-label="1.Installation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#preparation" class="md-nav__link">
    Preparation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#initialize-vms" class="md-nav__link">
    Initialize VMs
  </a>
  
    <nav class="md-nav" aria-label="Initialize VMs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#configure-etchosts-file" class="md-nav__link">
    Configure /etc/hosts file
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#disable-firewall" class="md-nav__link">
    Disable firewall
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#turn-off-swap" class="md-nav__link">
    Turn off swap
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set-timezone-and-locale" class="md-nav__link">
    Set timezone and locale
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kernel-setting" class="md-nav__link">
    Kernel setting
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#install-containerd" class="md-nav__link">
    Install Containerd
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#install-nerdctl" class="md-nav__link">
    Install nerdctl
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#install-kubeadm" class="md-nav__link">
    Install kubeadm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup-master-node" class="md-nav__link">
    Setup Master Node
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup-work-nodes" class="md-nav__link">
    Setup Work Nodes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#install-calico-or-flannel" class="md-nav__link">
    Install Calico or Flannel
  </a>
  
    <nav class="md-nav" aria-label="Install Calico or Flannel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#install-flannel" class="md-nav__link">
    Install Flannel
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#install-calico" class="md-nav__link">
    Install Calico
  </a>
  
    <nav class="md-nav" aria-label="Install Calico">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-calico-datastore" class="md-nav__link">
    The Calico Datastore
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configure-ip-pools" class="md-nav__link">
    Configure IP Pools
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#install-cni-plugin" class="md-nav__link">
    Install CNI plugin
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#install-typha" class="md-nav__link">
    Install Typha
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#install-caliconode" class="md-nav__link">
    Install calico/node
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test-networking" class="md-nav__link">
    Test networking
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#check-cluster-status" class="md-nav__link">
    Check Cluster Status
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reset-cluster" class="md-nav__link">
    Reset cluster
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#troubleshooting" class="md-nav__link">
    Troubleshooting
  </a>
  
    <nav class="md-nav" aria-label="Troubleshooting">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#issue-1" class="md-nav__link">
    Issue 1
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#issue-2" class="md-nav__link">
    Issue 2
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2post-installation" class="md-nav__link">
    2.Post Installation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3cluster-overview" class="md-nav__link">
    3.Cluster Overview
  </a>
  
    <nav class="md-nav" aria-label="3.Cluster Overview">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#container" class="md-nav__link">
    Container
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kubernetes-layer" class="md-nav__link">
    Kubernetes Layer
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#case-study" class="md-nav__link">
    Case Study
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4kubectl" class="md-nav__link">
    4.kubectl
  </a>
  
    <nav class="md-nav" aria-label="4.kubectl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#config-file" class="md-nav__link">
    Config File
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bash-autocomplete" class="md-nav__link">
    Bash Autocomplete
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#common-usage" class="md-nav__link">
    Common Usage
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5kubernetes-api-and-resource" class="md-nav__link">
    5.Kubernetes API and Resource
  </a>
  
    <nav class="md-nav" aria-label="5.Kubernetes API and Resource">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#static-pod" class="md-nav__link">
    Static Pod
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#init-containers" class="md-nav__link">
    Init containers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mutil-container-pod" class="md-nav__link">
    Mutil-Container Pod
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#usage-of-kubectl" class="md-nav__link">
    Usage of kubectl
  </a>
  
    <nav class="md-nav" aria-label="Usage of kubectl">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#grant-authorization-to-serviceaccount" class="md-nav__link">
    Grant Authorization to ServiceAccount
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#label-node" class="md-nav__link">
    Label Node
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deployment" class="md-nav__link">
    Deployment
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#namespace" class="md-nav__link">
    Namespace
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#expose-service" class="md-nav__link">
    Expose Service
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scalling" class="md-nav__link">
    Scalling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rolling" class="md-nav__link">
    Rolling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#event" class="md-nav__link">
    Event
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logging" class="md-nav__link">
    Logging
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#demo-workload-resources" class="md-nav__link">
    Demo: Workload Resources
  </a>
  
    <nav class="md-nav" aria-label="Demo: Workload Resources">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#deployment_1" class="md-nav__link">
    Deployment
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#statefulset" class="md-nav__link">
    StatefulSet
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#daemonset" class="md-nav__link">
    DaemonSet
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#job" class="md-nav__link">
    Job
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cronjob" class="md-nav__link">
    Cronjob
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6label-and-annotation" class="md-nav__link">
    6.Label and Annotation
  </a>
  
    <nav class="md-nav" aria-label="6.Label and Annotation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#label-and-annotation" class="md-nav__link">
    Label and Annotation
  </a>
  
    <nav class="md-nav" aria-label="Label and Annotation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#label" class="md-nav__link">
    Label
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#annotation" class="md-nav__link">
    Annotation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7health-check" class="md-nav__link">
    7.Health Check
  </a>
  
    <nav class="md-nav" aria-label="7.Health Check">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#status-of-pod-and-container" class="md-nav__link">
    Status of Pod and Container
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#livenessprobe" class="md-nav__link">
    LivenessProbe
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#readinessprobe" class="md-nav__link">
    ReadinessProbe
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#demo-of-health-check" class="md-nav__link">
    Demo of Health Check
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8namespace" class="md-nav__link">
    8.Namespace
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9horizontal-pod-autoscaling-hpa" class="md-nav__link">
    9.Horizontal Pod Autoscaling (HPA)
  </a>
  
    <nav class="md-nav" aria-label="9.Horizontal Pod Autoscaling (HPA)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#deploy-a-service-podinfo" class="md-nav__link">
    Deploy a Service podinfo
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#config-hpa" class="md-nav__link">
    Config HPA
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stress-testing" class="md-nav__link">
    Stress Testing
  </a>
  
    <nav class="md-nav" aria-label="Stress Testing">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#install-ab" class="md-nav__link">
    Install ab
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#concurrency-stres-test" class="md-nav__link">
    Concurrency Stres Test
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10service" class="md-nav__link">
    10.Service
  </a>
  
    <nav class="md-nav" aria-label="10.Service">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#clusterip" class="md-nav__link">
    ClusterIP
  </a>
  
    <nav class="md-nav" aria-label="ClusterIP">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#create-service" class="md-nav__link">
    Create Service
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#expose-service_1" class="md-nav__link">
    Expose Service
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nodeport" class="md-nav__link">
    NodePort
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#special-service" class="md-nav__link">
    Special Service
  </a>
  
    <nav class="md-nav" aria-label="Special Service">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#headless-service" class="md-nav__link">
    Headless Service
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11ingress" class="md-nav__link">
    11.Ingress
  </a>
  
    <nav class="md-nav" aria-label="11.Ingress">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#deploy-ingress-controller" class="md-nav__link">
    Deploy Ingress Controller
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-deployments" class="md-nav__link">
    Create Deployments
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-service_1" class="md-nav__link">
    Create Service
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-ingress" class="md-nav__link">
    Create Ingress
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test-accessiblity" class="md-nav__link">
    Test Accessiblity
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#12storage" class="md-nav__link">
    12.Storage
  </a>
  
    <nav class="md-nav" aria-label="12.Storage">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#emptydir" class="md-nav__link">
    emptyDir
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hostpath" class="md-nav__link">
    hostPath
  </a>
  
    <nav class="md-nav" aria-label="hostPath">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#verify-mysql-availability" class="md-nav__link">
    Verify MySQL Availability
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pv-and-pvc" class="md-nav__link">
    PV and PVC
  </a>
  
    <nav class="md-nav" aria-label="PV and PVC">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#set-up-nfs-server" class="md-nav__link">
    Set up NFS Server
  </a>
  
    <nav class="md-nav" aria-label="Set up NFS Server">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#install-nfs-kernel-server" class="md-nav__link">
    Install nfs-kernel-server
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configure-share-folder" class="md-nav__link">
    Configure Share Folder
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#install-nfs-client" class="md-nav__link">
    Install NFS Client
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#verify-nfs-server" class="md-nav__link">
    Verify NFS Server
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mount-nfs" class="md-nav__link">
    Mount NFS
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-pv" class="md-nav__link">
    Create PV
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-pvc" class="md-nav__link">
    Create PVC
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#consume-pvc" class="md-nav__link">
    Consume PVC
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#storageclass" class="md-nav__link">
    StorageClass
  </a>
  
    <nav class="md-nav" aria-label="StorageClass">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#configure-rbac-authorization" class="md-nav__link">
    Configure RBAC Authorization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#install-nfs-provisioner" class="md-nav__link">
    Install nfs-provisioner
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-nfs-storageclass" class="md-nav__link">
    Create NFS StorageClass
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#verify" class="md-nav__link">
    Verify
  </a>
  
    <nav class="md-nav" aria-label="Verify">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#create-pvc_1" class="md-nav__link">
    Create PVC
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#consume-pvc_1" class="md-nav__link">
    Consume PVC
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configuration" class="md-nav__link">
    Configuration
  </a>
  
    <nav class="md-nav" aria-label="Configuration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#configmap" class="md-nav__link">
    ConfigMap
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#secret" class="md-nav__link">
    Secret
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#additional-keys" class="md-nav__link">
    Additional Keys
  </a>
  
    <nav class="md-nav" aria-label="Additional Keys">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#various-way-to-create-configmap" class="md-nav__link">
    Various way to create ConfigMap
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set-environment-variable-via-configmap" class="md-nav__link">
    Set environment variable via ConfigMap
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#13scheduling" class="md-nav__link">
    13.Scheduling
  </a>
  
    <nav class="md-nav" aria-label="13.Scheduling">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#nodeselector" class="md-nav__link">
    nodeSelector
  </a>
  
    <nav class="md-nav" aria-label="nodeSelector">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#label-node_1" class="md-nav__link">
    Label Node
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configure-nodeselector-for-pod" class="md-nav__link">
    Configure nodeSelector for Pod
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nodename" class="md-nav__link">
    nodeName
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#affinity" class="md-nav__link">
    Affinity
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#taints-tolerations" class="md-nav__link">
    Taints &amp; Tolerations
  </a>
  
    <nav class="md-nav" aria-label="Taints &amp; Tolerations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#concept" class="md-nav__link">
    Concept
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set-taints" class="md-nav__link">
    Set Taints
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set-tolerations" class="md-nav__link">
    Set Tolerations
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#remove-taints" class="md-nav__link">
    Remove Taints
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#14resourcequota" class="md-nav__link">
    14.ResourceQuota
  </a>
  
    <nav class="md-nav" aria-label="14.ResourceQuota">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#create-namespace" class="md-nav__link">
    Create Namespace
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-resourcequota" class="md-nav__link">
    Create ResourceQuota
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#verify-test" class="md-nav__link">
    Verify &amp; Test
  </a>
  
    <nav class="md-nav" aria-label="Verify &amp; Test">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#test-nodeport" class="md-nav__link">
    Test NodePort
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test-pvc" class="md-nav__link">
    Test PVC
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#15limitrange" class="md-nav__link">
    15.LimitRange
  </a>
  
    <nav class="md-nav" aria-label="15.LimitRange">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#create-namespace_1" class="md-nav__link">
    Create Namespace
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set-limitrange" class="md-nav__link">
    Set LimitRange
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test-via-pod" class="md-nav__link">
    Test via Pod
  </a>
  
    <nav class="md-nav" aria-label="Test via Pod">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#scenario-1-pod-without-specified-limits" class="md-nav__link">
    Scenario 1: Pod without specified limits
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scenario-2-pod-with-cpu-limit-without-cpu-request" class="md-nav__link">
    Scenario 2: Pod with CPU limit, without CPU Request
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scenario-3-pod-with-cpu-request-onlyl-without-cpu-limits" class="md-nav__link">
    Scenario 3: Pod with CPU Request onlyl, without CPU Limits
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#16troubleshooting" class="md-nav__link">
    16.Troubleshooting
  </a>
  
    <nav class="md-nav" aria-label="16.Troubleshooting">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#event_1" class="md-nav__link">
    Event
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logs" class="md-nav__link">
    Logs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#monitoring-indicators" class="md-nav__link">
    Monitoring Indicators
  </a>
  
    <nav class="md-nav" aria-label="Monitoring Indicators">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#nodes" class="md-nav__link">
    Nodes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#node-eviction" class="md-nav__link">
    Node Eviction
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#17rbac" class="md-nav__link">
    17.RBAC
  </a>
  
    <nav class="md-nav" aria-label="17.RBAC">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#install-cfssl" class="md-nav__link">
    Install cfssl
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set-multiple-contexts" class="md-nav__link">
    Set Multiple Contexts
  </a>
  
    <nav class="md-nav" aria-label="Set Multiple Contexts">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#current-context" class="md-nav__link">
    Current Context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-ca-config-file" class="md-nav__link">
    Create CA Config File
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-csr-file-for-signature" class="md-nav__link">
    Create CSR file for signature
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-file-kubeconfig" class="md-nav__link">
    Create file kubeconfig
  </a>
  
    <nav class="md-nav" aria-label="Create file kubeconfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#set-up-cluster" class="md-nav__link">
    Set up cluster
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set-up-user" class="md-nav__link">
    Set up user
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set-up-context" class="md-nav__link">
    Set up Context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#verify_1" class="md-nav__link">
    Verify
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#merge-kubeconfig-files" class="md-nav__link">
    Merge kubeconfig files
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#namespaces-contexts" class="md-nav__link">
    Namespaces &amp; Contexts
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#role-rolebinding" class="md-nav__link">
    Role &amp; RoleBinding
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clusterrole-clusterrolebinding" class="md-nav__link">
    ClusterRole &amp; ClusterRoleBinding
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#18network-policy" class="md-nav__link">
    18.Network Policy
  </a>
  
    <nav class="md-nav" aria-label="18.Network Policy">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#replace-flannel-by-calico" class="md-nav__link">
    Replace Flannel by Calico
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inbound-rules" class="md-nav__link">
    Inbound Rules
  </a>
  
    <nav class="md-nav" aria-label="Inbound Rules">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#create-workload-for-test" class="md-nav__link">
    Create workload for test.
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deny-for-all-ingress" class="md-nav__link">
    Deny For All Ingress
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#allow-for-specific-ingress" class="md-nav__link">
    Allow For Specific Ingress
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#verify-networkpolicy" class="md-nav__link">
    Verify NetworkPolicy
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inbound-across-namespace" class="md-nav__link">
    Inbound Across Namespace
  </a>
  
    <nav class="md-nav" aria-label="Inbound Across Namespace">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#create-workload-and-namespace-for-test" class="md-nav__link">
    Create workload and namespace for test
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-allow-ingress" class="md-nav__link">
    Create Allow Ingress
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#verify-policy" class="md-nav__link">
    Verify Policy
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#19cluster-management" class="md-nav__link">
    19.Cluster Management
  </a>
  
    <nav class="md-nav" aria-label="19.Cluster Management">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#etcd-backup-and-restore" class="md-nav__link">
    etcd Backup and Restore
  </a>
  
    <nav class="md-nav" aria-label="etcd Backup and Restore">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#install-etcdctl" class="md-nav__link">
    Install etcdctl
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-deployment-before-backup" class="md-nav__link">
    Create Deployment Before Backup
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backup-etcd" class="md-nav__link">
    Backup etcd
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#create-deployment-after-backup" class="md-nav__link">
    Create Deployment After Backup
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#restore-etcd" class="md-nav__link">
    Restore etcd
  </a>
  
    <nav class="md-nav" aria-label="Restore etcd">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#stop-services" class="md-nav__link">
    Stop Services
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#restore-etcd_1" class="md-nav__link">
    Restore etcd
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#start-services" class="md-nav__link">
    Start Services
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#verify_2" class="md-nav__link">
    Verify
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#upgrade-kubeadm" class="md-nav__link">
    Upgrade kubeadm
  </a>
  
    <nav class="md-nav" aria-label="Upgrade kubeadm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#upgrade-control-plane" class="md-nav__link">
    Upgrade Control Plane
  </a>
  
    <nav class="md-nav" aria-label="Upgrade Control Plane">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#preparation_1" class="md-nav__link">
    Preparation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#upgrade" class="md-nav__link">
    Upgrade
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#upgrade-worker" class="md-nav__link">
    Upgrade Worker
  </a>
  
    <nav class="md-nav" aria-label="Upgrade Worker">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#preparation_2" class="md-nav__link">
    Preparation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#upgrade_1" class="md-nav__link">
    Upgrade
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#verify_3" class="md-nav__link">
    Verify
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#20helm-chart" class="md-nav__link">
    20.Helm Chart
  </a>
  
    <nav class="md-nav" aria-label="20.Helm Chart">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#install-helm" class="md-nav__link">
    Install Helm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#usage-of-helm" class="md-nav__link">
    Usage of Helm
  </a>
  
    <nav class="md-nav" aria-label="Usage of Helm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#install-mysql-from-helm" class="md-nav__link">
    Install MySQL from Helm
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#develop-a-chart" class="md-nav__link">
    Develop a Chart
  </a>
  
    <nav class="md-nav" aria-label="Develop a Chart">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#notestxt" class="md-nav__link">
    NOTES.txt
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deployment-template" class="md-nav__link">
    Deployment Template
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configmap-template" class="md-nav__link">
    ConfigMap Template
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_helperstpl" class="md-nav__link">
    _helpers.tpl
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#chartyaml" class="md-nav__link">
    Chart.yaml
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#chart-debug" class="md-nav__link">
    Chart Debug
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reference" class="md-nav__link">
    Reference
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#a1discussion" class="md-nav__link">
    A1.Discussion
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="kubernetes-tutourials-ubuntualiyun">Kubernetes Tutourials: Ubuntu@Aliyun</h1>
<h2 id="1installation">1.Installation</h2>
<h3 id="preparation">Preparation</h3>
<p>Register Aliyun account via <a href="https://home.console.aliyun.com/home/dashboard/ProductAndService">Alibaba Cloud home console</a>.</p>
<p>Request three Elastic Computer Service(ECS) instances with below sizing:</p>
<ul>
<li>System: 2vCPU+4GiB</li>
<li>OS: Ubuntu  20.04 x86_64</li>
<li>Instance Type: ecs.sn1.medium </li>
<li>Instance Name: cka001, cka002, cka003</li>
<li>Network: both public IPs and private IPs</li>
<li>Maximum Bandwidth: 100Mbps (Peak Value)</li>
<li>Cloud disk: 40GiB</li>
<li>Billing Method: Preemptible instance (spot price)</li>
</ul>
<p>Generate SSH key pairs with name <code>cka-key-pair</code> in local directcory <code>/opt</code>.</p>
<p>Change access control to <code>400</code> per security required by command <code>sudo chmod 400 cka-key-pair.pem</code>.
cka003
Access remote cka servers via command <code>ssh -i cka-key-pair.pem root@&lt;your public ip address&gt;</code></p>
<h3 id="initialize-vms">Initialize VMs</h3>
<h4 id="configure-etchosts-file">Configure /etc/hosts file</h4>
<p>Add private IPs in the <code>/etc/hosts</code> file in all VMs.</p>
<h4 id="disable-firewall">Disable firewall</h4>
<p>Disable firewall by command <code>ufw disable</code> in all VMs.</p>
<p>Disable swap on Ubuntu.</p>
<pre><code>sudo ufw disable
</code></pre>
<p>Check status of swap on Ubuntu.</p>
<pre><code>sudo ufw status verbose
</code></pre>
<h3 id="turn-off-swap">Turn off swap</h3>
<p>Turn off swap by command <code>swapoff -a</code> in all VMs.</p>
<h3 id="set-timezone-and-locale">Set timezone and locale</h3>
<p>Set timezone and local for all VMs. For ECS with Ubuntu 20.04 version created by Aliyun, this step is not needed.</p>
<pre><code>ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
sudo echo 'LANG=&quot;en_US.UTF-8&quot;' &gt;&gt; /etc/profile
source /etc/profile
</code></pre>
<p>Something like this:</p>
<pre><code>ll /etc/localtime
</code></pre>
<pre><code>lrwxrwxrwx 1 root root 33 May 24 18:14 /etc/localtime -&gt; /usr/share/zoneinfo/Asia/Shanghai
</code></pre>
<h3 id="kernel-setting">Kernel setting</h3>
<p>Perform below kernel setting in all VMs.</p>
<p>Create file <code>/etc/modules-load.d/containerd.conf</code> to set up containerd configure file.
It's to load two modules <code>overlay</code> and <code>br_netfilter</code>.</p>
<p>Service <code>containerd</code> depends on <code>overlay</code> filesystem. Sometimes referred to as union-filesystems. An <a href="https://developer.aliyun.com/article/660712">overlay-filesystem</a> tries to present a filesystem which is the result over overlaying one filesystem on top of the other. </p>
<p>The <code>br_netfilter</code> module is required to enable transparent masquerading and to facilitate Virtual Extensible LAN (VxLAN) traffic for communication between Kubernetes pods across the cluster. </p>
<pre><code>cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
</code></pre>
<p>Load <code>overlay</code> and <code>br_netfilter</code> modules.</p>
<pre><code>sudo modprobe overlay
sudo modprobe br_netfilter
</code></pre>
<p>Create file <code>99-kubernetes-cri.conf</code> to set up configure file for Kubernetes CRI.</p>
<p>Set <code>net/bridge/bridge-nf-call-iptables=1</code> to ensure simple configurations (like Docker with a bridge) work correctly with the iptables proxy. <a href="https://cloud.tencent.com/developer/article/1828060">Why <code>net/bridge/bridge-nf-call-iptables=1</code> need to be enable by Kubernetes</a>.</p>
<p>IP forwarding is also known as routing. When it comes to Linux, it may also be called Kernel IP forwarding because it uses the kernel variable <code>net.ipv4.ip_forward</code> to enable or disable the IP forwarding feature. The default preset value is <code>ip_forward=0</code>. Hence, the Linux IP forwarding feature is disabled by default.</p>
<pre><code>cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
</code></pre>
<p>The <code>sysctl</code> command reads the information from the <code>/proc/sys</code> directory. <code>/proc/sys</code> is a virtual directory that contains file objects that can be used to view and set the current kernel parameters.</p>
<p>By commadn <code>sysctl -w net.ipv4.ip_forward=1</code>, the change takes effect immediately, but it is not persistent. After a system reboot, the default value is loaded. Write the settings to <code>/etc/sysctl.conf</code> is to set a parameter permanently, you’ll need to  or another configuration file in the /etc/sysctl.d directory:</p>
<pre><code>sudo sysctl --system
</code></pre>
<h3 id="install-containerd">Install Containerd</h3>
<p>Install Containerd sevice for all VMs.</p>
<p>Backup source file.</p>
<pre><code>sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak
</code></pre>
<p>Add proper repo sources. For ECS with Ubuntu 20.04 version created by Aliyun, this step is not needed.</p>
<pre><code>cat &gt; /etc/apt/sources.list &lt;&lt; EOF
deb http://mirrors.cloud.aliyuncs.com/ubuntu/ focal main restricted
deb-src http://mirrors.cloud.aliyuncs.com/ubuntu/ focal main restricted
deb http://mirrors.cloud.aliyuncs.com/ubuntu/ focal-updates main restricted
deb-src http://mirrors.cloud.aliyuncs.com/ubuntu/ focal-updates main restricted
deb http://mirrors.cloud.aliyuncs.com/ubuntu/ focal universe
deb-src http://mirrors.cloud.aliyuncs.com/ubuntu/ focal universe
deb http://mirrors.cloud.aliyuncs.com/ubuntu/ focal-updates universe
deb-src http://mirrors.cloud.aliyuncs.com/ubuntu/ focal-updates universe
deb http://mirrors.cloud.aliyuncs.com/ubuntu/ focal multiverse
deb-src http://mirrors.cloud.aliyuncs.com/ubuntu/ focal multiverse
deb http://mirrors.cloud.aliyuncs.com/ubuntu/ focal-updates multiverse
deb-src http://mirrors.cloud.aliyuncs.com/ubuntu/ focal-updates multiverse
deb http://mirrors.cloud.aliyuncs.com/ubuntu/ focal-backports main restricted universe multiverse
deb-src http://mirrors.cloud.aliyuncs.com/ubuntu/ focal-backports main restricted universe multivers
deb http://mirrors.cloud.aliyuncs.com/ubuntu focal-security main restricted
deb-src http://mirrors.cloud.aliyuncs.com/ubuntu focal-security main restricted
deb http://mirrors.cloud.aliyuncs.com/ubuntu focal-security universe
deb-src http://mirrors.cloud.aliyuncs.com/ubuntu focal-security universe
# deb http://mirrors.cloud.aliyuncs.com/ubuntu focal-security multiverse
# deb-src http://mirrors.cloud.aliyuncs.com/ubuntu focal-security multiverse
EOF
</code></pre>
<p>Install Containered.</p>
<pre><code>sudo apt-get update &amp;&amp; sudo apt-get install -y containerd
</code></pre>
<p>Configure Containerd. Modify file <code>/etc/containerd/config.toml</code>.</p>
<pre><code>sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
vi /etc/containerd/config.toml
</code></pre>
<p>Update <code>sandbox_image</code> with new value <code>"registry.aliyuncs.com/google_containers/pause:3.6"</code>.
Update <code>SystemdCgroup</code> with new value <code>true</code>.</p>
<pre><code>[plugins]
  [plugins.&quot;io.containerd.gc.v1.scheduler&quot;]

  [plugins.&quot;io.containerd.grpc.v1.cri&quot;]
    sandbox_image = &quot;registry.aliyuncs.com/google_containers/pause:3.6&quot;

    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.cni]
    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd]
      [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.default_runtime]
        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.default_runtime.options]
      [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes]
        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc]

          [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc.options]
            SystemdCgroup = true
</code></pre>
<p>Restart Containerd service.</p>
<pre><code>sudo systemctl restart containerd
sudo systemctl status containerd
</code></pre>
<h3 id="install-nerdctl">Install nerdctl</h3>
<p>Install nerdctl sevice fro all VMs.</p>
<p>The goal of <a href="https://github.com/containerd/nerdctl"><code>nerdctl</code></a> is to facilitate experimenting the cutting-edge features of containerd that are not present in Docker.</p>
<p>Get the release from the link https://github.com/containerd/nerdctl/releases.</p>
<pre><code>wget https://github.com/containerd/nerdctl/releases/download/v0.22.0/nerdctl-0.22.0-linux-amd64.tar.gz
tar -zxvf nerdctl-0.22.0-linux-amd64.tar.gz
cp nerdctl /usr/bin/
</code></pre>
<p>Verify nerdctl.</p>
<pre><code>nerdctl --help
</code></pre>
<p>To list local Kubernetes containers.</p>
<pre><code>nerdctl -n k8s.io ps
</code></pre>
<h3 id="install-kubeadm">Install kubeadm</h3>
<p>Update <code>apt-transport-https</code>,  <code>ca-certificates</code>, and <code>curl</code>.</p>
<pre><code>apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https ca-certificates curl
</code></pre>
<p>Install gpg certificate. Just choose one of below command and execute.</p>
<pre><code># For Ubuntu 20.04 release.
curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -

# For Ubuntu 22.04 release
sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg
</code></pre>
<p>Add Kubernetes repo. Just choose one of below command and execute.</p>
<pre><code># For Ubuntu20.04 release
cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF

# For Ubuntu 22.04 release
echo &quot;deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://mirrors.aliyun.com/kubernetes/apt/ \
  kubernetes-xenial main&quot; | sudo tee /etc/apt/sources.list.d/kubernetes.list
</code></pre>
<p>Update and install dependencied packages.</p>
<pre><code>apt-get update
apt-get install ebtables
apt-get install libxtables12
apt-get upgrade iptables
</code></pre>
<p>Check available versions of kubeadm.</p>
<pre><code>apt policy kubeadm
</code></pre>
<p>Install <code>1.23.8-00</code> version of kubeadm and will upgrade to <code>1.24.2</code> later.</p>
<pre><code>sudo apt-get -y install kubelet=1.23.8-00 kubeadm=1.23.8-00 kubectl=1.23.8-00 --allow-downgrades
</code></pre>
<h3 id="setup-master-node">Setup Master Node</h3>
<p>Set up Control Plane on VM playing master node.</p>
<p>Check kubeadm default parameters for initialization.</p>
<pre><code>kubeadm config print init-defaults
</code></pre>
<pre><code>apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 1.2.3.4
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  imagePullPolicy: IfNotPresent
  name: node
  taints: null
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns: {}
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
kubernetesVersion: 1.23.0
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}
</code></pre>
<p>Dry rune and run. Save the output, which will be used later on work nodes.</p>
<p>With <code>kubeadm init</code> to initiate cluster, we need understand below three options about network.</p>
<ul>
<li><code>--pod-network-cidr</code>: <ul>
<li>Specify range of IP addresses for the pod network. If set, the control plane will automatically allocate CIDRs for every node.</li>
<li>Be noted that <code>10.244.0.0/16</code> is default range of flannel. If it's changed here, please do change the same when deploy <code>Flannel</code>.</li>
</ul>
</li>
<li><code>--apiserver-bind-port</code>: <ul>
<li>Port for the API Server to bind to. (default 6443)</li>
</ul>
</li>
<li><code>--service-cidr</code>: <ul>
<li>Use alternative range of IP address for service VIPs. (default "10.96.0.0/12")</li>
</ul>
</li>
</ul>
<p>Note: </p>
<ul>
<li>service VIPs (a.k.a. Cluster IP), specified by option <code>--service-cidr</code>.</li>
<li>podCIDR (a.k.a. endpoint IP)，specified by option <code>--pod-network-cidr</code>.</li>
</ul>
<p>There are 4 distinct networking problems to address:</p>
<ul>
<li>Highly-coupled container-to-container communications: this is solved by Pods (podCIDR) and localhost communications.</li>
<li>Pod-to-Pod communications: <ul>
<li>a.k.a. container-to-container. </li>
<li>Example with Flannel, the flow is: Pod --&gt; veth pair --&gt; cni0 --&gt; flannel.1 --&gt; host eth0 --&gt; host eth0 --&gt; flannel.1 --&gt; cni0 --&gt; veth pair --&gt; Pod.</li>
</ul>
</li>
<li>Pod-to-Service communications:<ul>
<li>Flow: Pod --&gt; Kernel --&gt; Servive iptables --&gt; service --&gt; Pod iptables --&gt; Pod</li>
</ul>
</li>
<li>External-to-Service communications: <ul>
<li>LoadBalancer: SLB --&gt; NodePort --&gt; Service --&gt; Pod</li>
</ul>
</li>
</ul>
<p><code>kube-proxy</code> is responsible for iptables, not traffic. </p>
<pre><code>kubeadm init \
  --dry-run \
  --pod-network-cidr=10.244.0.0/16 \
  --service-cidr 11.244.0.0/16 \
  --image-repository=registry.aliyuncs.com/google_containers \
  --kubernetes-version=v1.23.8

kubeadm init \
  --pod-network-cidr=10.244.0.0/16 \
  --service-cidr 11.244.0.0/16 \
  --image-repository=registry.aliyuncs.com/google_containers \
  --kubernetes-version=v1.23.8
</code></pre>
<p>Set <code>kubeconfig</code> file for current user (here it's <code>root</code>).</p>
<pre><code>mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre>
<p>Set <code>kubectl</code> <a href="https://github.com/scop/bash-completion">auto-completion</a> following the <a href="https://kubernetes.io/docs/tasks/tools/included/optional-kubectl-configs-bash-linux/">guideline</a>.</p>
<pre><code>apt install -y bash-completion
source /usr/share/bash-completion/bash_completion
source &lt;(kubectl completion bash)
echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~/.bashrc
</code></pre>
<p>If we set an alias for kubectl, we can extend shell completion to work with that alias:</p>
<pre><code>echo 'alias k=kubectl' &gt;&gt;~/.bashrc
echo 'complete -o default -F __start_kubectl k' &gt;&gt;~/.bashrc
</code></pre>
<h3 id="setup-work-nodes">Setup Work Nodes</h3>
<p>Perform on all VMs playing work nodes.</p>
<pre><code># kubeadm join &lt;your master node eth0 ip&gt;:6443 --token &lt;token generated by kubeadm init&gt; --discovery-token-ca-cert-hash &lt;hash key generated by kubeadm init&gt;
</code></pre>
<p>Use <code>kubeadm token</code> to generate the join token and hash value.</p>
<pre><code>kubeadm token create --print-join-command
</code></pre>
<p>Verify status on master node.</p>
<pre><code>root@cka001:~# kubectl get node
NAME     STATUS   ROLES                  AGE     VERSION
cka001   Ready    control-plane,master   24m     v1.23.8
cka002   Ready    &lt;none&gt;                 9m39s   v1.23.8
cka003   Ready    &lt;none&gt;                 9m27s   v1.23.8
</code></pre>
<h3 id="install-calico-or-flannel">Install Calico or Flannel</h3>
<p>Choose Calico or Flannel. </p>
<p>For NetworkPolicy purpose, choose Calico.</p>
<h4 id="install-flannel">Install Flannel</h4>
<p><a href="https://github.com/flannel-io/flannel">Flannel</a> is a simple and easy way to configure a layer 3 network fabric designed for Kubernetes.</p>
<p>Deploy Flannel on master node.
In the kube-flannel.yml we can get the default network setting of Flannel, which is same with <code>--pod-network-cidr=10.244.0.0/16</code> we defined before when we initiated <code>kubeadm</code>.</p>
<pre><code>  net-conf.json: |
    {
      &quot;Network&quot;: &quot;10.244.0.0/16&quot;,
      &quot;Backend&quot;: {
        &quot;Type&quot;: &quot;vxlan&quot;
      }
    }
</code></pre>
<pre><code>root@cka001:~# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Warning: policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
podsecuritypolicy.policy/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created
</code></pre>
<h4 id="install-calico">Install Calico</h4>
<p><a href="https://projectcalico.docs.tigera.io/getting-started/kubernetes/hardway/">End-to-end Calico installation</a></p>
<h5 id="the-calico-datastore">The Calico Datastore</h5>
<p>In order to use Kubernetes as the Calico datastore, we need to define the custom resources Calico uses.</p>
<p>Download and examine the list of Calico custom resource definitions, and open it in a file editor.</p>
<pre><code>wget https://projectcalico.docs.tigera.io/manifests/crds.yaml
</code></pre>
<p>Create the custom resource definitions in Kubernetes.</p>
<pre><code>kubectl apply -f crds.yaml
</code></pre>
<p>Install <code>calicoctl</code>. To interact directly with the Calico datastore, use the <code>calicoctl</code> client tool.</p>
<p>Download the calicoctl binary to a Linux host with access to Kubernetes. 
The latest release of calicoctl can be found in the <a href="https://github.com/projectcalico/calico/releases">git page</a> and replace below <code>v3.23.2</code> by actual release number.</p>
<pre><code>wget https://github.com/projectcalico/calico/releases/download/v3.23.3/calicoctl-linux-amd64
chmod +x calicoctl-linux-amd64
sudo cp calicoctl-linux-amd64 /usr/local/bin/calicoctl
</code></pre>
<p>Configure calicoctl to access Kubernetes</p>
<pre><code>echo &quot;export KUBECONFIG=/root/.kube/config&quot; &gt;&gt; ~/.bashrc
echo &quot;export DATASTORE_TYPE=kubernetes&quot; &gt;&gt; ~/.bashrc

echo $KUBECONFIG
echo $DATASTORE_TYPE
</code></pre>
<p>Verify <code>calicoctl</code> can reach the datastore by running：</p>
<pre><code>calicoctl get nodes -o wide
</code></pre>
<p>Output similar to below:</p>
<pre><code>NAME     ASN   IPV4   IPV6   
cka001                       
cka002                       
cka003  
</code></pre>
<p>Nodes are backed by the Kubernetes node object, so we should see names that match <code>kubectl get nodes</code>.</p>
<pre><code>kubectl get nodes -o wide
</code></pre>
<pre><code>NAME     STATUS     ROLES                  AGE   VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
cka001   NotReady   control-plane,master   23m   v1.23.8   172.16.18.161   &lt;none&gt;        Ubuntu 20.04.4 LTS   5.4.0-113-generic   containerd://1.5.9
cka002   NotReady   &lt;none&gt;                 22m   v1.23.8   172.16.18.160   &lt;none&gt;        Ubuntu 20.04.4 LTS   5.4.0-113-generic   containerd://1.5.9
cka003   NotReady   &lt;none&gt;                 21m   v1.23.8   172.16.18.159   &lt;none&gt;        Ubuntu 20.04.4 LTS   5.4.0-113-generic   containerd://1.5.9
</code></pre>
<h5 id="configure-ip-pools">Configure IP Pools</h5>
<p>A workload is a container or VM that Calico handles the virtual networking for. 
In Kubernetes, workloads are pods. 
A workload endpoint is the virtual network interface a workload uses to connect to the Calico network.</p>
<p>IP pools are ranges of IP addresses that Calico uses for workload endpoints.</p>
<p>Get current IP pools in the cluster. So far, it's empty after fresh installation.</p>
<pre><code>calicoctl get ippools
</code></pre>
<pre><code>NAME   CIDR   SELECTOR 
</code></pre>
<p>The Pod CIDR is <code>10.244.0.0/16</code> we specified via <code>kubeadm init</code>.</p>
<p>Let's create two IP pools for use in the cluster. Each pool can not have any overlaps.</p>
<ul>
<li>ipv4-ippool-1: <code>10.244.0.0/18</code></li>
<li>ipv4-ippool-2: <code>10.244.192.0/19</code></li>
</ul>
<pre><code>calicoctl apply -f - &lt;&lt;EOF
apiVersion: projectcalico.org/v3
kind: IPPool
metadata:
  name: ipv4-ippool-1
spec:
  cidr: 10.244.0.0/18
  ipipMode: Never
  natOutgoing: true
  disabled: false
  nodeSelector: all()
EOF
</code></pre>
<pre><code>calicoctl apply -f - &lt;&lt;EOF
apiVersion: projectcalico.org/v3
kind: IPPool
metadata:
  name: ipv4-ippool-2
spec:
  cidr: 10.244.192.0/19
  ipipMode: Never
  natOutgoing: true
  disabled: true
  nodeSelector: all()
EOF
</code></pre>
<p>IP pool now looks like below.</p>
<pre><code>calicoctl get ippools -o wide
</code></pre>
<pre><code>NAME            CIDR              NAT    IPIPMODE   VXLANMODE   DISABLED   DISABLEBGPEXPORT   SELECTOR   
ipv4-ippool-1   10.244.0.0/18     true   Never      Never       false      false              all()      
ipv4-ippool-2   10.244.192.0/19   true   Never      Never       true       false              all()     
</code></pre>
<h5 id="install-cni-plugin">Install CNI plugin</h5>
<ul>
<li>Provision Kubernetes user account for the plugin.</li>
</ul>
<p>Kubernetes uses the Container Network Interface (CNI) to interact with networking providers like Calico. 
The Calico binary that presents this API to Kubernetes is called the CNI plugin and must be installed on every node in the Kubernetes cluster.</p>
<p>The CNI plugin interacts with the Kubernetes API server while creating pods, both to obtain additional information and to update the datastore with information about the pod.</p>
<p>On the Kubernetes <em>master</em> node, create a key for the CNI plugin to authenticate with and certificate signing request.</p>
<p>Change to directory <code>/etc/kubernetes/pki/</code>.</p>
<pre><code>cd /etc/kubernetes/pki/
</code></pre>
<pre><code>openssl req -newkey rsa:4096 \
  -keyout cni.key \
  -nodes \
  -out cni.csr \
  -subj &quot;/CN=calico-cni&quot;
</code></pre>
<p>We will sign this certificate using the main Kubernetes CA.</p>
<pre><code>sudo openssl x509 -req -in cni.csr \
  -CA /etc/kubernetes/pki/ca.crt \
  -CAkey /etc/kubernetes/pki/ca.key \
  -CAcreateserial \
  -out cni.crt \
  -days 3650
</code></pre>
<p>Output looks like below. User is <code>calico-cni</code>.</p>
<pre><code>Signature ok
subject=CN = calico-cni
Getting CA Private Key
</code></pre>
<pre><code>sudo chown $(id -u):$(id -g) cni.crt
</code></pre>
<p>Next, we create a kubeconfig file for the CNI plugin to use to access Kubernetes. 
Copy this <code>cni.kubeconfig</code> file to every node in the cluster.</p>
<p>Stay in directory <code>/etc/kubernetes/pki/</code>.</p>
<pre><code>APISERVER=$(kubectl config view -o jsonpath='{.clusters[0].cluster.server}')

echo $APISERVER

kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/pki/ca.crt \
  --embed-certs=true \
  --server=$APISERVER \
  --kubeconfig=cni.kubeconfig

kubectl config set-credentials calico-cni \
  --client-certificate=cni.crt \
  --client-key=cni.key \
  --embed-certs=true \
  --kubeconfig=cni.kubeconfig

kubectl config set-context cni@kubernetes \
  --cluster=kubernetes \
  --user=calico-cni \
  --kubeconfig=cni.kubeconfig

kubectl config use-context cni@kubernetes --kubeconfig=cni.kubeconfig
</code></pre>
<p>The context for CNI looks like below.</p>
<pre><code>kubectl config get-contexts --kubeconfig=/root/.kube/cni.kubeconfig
</code></pre>
<pre><code>CURRENT   NAME             CLUSTER      AUTHINFO     NAMESPACE
*         cni@kubernetes   kubernetes   calico-cni 
</code></pre>
<p>Copy <code>/root/.kube/cni.kubeconfig</code> to directory <code>/root/.kube/</code> in nodes <code>cka002</code> and <code>cka003</code>.</p>
<ul>
<li>Provision RBAC</li>
</ul>
<p>Define a cluster role the CNI plugin will use to access Kubernetes.</p>
<pre><code>kubectl apply -f - &lt;&lt;EOF
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: calico-cni
rules:
  # The CNI plugin needs to get pods, nodes, and namespaces.
  - apiGroups: [&quot;&quot;]
    resources:
      - pods
      - nodes
      - namespaces
    verbs:
      - get
  # The CNI plugin patches pods/status.
  - apiGroups: [&quot;&quot;]
    resources:
      - pods/status
    verbs:
      - patch
 # These permissions are required for Calico CNI to perform IPAM allocations.
  - apiGroups: [&quot;crd.projectcalico.org&quot;]
    resources:
      - blockaffinities
      - ipamblocks
      - ipamhandles
    verbs:
      - get
      - list
      - create
      - update
      - delete
  - apiGroups: [&quot;crd.projectcalico.org&quot;]
    resources:
      - ipamconfigs
      - clusterinformations
      - ippools
    verbs:
      - get
      - list
EOF
</code></pre>
<p>Bind the cluster role to the <code>calico-cni</code> account.</p>
<pre><code>kubectl create clusterrolebinding calico-cni --clusterrole=calico-cni --user=calico-cni
</code></pre>
<ul>
<li>Install the plugin</li>
</ul>
<p>Do these steps on <strong>each node</strong> in your cluster.</p>
<p>Run these commands as <strong>root</strong>.</p>
<pre><code>sudo su
</code></pre>
<p>Install the CNI plugin Binaries. Get right release in the link <code>https://github.com/projectcalico/cni-plugin/releases</code>.</p>
<pre><code>curl -L -o /opt/cni/bin/calico https://github.com/projectcalico/cni-plugin/releases/download/v3.20.5/calico-amd64

chmod 755 /opt/cni/bin/calico

curl -L -o /opt/cni/bin/calico-ipam https://github.com/projectcalico/cni-plugin/releases/download/v3.20.5/calico-ipam-amd64

chmod 755 /opt/cni/bin/calico-ipam
</code></pre>
<p>Create the config directory</p>
<pre><code>mkdir -p /etc/cni/net.d/
</code></pre>
<p>Copy the kubeconfig from the previous section</p>
<pre><code>cp ~/.kube/cni.kubeconfig /etc/cni/net.d/calico-kubeconfig

chmod 600 /etc/cni/net.d/calico-kubeconfig
</code></pre>
<p>Write the CNI configuration</p>
<pre><code>cat &gt; /etc/cni/net.d/10-calico.conflist &lt;&lt;EOF
{
  &quot;name&quot;: &quot;k8s-pod-network&quot;,
  &quot;cniVersion&quot;: &quot;0.3.1&quot;,
  &quot;plugins&quot;: [
    {
      &quot;type&quot;: &quot;calico&quot;,
      &quot;log_level&quot;: &quot;info&quot;,
      &quot;datastore_type&quot;: &quot;kubernetes&quot;,
      &quot;mtu&quot;: 1500,
      &quot;ipam&quot;: {
          &quot;type&quot;: &quot;calico-ipam&quot;
      },
      &quot;policy&quot;: {
          &quot;type&quot;: &quot;k8s&quot;
      },
      &quot;kubernetes&quot;: {
          &quot;kubeconfig&quot;: &quot;/etc/cni/net.d/calico-kubeconfig&quot;
      }
    },
    {
      &quot;type&quot;: &quot;portmap&quot;,
      &quot;snat&quot;: true,
      &quot;capabilities&quot;: {&quot;portMappings&quot;: true}
    }
  ]
}
EOF
</code></pre>
<p>Exit from su and go back to the logged in user.</p>
<pre><code>exit
</code></pre>
<p>At this point Kubernetes nodes will become Ready because Kubernetes has a networking provider and configuration installed.</p>
<pre><code>kubectl get nodes
</code></pre>
<h5 id="install-typha">Install Typha</h5>
<p>Typha sits between the Kubernetes API server and per-node daemons like Felix and confd (running in calico/node). 
It watches the Kubernetes resources and Calico custom resources used by these daemons, and whenever a resource changes it fans out the update to the daemons. 
This reduces the number of watches the Kubernetes API server needs to serve and improves scalability of the cluster.</p>
<ul>
<li>Provision Certificates</li>
</ul>
<p>We will use mutually authenticated TLS to ensure that calico/node and Typha communicate securely. 
We generate a certificate authority (CA) and use it to sign a certificate for Typha.</p>
<p>Stay in directory <code>/etc/kubernetes/pki/</code>.</p>
<p>Create the CA certificate and key</p>
<pre><code>openssl req -x509 -newkey rsa:4096 \
  -keyout typhaca.key \
  -nodes \
  -out typhaca.crt \
  -subj &quot;/CN=Calico Typha CA&quot; \
  -days 365
</code></pre>
<p>Store the CA certificate in a ConfigMap that Typha &amp; calico/node will access.</p>
<pre><code>kubectl create configmap -n kube-system calico-typha-ca --from-file=typhaca.crt
</code></pre>
<p>Create the Typha key and certificate signing request (CSR)</p>
<pre><code>openssl req -newkey rsa:4096 \
  -keyout typha.key \
  -nodes \
  -out typha.csr \
  -subj &quot;/CN=calico-typha&quot;
</code></pre>
<p>The certificate presents the Common Name (CN) as <code>calico-typha</code>. <code>calico/node</code> will be configured to verify this name.</p>
<p>Sign the Typha certificate with the CA.</p>
<pre><code>openssl x509 -req -in typha.csr \
  -CA typhaca.crt \
  -CAkey typhaca.key \
  -CAcreateserial \
  -out typha.crt \
  -days 365
</code></pre>
<p>Store the Typha key and certificate in a secret that Typha will access</p>
<pre><code>kubectl create secret generic -n kube-system calico-typha-certs --from-file=typha.key --from-file=typha.crt
</code></pre>
<ul>
<li>Provision RBAC</li>
</ul>
<p>Create a ServiceAccount that will be used to run Typha.</p>
<pre><code>kubectl create serviceaccount -n kube-system calico-typha
</code></pre>
<p>Define a cluster role for Typha with permission to watch Calico datastore objects.</p>
<pre><code>kubectl apply -f - &lt;&lt;EOF
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: calico-typha
rules:
  - apiGroups: [&quot;&quot;]
    resources:
      - pods
      - namespaces
      - serviceaccounts
      - endpoints
      - services
      - nodes
    verbs:
      # Used to discover service IPs for advertisement.
      - watch
      - list
  - apiGroups: [&quot;networking.k8s.io&quot;]
    resources:
      - networkpolicies
    verbs:
      - watch
      - list
  - apiGroups: [&quot;crd.projectcalico.org&quot;]
    resources:
      - globalfelixconfigs
      - felixconfigurations
      - bgppeers
      - globalbgpconfigs
      - bgpconfigurations
      - ippools
      - ipamblocks
      - globalnetworkpolicies
      - globalnetworksets
      - networkpolicies
      - clusterinformations
      - hostendpoints
      - blockaffinities
      - networksets
    verbs:
      - get
      - list
      - watch
  - apiGroups: [&quot;crd.projectcalico.org&quot;]
    resources:
      #- ippools
      #- felixconfigurations
      - clusterinformations
    verbs:
      - get
      - create
      - update
EOF
</code></pre>
<p>Bind the cluster role to the calico-typha ServiceAccount.</p>
<pre><code>kubectl create clusterrolebinding calico-typha --clusterrole=calico-typha --serviceaccount=kube-system:calico-typha
</code></pre>
<ul>
<li>Install Deployment</li>
</ul>
<p>Since Typha is required by <code>calico/node</code>, and <code>calico/node</code> establishes the pod network, we run Typha as a host networked pod to avoid a chicken-and-egg problem. 
We run 3 replicas of Typha so that even during a rolling update, a single failure does not make Typha unavailable.</p>
<pre><code>kubectl apply -f - &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: calico-typha
  namespace: kube-system
  labels:
    k8s-app: calico-typha
spec:
  replicas: 3
  revisionHistoryLimit: 2
  selector:
    matchLabels:
      k8s-app: calico-typha
  template:
    metadata:
      labels:
        k8s-app: calico-typha
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'
    spec:
      hostNetwork: true
      tolerations:
        # Mark the pod as a critical add-on for rescheduling.
        - key: CriticalAddonsOnly
          operator: Exists
      serviceAccountName: calico-typha
      priorityClassName: system-cluster-critical
      containers:
      - image: calico/typha:v3.8.0
        name: calico-typha
        ports:
        - containerPort: 5473
          name: calico-typha
          protocol: TCP
        env:
          # Disable logging to file and syslog since those don't make sense in Kubernetes.
          - name: TYPHA_LOGFILEPATH
            value: &quot;none&quot;
          - name: TYPHA_LOGSEVERITYSYS
            value: &quot;none&quot;
          # Monitor the Kubernetes API to find the number of running instances and rebalance
          # connections.
          - name: TYPHA_CONNECTIONREBALANCINGMODE
            value: &quot;kubernetes&quot;
          - name: TYPHA_DATASTORETYPE
            value: &quot;kubernetes&quot;
          - name: TYPHA_HEALTHENABLED
            value: &quot;true&quot;
          # Location of the CA bundle Typha uses to authenticate calico/node; volume mount
          - name: TYPHA_CAFILE
            value: /calico-typha-ca/typhaca.crt
          # Common name on the calico/node certificate
          - name: TYPHA_CLIENTCN
            value: calico-node
          # Location of the server certificate for Typha; volume mount
          - name: TYPHA_SERVERCERTFILE
            value: /calico-typha-certs/typha.crt
          # Location of the server certificate key for Typha; volume mount
          - name: TYPHA_SERVERKEYFILE
            value: /calico-typha-certs/typha.key
        livenessProbe:
          httpGet:
            path: /liveness
            port: 9098
            host: localhost
          periodSeconds: 30
          initialDelaySeconds: 30
        readinessProbe:
          httpGet:
            path: /readiness
            port: 9098
            host: localhost
          periodSeconds: 10
        volumeMounts:
        - name: calico-typha-ca
          mountPath: &quot;/calico-typha-ca&quot;
          readOnly: true
        - name: calico-typha-certs
          mountPath: &quot;/calico-typha-certs&quot;
          readOnly: true
      volumes:
      - name: calico-typha-ca
        configMap:
          name: calico-typha-ca
      - name: calico-typha-certs
        secret:
          secretName: calico-typha-certs
EOF
</code></pre>
<p>We set <code>TYPHA_CLIENTCN</code> to calico-node which is the common name we will use on the certificate <code>calico/node</code> will use late.</p>
<p>Verify Typha is up an running with three instances</p>
<pre><code>kubectl get pods -l k8s-app=calico-typha -n kube-system
</code></pre>
<p>Result looks like below.</p>
<pre><code>NAME                           READY   STATUS    RESTARTS   AGE
calico-typha-5b8669646-fsv48   1/1     Running   0          3m16s
calico-typha-5b8669646-ppqtt   0/1     Pending   0          3m16s
calico-typha-5b8669646-tj9r5   1/1     Running   0          3m16s
</code></pre>
<p>Here is an error message received:</p>
<pre><code>0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 node(s) didn't have free ports for the requested pod ports.
</code></pre>
<ul>
<li>Install Service</li>
</ul>
<p><code>calico/node</code> uses a Kubernetes Service to get load-balanced access to Typha.</p>
<pre><code>kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Service
metadata:
  name: calico-typha
  namespace: kube-system
  labels:
    k8s-app: calico-typha
spec:
  ports:
    - port: 5473
      protocol: TCP
      targetPort: calico-typha
      name: calico-typha
  selector:
    k8s-app: calico-typha
EOF
</code></pre>
<p>Validate that Typha is using TLS.</p>
<pre><code>TYPHA_CLUSTERIP=$(kubectl get svc -n kube-system calico-typha -o jsonpath='{.spec.clusterIP}')
curl https://$TYPHA_CLUSTERIP:5473 -v --cacert typhaca.crt
</code></pre>
<p>Result</p>
<pre><code>*   Trying 11.244.203.209:5473...
* TCP_NODELAY set
* Connected to 11.244.203.209 (11.244.203.209) port 5473 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: typhaca.crt
  CApath: /etc/ssl/certs
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Request CERT (13):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Certificate (11):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS alert, bad certificate (554):
* error:14094412:SSL routines:ssl3_read_bytes:sslv3 alert bad certificate
* Closing connection 0
curl: (35) error:14094412:SSL routines:ssl3_read_bytes:sslv3 alert bad certificate
</code></pre>
<p>This demonstrates that Typha is presenting its TLS certificate and rejecting our connection because we do not present a certificate. 
We will later deploy calico/node with a certificate Typha will accept.</p>
<h5 id="install-caliconode">Install calico/node</h5>
<p><code>calico/node</code> runs three daemons:</p>
<ul>
<li>Felix, the Calico per-node daemon</li>
<li>BIRD, a daemon that speaks the BGP protocol to distribute routing information to other nodes</li>
<li>
<p>confd, a daemon that watches the Calico datastore for config changes and updates BIRD’s config files</p>
</li>
<li>
<p>Provision Certificates</p>
</li>
</ul>
<p>Create the key <code>calico/node</code> will use to authenticate with Typha and the certificate signing request (CSR)</p>
<pre><code>openssl req -newkey rsa:4096 \
  -keyout calico-node.key \
  -nodes \
  -out calico-node.csr \
  -subj &quot;/CN=calico-node&quot;
</code></pre>
<p>The certificate presents the Common Name (CN) as <code>calico-node</code>, which is what we configured Typha to accept in the last lab.</p>
<p>Sign the Felix certificate with the CA we created earlier.</p>
<pre><code>openssl x509 -req -in calico-node.csr \
  -CA typhaca.crt \
  -CAkey typhaca.key \
  -CAcreateserial \
  -out calico-node.crt \
  -days 365
</code></pre>
<p>Store the key and certificate in a Secret that calico/node will access.</p>
<pre><code>kubectl create secret generic -n kube-system calico-node-certs --from-file=calico-node.key --from-file=calico-node.crt
</code></pre>
<ul>
<li>Provision RBAC</li>
</ul>
<p>Create the ServiceAccount that calico/node will run as.</p>
<pre><code>kubectl create serviceaccount -n kube-system calico-node
</code></pre>
<p>Provision a cluster role with permissions to read and modify Calico datastore objects</p>
<pre><code>kubectl apply -f - &lt;&lt;EOF
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: calico-node
rules:
  # The CNI plugin needs to get pods, nodes, and namespaces.
  - apiGroups: [&quot;&quot;]
    resources:
      - pods
      - nodes
      - namespaces
    verbs:
      - get
  # EndpointSlices are used for Service-based network policy rule
  # enforcement.
  - apiGroups: [&quot;discovery.k8s.io&quot;]
    resources:
      - endpointslices
    verbs:
      - watch
      - list
  - apiGroups: [&quot;&quot;]
    resources:
      - endpoints
      - services
    verbs:
      # Used to discover service IPs for advertisement.
      - watch
      - list
      # Used to discover Typhas.
      - get
  # Pod CIDR auto-detection on kubeadm needs access to config maps.
  - apiGroups: [&quot;&quot;]
    resources:
      - configmaps
    verbs:
      - get
  - apiGroups: [&quot;&quot;]
    resources:
      - nodes/status
    verbs:
      # Needed for clearing NodeNetworkUnavailable flag.
      - patch
      # Calico stores some configuration information in node annotations.
      - update
  # Watch for changes to Kubernetes NetworkPolicies.
  - apiGroups: [&quot;networking.k8s.io&quot;]
    resources:
      - networkpolicies
    verbs:
      - watch
      - list
  # Used by Calico for policy information.
  - apiGroups: [&quot;&quot;]
    resources:
      - pods
      - namespaces
      - serviceaccounts
    verbs:
      - list
      - watch
  # The CNI plugin patches pods/status.
  - apiGroups: [&quot;&quot;]
    resources:
      - pods/status
    verbs:
      - patch
  # Used for creating service account tokens to be used by the CNI plugin
  - apiGroups: [&quot;&quot;]
    resources:
      - serviceaccounts/token
    resourceNames:
      - calico-node
    verbs:
      - create
  # Calico monitors various CRDs for config.
  - apiGroups: [&quot;crd.projectcalico.org&quot;]
    resources:
      - globalfelixconfigs
      - felixconfigurations
      - bgppeers
      - globalbgpconfigs
      - bgpconfigurations
      - ippools
      - ipamblocks
      - globalnetworkpolicies
      - globalnetworksets
      - networkpolicies
      - networksets
      - clusterinformations
      - hostendpoints
      - blockaffinities
    verbs:
      - get
      - list
      - watch
  # Calico must create and update some CRDs on startup.
  - apiGroups: [&quot;crd.projectcalico.org&quot;]
    resources:
      - ippools
      - felixconfigurations
      - clusterinformations
    verbs:
      - create
      - update
  # Calico stores some configuration information on the node.
  - apiGroups: [&quot;&quot;]
    resources:
      - nodes
    verbs:
      - get
      - list
      - watch
  # These permissions are required for Calico CNI to perform IPAM allocations.
  - apiGroups: [&quot;crd.projectcalico.org&quot;]
    resources:
      - blockaffinities
      - ipamblocks
      - ipamhandles
    verbs:
      - get
      - list
      - create
      - update
      - delete
  - apiGroups: [&quot;crd.projectcalico.org&quot;]
    resources:
      - ipamconfigs
    verbs:
      - get
  # Block affinities must also be watchable by confd for route aggregation.
  - apiGroups: [&quot;crd.projectcalico.org&quot;]
    resources:
      - blockaffinities
    verbs:
      - watch
EOF
</code></pre>
<p>Bind the cluster role to the calico-node ServiceAccount</p>
<pre><code>kubectl create clusterrolebinding calico-node --clusterrole=calico-node --serviceaccount=kube-system:calico-node
</code></pre>
<ul>
<li>Install daemon set</li>
</ul>
<p><code>calico/node</code> runs as a daemon set so that it is installed on every node in the cluster.</p>
<p>Create the daemon set</p>
<pre><code>kubectl apply -f - &lt;&lt;EOF
kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: calico-node
  namespace: kube-system
  labels:
    k8s-app: calico-node
spec:
  selector:
    matchLabels:
      k8s-app: calico-node
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        k8s-app: calico-node
    spec:
      nodeSelector:
        kubernetes.io/os: linux
      hostNetwork: true
      tolerations:
        # Make sure calico-node gets scheduled on all nodes.
        - effect: NoSchedule
          operator: Exists
        # Mark the pod as a critical add-on for rescheduling.
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoExecute
          operator: Exists
      serviceAccountName: calico-node
      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a &quot;force
      # deletion&quot;: https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.
      terminationGracePeriodSeconds: 0
      priorityClassName: system-node-critical
      containers:
        # Runs calico-node container on each Kubernetes node.  This
        # container programs network policy and routes on each
        # host.
        - name: calico-node
          image: calico/node:v3.20.0
          env:
            # Use Kubernetes API as the backing datastore.
            - name: DATASTORE_TYPE
              value: &quot;kubernetes&quot;
            - name: FELIX_TYPHAK8SSERVICENAME
              value: calico-typha
            # Wait for the datastore.
            - name: WAIT_FOR_DATASTORE
              value: &quot;true&quot;
            # Set based on the k8s node name.
            - name: NODENAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            # Choose the backend to use.
            - name: CALICO_NETWORKING_BACKEND
              value: bird
            # Cluster type to identify the deployment type
            - name: CLUSTER_TYPE
              value: &quot;k8s,bgp&quot;
            # Auto-detect the BGP IP address.
            - name: IP
              value: &quot;autodetect&quot;
            # Disable file logging so kubectl logs works.
            - name: CALICO_DISABLE_FILE_LOGGING
              value: &quot;true&quot;
            # Set Felix endpoint to host default action to ACCEPT.
            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
              value: &quot;ACCEPT&quot;
            # Disable IPv6 on Kubernetes.
            - name: FELIX_IPV6SUPPORT
              value: &quot;false&quot;
            # Set Felix logging to &quot;info&quot;
            - name: FELIX_LOGSEVERITYSCREEN
              value: &quot;info&quot;
            - name: FELIX_HEALTHENABLED
              value: &quot;true&quot;
            # Location of the CA bundle Felix uses to authenticate Typha; volume mount
            - name: FELIX_TYPHACAFILE
              value: /calico-typha-ca/typhaca.crt
            # Common name on the Typha certificate; used to verify we are talking to an authentic typha
            - name: FELIX_TYPHACN
              value: calico-typha
            # Location of the client certificate for connecting to Typha; volume mount
            - name: FELIX_TYPHACERTFILE
              value: /calico-node-certs/calico-node.crt
            # Location of the client certificate key for connecting to Typha; volume mount
            - name: FELIX_TYPHAKEYFILE
              value: /calico-node-certs/calico-node.key
          securityContext:
            privileged: true
          resources:
            requests:
              cpu: 250m
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/calico-node
                - -shutdown
          livenessProbe:
            httpGet:
              path: /liveness
              port: 9099
              host: localhost
            periodSeconds: 10
            initialDelaySeconds: 10
            failureThreshold: 6
          readinessProbe:
            exec:
              command:
              - /bin/calico-node
              - -bird-ready
              - -felix-ready
            periodSeconds: 10
          volumeMounts:
            - mountPath: /lib/modules
              name: lib-modules
              readOnly: true
            - mountPath: /run/xtables.lock
              name: xtables-lock
              readOnly: false
            - mountPath: /var/run/calico
              name: var-run-calico
              readOnly: false
            - mountPath: /var/lib/calico
              name: var-lib-calico
              readOnly: false
            - mountPath: /var/run/nodeagent
              name: policysync
            - mountPath: &quot;/calico-typha-ca&quot;
              name: calico-typha-ca
              readOnly: true
            - mountPath: /calico-node-certs
              name: calico-node-certs
              readOnly: true
      volumes:
        # Used by calico-node.
        - name: lib-modules
          hostPath:
            path: /lib/modules
        - name: var-run-calico
          hostPath:
            path: /var/run/calico
        - name: var-lib-calico
          hostPath:
            path: /var/lib/calico
        - name: xtables-lock
          hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
        # Used to create per-pod Unix Domain Sockets
        - name: policysync
          hostPath:
            type: DirectoryOrCreate
            path: /var/run/nodeagent
        - name: calico-typha-ca
          configMap:
            name: calico-typha-ca
        - name: calico-node-certs
          secret:
            secretName: calico-node-certs
EOF
</code></pre>
<p>Verify that calico/node is running on each node in your cluster, and goes to Running within a few minutes.</p>
<pre><code>kubectl get pod -l k8s-app=calico-node -n kube-system
</code></pre>
<p>Result looks like below.</p>
<pre><code>NAME                READY   STATUS    RESTARTS   AGE
calico-node-9577c   1/1     Running   0          3d
calico-node-hpv4n   1/1     Running   0          3d
calico-node-vhtjg   0/1     Running   0          17s
</code></pre>
<p>Install Calico.</p>
<pre><code>curl https://docs.projectcalico.org/manifests/calico.yaml -O

kubectl apply -f calico.yaml
</code></pre>
<p>Output:</p>
<pre><code>configmap/calico-config unchanged
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org configured
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers unchanged
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers unchanged
clusterrole.rbac.authorization.k8s.io/calico-node configured
clusterrolebinding.rbac.authorization.k8s.io/calico-node unchanged
daemonset.apps/calico-node configured
serviceaccount/calico-node unchanged
deployment.apps/calico-kube-controllers configured
serviceaccount/calico-kube-controllers unchanged
poddisruptionbudget.policy/calico-kube-controllers configured
</code></pre>
<p>Verify status of Calico. Make sure all Pods are running</p>
<pre><code>kubectl get pod -n kube-system -o wide | grep calico
</code></pre>
<p>Output:</p>
<pre><code>calico-kube-controllers-5c64b68895-2b96w   1/1     Running   2 (2m4s ago)   2m58s   10.244.102.1     cka003   &lt;none&gt;           &lt;none&gt;
calico-node-l42kd                          1/1     Running   0              119s    172.16.18.160    cka002   &lt;none&gt;           &lt;none&gt;
calico-node-ndrnw                          1/1     Running   0              3m      172.16.18.159    cka003   &lt;none&gt;           &lt;none&gt;
calico-node-t64vq                          1/1     Running   0              69s     172.16.18.161    cka001   &lt;none&gt;           &lt;none&gt;
calico-typha-5b8669646-fsv48               1/1     Running   0              17m     172.16.18.160    cka002   &lt;none&gt;           &lt;none&gt;
calico-typha-5b8669646-ppqtt               0/1     Pending   0              17m     &lt;none&gt;           &lt;none&gt;   &lt;none&gt;           &lt;none&gt;
calico-typha-5b8669646-tj9r5               1/1     Running   0              17m     172.16.18.159    cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<h5 id="test-networking">Test networking</h5>
<ul>
<li>Pod to pod pings</li>
</ul>
<p>Create three busybox instances</p>
<pre><code>kubectl create deployment pingtest --image=busybox --replicas=3 -- sleep infinity
</code></pre>
<p>Check their IP addresses</p>
<pre><code>kubectl get pods --selector=app=pingtest --output=wide
</code></pre>
<p>Result</p>
<pre><code>NAME                        READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
pingtest-585b76c894-sfks2   1/1     Running   0          9s    10.244.112.2   cka002   &lt;none&gt;           &lt;none&gt;
pingtest-585b76c894-xgskp   1/1     Running   0          9s    10.244.112.1   cka002   &lt;none&gt;           &lt;none&gt;
pingtest-585b76c894-zjjgh   1/1     Running   0          9s    10.244.102.2   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Note the IP addresses of the second two pods, then exec into the first one. For example</p>
<pre><code>kubectl exec -ti pingtest-585b76c894-sfks2 -- sh
</code></pre>
<p>From inside the pod, ping the other two pod IP addresses. For example</p>
<pre><code>ping 10.244.112.1 -c 4
</code></pre>
<ul>
<li>Check routes</li>
</ul>
<p>From one of the nodes, verify that routes exist to each of the pingtest pods’ IP addresses. For example</p>
<pre><code>ip route get 10.244.102.2
</code></pre>
<p>Result</p>
<pre><code>10.244.102.2 via 169.254.1.1 dev eth0  src 10.244.112.2 
</code></pre>
<p>The via <code>169.254.1.1</code> in this example indicates the next-hop for this pod IP, which matches the IP address of the node the pod is scheduled on, as expected.
IPAM allocations from different pools</p>
<p>Recall that we created two IP pools, but left one disabled.</p>
<pre><code>calicoctl get ippools -o wide
</code></pre>
<p>Result</p>
<pre><code>NAME                  CIDR              NAT    IPIPMODE   VXLANMODE   DISABLED   DISABLEBGPEXPORT   SELECTOR   
default-ipv4-ippool   10.244.0.0/16     true   Always     Never       false      false              all()      
new-ipv4-ippool       10.245.192.0/24   true   Never      Never       true       false              all() 
</code></pre>
<p>Enable the second pool.</p>
<pre><code>calicoctl apply -f - &lt;&lt;EOF
apiVersion: projectcalico.org/v3
kind: IPPool
metadata:
  name: new-ipv4-ippool
spec:
  cidr: 10.245.192.0/24
  ipipMode: Never
  natOutgoing: true
  disabled: false
  nodeSelector: all()
EOF
</code></pre>
<p>Create a pod, explicitly requesting an address from pool2</p>
<pre><code>kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: pingtest-pool2
  annotations:
    cni.projectcalico.org/ipv4pools: &quot;[\&quot;pool2\&quot;]&quot;
spec:
  containers:
  - args:
    - sleep
    - infinity
    image: busybox
    imagePullPolicy: Always
    name: pingtest
EOF
</code></pre>
<p>Verify it has an IP address from pool2</p>
<pre><code>kubectl get pod pingtest-pool2 -o wide
</code></pre>
<p>Result</p>
<pre><code>NAME             READY   STATUS              RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
pingtest-pool2   0/1     ContainerCreating   0          9s    &lt;none&gt;   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>From one of the original pingtest pods, ping the IP address.</p>
<pre><code>ping 192.168.219.0 -c 4
</code></pre>
<p>Clean up</p>
<pre><code>kubectl delete deployments.apps pingtest
kubectl delete pod pingtest-pool2
</code></pre>
<h3 id="check-cluster-status">Check Cluster Status</h3>
<p>Perform <code>kubectl cluster-info</code> command on master node we will get below information.</p>
<ul>
<li>Kubernetes control plane is running at https://<mster node ip>:6443</li>
<li>CoreDNS is running at https://<mster node ip>:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</li>
</ul>
<pre><code># kubectl cluster-info
# kubectl get nodes -owide
# kubectl get pod -A
</code></pre>
<h3 id="reset-cluster">Reset cluster</h3>
<p>CAUTION: below steps will destroy current cluster. </p>
<p>Delete all nodes in the cluster.</p>
<pre><code>kubeadm reset
</code></pre>
<p>Output:</p>
<pre><code>[reset] Reading configuration from the cluster...
[reset] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0717 08:15:17.411992 3913615 preflight.go:55] [reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.
[reset] Are you sure you want to proceed? [y/N]: y
[preflight] Running pre-flight checks
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in &quot;/var/lib/kubelet&quot;
[reset] Deleting contents of directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]
[reset] Deleting contents of stateful directories: [/var/lib/etcd /var/lib/kubelet /var/lib/dockershim /var/run/kubernetes /var/lib/cni]

The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must do so manually by using the &quot;iptables&quot; command.

If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
to reset your system's IPVS tables.

The reset process does not clean your kubeconfig files and you must remove them manually.
Please, check the contents of the $HOME/.kube/config file.
</code></pre>
<p>Clean up network setting</p>
<pre><code>rm -rf /var/run/flannel /opt/cni /etc/cni /var/lib/cni
</code></pre>
<p>Clean up rule of <code>iptables</code>.</p>
<pre><code>iptables -F &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t mangle -F &amp;&amp; iptables -X
</code></pre>
<p>Clean up rule of <code>IPVS</code> if using <code>IPVS</code>.</p>
<pre><code>ipvsadm --clear
</code></pre>
<h3 id="troubleshooting">Troubleshooting</h3>
<h4 id="issue-1">Issue 1</h4>
<p>The connection to the server <master>:6443 was refused - did you specify the right host or port?</p>
<p><strong>Try</strong>:</p>
<p><a href="https://discuss.kubernetes.io/t/the-connection-to-the-server-host-6443-was-refused-did-you-specify-the-right-host-or-port/552/15">Reference</a></p>
<p>Check environment setting.</p>
<pre><code>env | grep -i kub
</code></pre>
<p>Check container status.</p>
<pre><code>sudo systemctl status containerd.service 
</code></pre>
<p>Check kubelet service.</p>
<pre><code>sudo systemctl status kubelet.service 
</code></pre>
<p>Check port listening status.</p>
<pre><code>netstat -pnlt | grep 6443
</code></pre>
<p>Check firewall status.</p>
<pre><code>sudo systemctl status firewalld.service
</code></pre>
<p>Check log.</p>
<pre><code>journalctl -xeu kubelet
</code></pre>
<h4 id="issue-2">Issue 2</h4>
<p>"Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"</p>
<p><strong>Try</strong>:</p>
<p>Restart Containerd service.</p>
<pre><code>sudo systemctl restart containerd
sudo systemctl status containerd
</code></pre>
<p>Till now, the initial deployment is completed sucessfully.</p>
<h2 id="2post-installation">2.Post Installation</h2>
<h2 id="3cluster-overview">3.Cluster Overview</h2>
<h3 id="container">Container</h3>
<p>We are using Containerd service to manage our images and containers via command <code>nerdctl</code>, which is same concept with Docker.</p>
<p>Tasks:</p>
<ul>
<li>Get container's namespace.</li>
<li>Get container's status.</li>
</ul>
<p>Get current namespaces.</p>
<pre><code>nerdctl namespace ls
</code></pre>
<pre><code>NAME       CONTAINERS    IMAGES    VOLUMES    LABELS
default    1             1         0              
k8s.io     20            60        0      
</code></pre>
<p>Get containers under specific namespace with <code>-n</code> option.</p>
<pre><code>nerdctl -n k8s.io ps
</code></pre>
<pre><code>CONTAINER ID    IMAGE                                                                      COMMAND                   CREATED       STATUS    PORTS    NAMES
06d6c23a4d38    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  2 days ago    Up                 k8s://kube-system/kube-apiserver-cka001                                               
086f9192513d    registry.aliyuncs.com/google_containers/kube-controller-manager:v1.23.8    &quot;kube-controller-man…&quot;    2 days ago    Up                 k8s://kube-system/kube-controller-manager-cka001/kube-controller-manager              
0923a733ee1e    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  2 days ago    Up                 k8s://kube-system/coredns-6d8c4cb4d-z5cbb                                             
0d6d7cea48ae    registry.aliyuncs.com/google_containers/coredns:v1.8.6                     &quot;/coredns -conf /etc…&quot;    2 days ago    Up                 k8s://kube-system/coredns-6d8c4cb4d-z5cbb/coredns                                     
43fe1ef0aac2    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  2 days ago    Up                 k8s://kube-system/kube-proxy-rzmpb                                                    
454abe460028    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  2 days ago    Up                 k8s://kube-system/coredns-6d8c4cb4d-kv98s                                             
508464b96dcf    registry.aliyuncs.com/google_containers/etcd:3.5.1-0                       &quot;etcd --advertise-cl…&quot;    2 days ago    Up                 k8s://kube-system/etcd-cka001/etcd                                                    
535f7cf7f001    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  2 days ago    Up                 k8s://kube-system/kube-controller-manager-cka001                                      
5434068a0358    registry.aliyuncs.com/google_containers/kube-apiserver:v1.23.8             &quot;kube-apiserver --ad…&quot;    2 days ago    Up                 k8s://kube-system/kube-apiserver-cka001/kube-apiserver                                
67c0b63b50e6    registry.aliyuncs.com/google_containers/kube-scheduler:v1.23.8             &quot;kube-scheduler --au…&quot;    2 days ago    Up                 k8s://kube-system/kube-scheduler-cka001/kube-scheduler                                
71afea5a6fb5    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  2 days ago    Up                 k8s://kube-system/etcd-cka001                                                         
874a0e2798aa    registry.aliyuncs.com/google_containers/kube-proxy:v1.23.8                 &quot;/usr/local/bin/kube…&quot;    2 days ago    Up                 k8s://kube-system/kube-proxy-rzmpb/kube-proxy                                         
c0bdd2f73da7    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  2 days ago    Up                 k8s://kube-system/kube-scheduler-cka001                                               
cb32b00cb43b    registry.aliyuncs.com/google_containers/coredns:v1.8.6                     &quot;/coredns -conf /etc…&quot;    2 days ago    Up                 k8s://kube-system/coredns-6d8c4cb4d-kv98s/coredns                                     
d57a2071270a    docker.io/calico/node:v3.23.2                                              &quot;start_runit&quot;             2 days ago    Up                 k8s://kube-system/calico-node-9577c/calico-node                                       
d74d2d4c6a01    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  2 days ago    Up                 k8s://kube-system/calico-node-9577c                                                   
ebfebd851a31    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  2 days ago    Up                 k8s://kube-system/calico-kube-controllers-7bc6547ffb-sf5xf                            
f5a876bc5e3f    docker.io/calico/kube-controllers:v3.23.2                                  &quot;/usr/bin/kube-contr…&quot;    2 days ago    Up                 k8s://kube-system/calico-kube-controllers-7bc6547ffb-sf5xf/calico-kube-controllers    
</code></pre>
<pre><code>nerdctl -n default ps
</code></pre>
<pre><code>CONTAINER ID    IMAGE    COMMAND    CREATED    STATUS    PORTS    NAMES
</code></pre>
<p>Some management and commands options of <code>nertctl</code>.</p>
<pre><code>nertctl --help
nerdctl image ls -a
nerdctl volume ls
nerdctl stats
</code></pre>
<p>Get below network list with command <code>nerdctl network ls</code> in Containerd layer.</p>
<pre><code>nerdctl network ls
</code></pre>
<pre><code>NETWORK ID    NAME      FILE
              cbr0      /etc/cni/net.d/10-flannel.conflist
0             bridge    /etc/cni/net.d/nerdctl-bridge.conflist
              host      
              none  
</code></pre>
<p>Get network interface in host <code>cka001</code> with command <code>ip addr list</code>.</p>
<pre><code>lo               : inet 127.0.0.1/8 qlen 1000
eth0             : inet 172.16.18.161/24 brd 172.16.18.255 qlen 1000
flannel.1        : inet 10.244.0.0/32
cni0             : inet 10.244.0.1/24 brd 10.244.0.255 qlen 1000
vethb0a35696@if3 : noqueue master cni0
veth72791f64@if3 : noqueue master cni0
</code></pre>
<h3 id="kubernetes-layer">Kubernetes Layer</h3>
<p>Kubernetes is beyond container layer above. </p>
<p>In Kubernetes layer, we have three nodes, <code>cka001</code>, <code>cka002</code>, and <code>cka003</code>.</p>
<pre><code>root@cka001:~# kubectl get node
NAME     STATUS   ROLES                  AGE   VERSION
cka001   Ready    control-plane,master   27h   v1.23.8
cka002   Ready    &lt;none&gt;                 27h   v1.23.8
cka003   Ready    &lt;none&gt;                 27h   v1.23.8
</code></pre>
<p>We have four initial namespaces across three nodes.</p>
<pre><code>root@cka001:~# kubectl get namespace -A
NAME              STATUS   AGE
default           Active   27h
kube-node-lease   Active   27h
kube-public       Active   27h
kube-system       Active   27h
</code></pre>
<p>We have some initial pods. </p>
<pre><code>root@cka001:~# kubectl get pod -A -o wide
NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES
kube-system   coredns-6d8c4cb4d-9khd8          1/1     Running   0          27h   &lt;cni0 IP&gt;       cka001   &lt;none&gt;           &lt;none&gt;
kube-system   coredns-6d8c4cb4d-qcp2l          1/1     Running   0          27h   &lt;cni0 IP&gt;       cka001   &lt;none&gt;           &lt;none&gt;
kube-system   etcd-cka001                      1/1     Running   0          27h   &lt;eth0 IP&gt;       cka001   &lt;none&gt;           &lt;none&gt;
kube-system   kube-apiserver-cka001            1/1     Running   0          27h   &lt;eth0 IP&gt;       cka001   &lt;none&gt;           &lt;none&gt;
kube-system   kube-controller-manager-cka001   1/1     Running   0          27h   &lt;eth0 IP&gt;       cka001   &lt;none&gt;           &lt;none&gt;
kube-system   kube-flannel-ds-hfvf7            1/1     Running   0          27h   &lt;eth0 IP&gt;       cka003   &lt;none&gt;           &lt;none&gt;
kube-system   kube-flannel-ds-m5mdl            1/1     Running   0          27h   &lt;eth0 IP&gt;       cka002   &lt;none&gt;           &lt;none&gt;
kube-system   kube-flannel-ds-rf54c            1/1     Running   0          27h   &lt;eth0 IP&gt;       cka001   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-bj75j                 1/1     Running   0          27h   &lt;eth0 IP&gt;       cka002   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-gxjj4                 1/1     Running   0          27h   &lt;eth0 IP&gt;       cka003   &lt;none&gt;           &lt;none&gt;
kube-system   kube-proxy-v7rsr                 1/1     Running   0          27h   &lt;eth0 IP&gt;       cka001   &lt;none&gt;           &lt;none&gt;
kube-system   kube-scheduler-cka001            1/1     Running   0          27h   &lt;eth0 IP&gt;       cka001   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Summary below shows the relationship between containers and pods. 
Good references about container pause: <a href="https://zhuanlan.zhihu.com/p/464712164">article</a> and <a href="https://cloud.tencent.com/developer/article/1583919">artical</a>.</p>
<ul>
<li>Master node:<ul>
<li>CoreDNS: 2 pods, 2 containers of each pod<ul>
<li>From image <code>coredns:v1.8.6</code>:<ul>
<li>k8s://kube-system/coredns-6d8c4cb4d-9khd8/coredns</li>
<li>k8s://kube-system/coredns-6d8c4cb4d-qcp2l/coredns</li>
</ul>
</li>
<li>By image <code>pause:3.6</code><ul>
<li>k8s://kube-system/coredns-6d8c4cb4d-9khd8</li>
<li>k8s://kube-system/coredns-6d8c4cb4d-qcp2l</li>
</ul>
</li>
</ul>
</li>
<li>etcd: 1 pod, 2 containers<ul>
<li>By image <code>etcd:3.5.1-0</code><ul>
<li>k8s://kube-system/etcd-cka001/etcd</li>
</ul>
</li>
<li>By image <code>pause:3.6</code><ul>
<li>k8s://kube-system/etcd-cka001</li>
</ul>
</li>
</ul>
</li>
<li>apiserver: 1 pod, 2 containers<ul>
<li>By image <code>kube-apiserver:v1.23.8</code><ul>
<li>k8s://kube-system/kube-apiserver-cka001/kube-apiserver</li>
</ul>
</li>
<li>By image <code>pause:3.6</code><ul>
<li>k8s://kube-system/kube-apiserver-cka001</li>
</ul>
</li>
</ul>
</li>
<li>controller-manager: 1 pod, 2 containers<ul>
<li>By image <code>kube-controller-manager:v1.23.8</code><ul>
<li>k8s://kube-system/kube-controller-manager-cka001/kube-controller-manager</li>
</ul>
</li>
<li>By image <code>pause:3.6</code><ul>
<li>k8s://kube-system/kube-controller-manager-cka001</li>
</ul>
</li>
</ul>
</li>
<li>scheduler: 1 pod, 2 containers<ul>
<li>By image <code>kube-scheduler:v1.23.8</code><ul>
<li>k8s://kube-system/kube-scheduler-cka001/kube-scheduler</li>
</ul>
</li>
<li>By image <code>pause:3.6</code><ul>
<li>k8s://kube-system/kube-scheduler-cka001</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>All nodes:<ul>
<li>Flannel DS: 1 pod of each, 2 containers of each pod<ul>
<li>By image <code>mirrored-flannelcni-flannel:v0.18.1</code><ul>
<li>k8s://kube-system/kube-flannel-ds-rf54c/kube-flannel</li>
</ul>
</li>
<li>By image <code>pause:3.6</code><ul>
<li>k8s://kube-system/kube-flannel-ds-rf54c</li>
</ul>
</li>
</ul>
</li>
<li>Proxy: 1 pod of each, 2 containers of each pod<ul>
<li>By image <code>kube-proxy:v1.23.8</code><ul>
<li>k8s://kube-system/kube-proxy-v7rsr/kube-proxy</li>
</ul>
</li>
<li>By image <code>pause:3.6</code><ul>
<li>k8s://kube-system/kube-proxy-v7rsr</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Let's check current configuration context of Kubernetes we just initialized. </p>
<ul>
<li>Contenxt name is <code>kubernetes-admin@kubernetes</code>.</li>
<li>Cluster name is <code>kubernetes</code>.</li>
<li>User is <code>kubernetes-admin</code>.</li>
<li>No namespace explicitly defined.</li>
</ul>
<pre><code>root@cka001:~# kubectl config get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
*         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin 
</code></pre>
<p>Create a new namespace <code>jh-namespace</code>.</p>
<pre><code>root@cka001:~# kubectl create namespace jh-namespace
</code></pre>
<p>Update current context <code>kubernetes-admin@kubernetes</code> with new namespace <code>jh-namespace</code> as default namespace. </p>
<pre><code>root@cka001:~# kubectl config set-context kubernetes-admin@kubernetes --cluster=kubernetes --namespace=jh-namespace --user=kubernetes-admin 
</code></pre>
<p>Now default namespace is shown in current configuration context. </p>
<pre><code>root@cka001:~# kubectl config get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
*         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   jh-namespace
</code></pre>
<p>Let's execute command <code>kubectl apply -f 02-sample-pod.yaml</code> to create a pod <code>my-first-pod</code> on namespace <code>jh-namespace</code> with below content of file <code>02-sample-pod.yaml</code>.</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: my-first-pod
spec:
  containers:
  - name: nginx
    image: nginx:mainline
    ports:
    - containerPort: 80
</code></pre>
<p>By command <code>kubectl get pod -o wide</code> we get the pod status.</p>
<p>The pod's ip is allocated by <code>cni0</code>. Node is assigned by <code>Scheduler</code>. </p>
<p>We can also find related containers of pod <code>my-first-pod</code> via command <code>nerdctl -n k8s.io container ls</code> on <code>cka003</code>.</p>
<pre><code>root@cka001:~# kubectl get pod -o wide
NAME           READY   STATUS    RESTARTS   AGE   IP           NODE     NOMINATED NODE   READINESS GATES
my-first-pod   1/1     Running   0          19s   10.244.2.2   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<h3 id="case-study">Case Study</h3>
<p>Scenario: stop kubelet service on worker node <code>cka003</code>.</p>
<p>Question:</p>
<ul>
<li>What's the status of each node?</li>
<li>What's containers changed via command <code>nerdctl</code>?</li>
<li>What's pods status via command <code>kubectl get pod -owide -A</code>? </li>
</ul>
<p>Demo:</p>
<p>Execute command <code>systemctl stop kubelet.service</code> on <code>cka003</code>.</p>
<p>Execute command <code>kubectl get node</code> on either <code>cka001</code> or <code>cka003</code>, the status of <code>cka003</code> is <code>NotReady</code>.</p>
<p>Execute command <code>nerdctl -n k8s.io container ls</code> on <code>cka003</code> and we can observe all containers are still up and running, including the pod <code>my-first-pod</code>.</p>
<p>Execute command <code>systemctl start kubelet.service</code> on <code>cka003</code>.</p>
<p>Conclusion:</p>
<ul>
<li>The node status is changed to <code>NotReady</code> from <code>Ready</code>.</li>
<li>For those DaemonSet pods, like <code>flannel</code>、<code>kube-proxy</code>, are exclusively running on each node. They won't be terminated after <code>kubelet</code> is down.</li>
<li>The status of pod <code>my-first-pod</code> keeps showing <code>Terminating</code> on each node because status can not be synced to other nodes via <code>apiserver</code> from <code>cka003</code> because <code>kubelet</code> is down.</li>
<li>The status of pod is marked by <code>controller</code> and recycled by <code>kubelet</code>.</li>
<li>When we start kubelet service on <code>cka003</code>, the pod <code>my-first-pod</code> will be termiated completely on <code>cka003</code>.</li>
</ul>
<p>In addition, let's create a deployment with 3 replicas. Two are running on <code>cka003</code> and one is running on <code>cka002</code>.</p>
<pre><code>root@cka001:~# kubectl get pod -o wide -w
NAME                               READY   STATUS    RESTARTS   AGE    IP           NODE     NOMINATED NODE   READINESS GATES
nginx-deployment-9d745469b-2xdk4   1/1     Running   0          2m8s   10.244.2.3   cka003   &lt;none&gt;           &lt;none&gt;
nginx-deployment-9d745469b-4gvmr   1/1     Running   0          2m8s   10.244.2.4   cka003   &lt;none&gt;           &lt;none&gt;
nginx-deployment-9d745469b-5j927   1/1     Running   0          2m8s   10.244.1.3   cka002   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>After we stop kubelet service on <code>cka003</code>, the two running on <code>cka003</code> are terminated and another two are created and running on <code>cka002</code> automatically. </p>
<h2 id="4kubectl">4.kubectl</h2>
<p>Three approach to operate Kubernetes cluster:</p>
<ul>
<li>via <a href="https://kubernetes.io/docs/reference/kubernetes-api/">API</a></li>
<li>via kubectl</li>
<li>via Dashboard</li>
</ul>
<h3 id="config-file">Config File</h3>
<p>Kubernetes provides a command line tool <code>kubectl</code> for communicating with a Kubernetes cluster's control plane, using the Kubernetes API.</p>
<p>kubectl controls the Kubernetes <em>cluster manager</em>.</p>
<p>For configuration, kubectl looks for a file named config in the <code>$HOME/.kube</code> directory, which is a copy of file <code>/etc/kubernetes/admin.conf</code> generated by <code>kubeadm init</code>. </p>
<p>We can specify other kubeconfig files by setting the <code>KUBECONFIG</code> environment variable or by setting the <code>--kubeconfig flag</code>.  If the <code>KUBECONFIG</code> environment variable doesn't exist, kubectl uses the default kubeconfig file, <code>$HOME/.kube/config</code>.</p>
<p>A <em>context</em> element in a kubeconfig file is used to group access parameters under a convenient name. Each context has three parameters: cluster, namespace, and user. By default, the kubectl command-line tool uses parameters from the current context to communicate with the cluster.</p>
<p>A sample of <code>.kube/config</code>.</p>
<pre><code>apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: &lt;certificate string&gt;
    server: https://&lt;eth0 ip&gt;:6443
  name: &lt;cluster name&gt;
contexts:
- context:
    cluster: &lt;cluster name&gt;
    namespace: &lt;namespace name&gt;
    user: &lt;user name&gt;
  name: &lt;context user&gt;@&lt;context name&gt;
current-context: &lt;context name&gt;
kind: Config
preferences: {}
users:
- name: &lt;user name&gt;
  user:
    client-certificate-data: &lt;certificate string&gt;
    client-key-data: &lt;certificate string&gt;
</code></pre>
<p>To get the current context:</p>
<pre><code>root@cka001:~# kubectl config get-contexts
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
*         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   jh-namespace
</code></pre>
<p>To set a context with new update, e.g, update default namespace, etc..</p>
<pre><code>kubectl config set-context &lt;context name&gt; --cluster=&lt;cluster name&gt; --namespace=&lt;namespace name&gt; --user=&lt;user name&gt; 
</code></pre>
<p>To use a new context.</p>
<pre><code>kubectl config use-contexts &lt;context name&gt;
</code></pre>
<p>Reference of <a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/">kubectl</a> and <a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands">commandline</a>. </p>
<h3 id="bash-autocomplete">Bash Autocomplete</h3>
<p>Use TAB for bash auto-completion.</p>
<p>Ubuntu：</p>
<pre><code>apt install -y bash-completion
source /usr/share/bash-completion/bash_completion
echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~/.bashrc
source &lt;(kubectl completion bash)
</code></pre>
<h3 id="common-usage">Common Usage</h3>
<p>Get cluster status.</p>
<pre><code># kubectl cluster-info
# kubectl cluster-info dump
</code></pre>
<p>Get health status of control plane.</p>
<pre><code># kubectl get componentstatuses
# kubectl get cs
</code></pre>
<p>Get node status.</p>
<pre><code>kubectl get nodes
kubectl get nodes -o wide
</code></pre>
<p>Update or get node lable.</p>
<pre><code># Update node label
kubectl label node cka002 node=demonode

# Get node info with label info
kubectl get nodes –show-labels

# Search node by label
kubectl get node -l node=demonode
</code></pre>
<p>Create a deployment, option <code>--image</code> specifies a image，option <code>--port</code> specifies port for external access. A pod is also created when deployment is created.</p>
<pre><code>kubectl create deployment myapp --image=nginx --replicas=1 --port=80
kubectl get deployment myapp -o wide
</code></pre>
<p>Get detail information of pod.</p>
<pre><code>kubectl describe pod &lt;pod name&gt;
</code></pre>
<p>Get detail information of deployment.</p>
<pre><code>kubectl describe deployment &lt;deployment&gt;
</code></pre>
<p>Get resources under a namespace or all namespace.</p>
<pre><code>kubectl get namespace
kubectl get pod -n &lt;namespace name&gt;
kubectl get pod --all-namespaces
kubectl get pod -A

kubectl get deployment -n &lt;namespace name&gt;
kubectl get deployment --all-namespaces
kubectl get deployment -A
</code></pre>
<p>By default, pod can only be internally accessed within cluster. 
We can map pod port to node port for external access by exposing a pod, e.g., browser <code>http://&lt;node_ip&gt;:&lt;port_number&gt;</code>.</p>
<pre><code># Expose myapp as service to node port 80.
kubectl expose deployment myapp --type=NodePort --port=80

# Get service
kubectl get service
kubectl get svc -o wide
</code></pre>
<p>Scale out by replicaset. We set three replicasets to scale out deployment <code>myapp</code>. The number of deployment <code>myapp</code> is now three.</p>
<pre><code># Scale out deployment
kubectl scale deployment myapp --replicas=3

# Get status of deployment
kubectl get deployment myapp

# Get status of replicaset
kubectl get replicaset
</code></pre>
<p>Rolling update.</p>
<p>Command usage: <code>kubectl set image (-f FILENAME | TYPE NAME) CONTAINER_NAME_1=CONTAINER_IMAGE_1 ... CONTAINER_NAME_N=CONTAINER_IMAGE_N</code>.</p>
<p>With the command <code>kubectl get deployment</code>, we will get deployment name <code>myapp</code> and related container name <code>nginx</code>.</p>
<pre><code>kubectl get deployment myapp -o wide
</code></pre>
<p>With the command <code>kubectl set image</code> to update image nginx from <code>nginx</code> to <code>nginx:1.19</code> and log the change under deployment's annotations with option <code>--record</code>.</p>
<pre><code>kubectl set image deployment myapp nginx=nginx:1.19 --record
</code></pre>
<p>By the command <code>kubectl set image</code>, all pods are running under new replicaset <code>myapp-b997fb85f</code> with new image version <code>nginx:1.19</code>.</p>
<pre><code>root@cka001:~# kubectl get replicaset -o wide -l app=myapp
NAME              DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES       SELECTOR
myapp-948688ff6   0         0         0       80m     nginx        nginx        app=myapp,pod-template-hash=948688ff6
myapp-b997fb85f   3         3         3       6m29s   nginx        nginx:1.19   app=myapp,pod-template-hash=b997fb85f
</code></pre>
<p>We can get the change history under <code>metadata.annotations</code> by command <code>kubectl get deployment -o yaml</code>.</p>
<pre><code>root@cka001:~# kubectl get deployment myapp -o yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: &quot;2&quot;
    kubernetes.io/change-cause: kubectl set image deployment myapp nginx=nginx:1.19
      --record=true
  creationTimestamp: &quot;2022-06-28T06:33:14Z&quot;
</code></pre>
<p>We can also get the change history by command <code>kubectl rollout history</code>, and show details with specific revision <code>--revision=&lt;revision_number&gt;</code>.</p>
<pre><code>root@cka001:~# kubectl rollout history deployment/myapp
deployment.apps/myapp 
REVISION  CHANGE-CAUSE
1         kubectl set image deployment/myapp myapp=nginx:1.19 --record=true
2         kubectl set image deployment myapp nginx=nginx:1.19 --record=true

root@cka001:~# kubectl rollout history deployment/myapp --revision=1
deployment.apps/myapp with revision #1
Pod Template:
  Labels:       app=myapp
        pod-template-hash=948688ff6
  Annotations:  kubernetes.io/change-cause: kubectl set image deployment/myapp myapp=nginx:1.19 --record=true
  Containers:
   nginx:
    Image:      nginx
    Port:       80/TCP
    Host Port:  0/TCP
    Environment:        &lt;none&gt;
    Mounts:     &lt;none&gt;
  Volumes:      &lt;none&gt;


root@cka001:~# kubectl rollout history deployment/myapp --revision=2
deployment.apps/myapp with revision #2
Pod Template:
  Labels:       app=myapp
        pod-template-hash=b997fb85f
  Annotations:  kubernetes.io/change-cause: kubectl set image deployment myapp nginx=nginx:1.19 --record=true
  Containers:
   nginx:
    Image:      nginx:1.19
    Port:       80/TCP
    Host Port:  0/TCP
    Environment:        &lt;none&gt;
    Mounts:     &lt;none&gt;
  Volumes:      &lt;none&gt;
</code></pre>
<p>Roll back to previous revision with command <code>kubectl rollout undo</code>, or roll back to specific revision with option <code>--to-revision=&lt;revision_number&gt;</code>.</p>
<pre><code># kubectl rollout undo deployment/myapp --to-revision=1
</code></pre>
<p>Get system event information.</p>
<pre><code>kubectl describe pod &lt;pod_name&gt; -n &lt;namespace_name&gt;
</code></pre>
<p>Get the logs for a container in a pod or specified resource. If the pod has only one container, the container name is optional.</p>
<h2 id="5kubernetes-api-and-resource">5.Kubernetes API and Resource</h2>
<h3 id="static-pod">Static Pod</h3>
<p>Create yaml file in directory <code>/etc/kubernetes/manifests/</code>.
<code>kubectl</code> will automatically check yaml file in <code>/etc/kubernetes/manifests/</code> and create the static pod once it's detected.</p>
<pre><code>root@cka001:~# kubectl run nginx --image=nginx:mainline --dry-run=client -n jh-namespace -oyaml &gt; /etc/kubernetes/manifests/my-nginx.yaml

root@cka001:~# kubectl get pod
NAME           READY   STATUS    RESTARTS   AGE
nginx-cka001   1/1     Running   0          6s
</code></pre>
<p>Delete the yaml file <code>/etc/kubernetes/manifests/my-nginx.yaml</code>, the static pod will be deleted automatically.</p>
<pre><code>root@cka001:~# rm /etc/kubernetes/manifests/my-nginx.yaml 
</code></pre>
<h3 id="init-containers">Init containers</h3>
<p>This example defines a simple Pod that has two init containers in <code>02-init-pod.yaml</code>. 
The first waits for myservice, and the second waits for mydb. 
Once both init containers complete, the Pod runs the app container from its spec section.</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', &quot;until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done&quot;]
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', &quot;until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done&quot;]
</code></pre>
<p>Create the Pod <code>myapp-pod</code>.</p>
<pre><code>kubectl apply -f 02-init-pod.yaml
</code></pre>
<p>Check Pod status.</p>
<pre><code># kubectl get pod myapp-pod
NAME        READY   STATUS     RESTARTS   AGE
myapp-pod   0/1     Init:0/2   0          12m
</code></pre>
<p>Inspect Pods.</p>
<pre><code>kubectl logs myapp-pod -c init-myservice # Inspect the first init container
kubectl logs myapp-pod -c init-mydb      # Inspect the second init container
</code></pre>
<p>At this point, those init containers will be waiting to discover Services named mydb and myservice.</p>
<p>Here's a configuration <code>04-myservice.yaml</code> we can use to make those Services appear :</p>
<pre><code>---
apiVersion: v1
kind: Service
metadata:
  name: myservice
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
---
apiVersion: v1
kind: Service
metadata:
  name: mydb
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9377
</code></pre>
<p>To create the <code>mydb</code> and <code>myservice</code> services:</p>
<pre><code>kubectl apply -f 04-myservice.yaml
</code></pre>
<p>We'll now see that those init containers complete, and that the myapp-pod Pod moves into the Running state:</p>
<pre><code>root@cka001:~# kubectl get -f 04-myservice.yaml
NAME        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
myservice   ClusterIP   10.103.101.99   &lt;none&gt;        80/TCP    40s
mydb        ClusterIP   10.96.79.220    &lt;none&gt;        80/TCP    40s

root@cka001:~# kubectl get pod myapp-pod
NAME        READY   STATUS    RESTARTS   AGE
myapp-pod   1/1     Running   0          13m
</code></pre>
<h3 id="mutil-container-pod">Mutil-Container Pod</h3>
<p>Create the sampel yaml file <code>multi-pod.yaml</code>.</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: multi-container-pod
spec:
  containers:
  - name: container-1-nginx
    image: nginx
    ports:
    - containerPort: 80  
  - name: container-2-alpine
    image: alpine
    command: [&quot;watch&quot;, &quot;wget&quot;, &quot;-qO-&quot;, &quot;localhost&quot;]
</code></pre>
<p>Create the pod.</p>
<pre><code>root@cka001:~# kubectl apply -f multi-pod.yaml

root@cka001:~# kubectl get pod multi-container-pod
NAME                  READY   STATUS    RESTARTS   AGE
multi-container-pod   2/2     Running   0          81s
</code></pre>
<p>Get details of the pod we created via command <code>kubectl describe pod multi-container-pod</code> and we can see below events.</p>
<pre><code>Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  3m14s  default-scheduler  Successfully assigned jh-namespace/multi-container-pod to cka003
  Normal  Pulling    3m14s  kubelet            Pulling image &quot;nginx&quot;
  Normal  Pulled     3m12s  kubelet            Successfully pulled image &quot;nginx&quot; in 2.02130736s
  Normal  Created    3m11s  kubelet            Created container container-1-nginx
  Normal  Started    3m11s  kubelet            Started container container-1-nginx
  Normal  Pulling    3m11s  kubelet            Pulling image &quot;alpine&quot;
  Normal  Pulled     3m3s   kubelet            Successfully pulled image &quot;alpine&quot; in 8.317148653s
  Normal  Created    3m3s   kubelet            Created container container-2-alpine
  Normal  Started    3m3s   kubelet            Started container container-2-alpine
</code></pre>
<p>For multi-container pod, container name is needed if we want to get log of pod via command <code>kubectl logs &lt;pod_name&gt; &lt;container_name&gt;</code>.</p>
<pre><code>root@cka001:~# kubectl logs multi-container-pod
error: a container name must be specified for pod multi-container-pod, choose one of: [container-1-nginx container-2-alpine]

root@cka001:~# kubectl logs multi-container-pod container-1-nginx
......
::1 - - [02/Jul/2022:01:12:29 +0000] &quot;GET / HTTP/1.1&quot; 200 615 &quot;-&quot; &quot;Wget&quot; &quot;-&quot;
</code></pre>
<p>Same if we need specify container name to login into the pod via command <code>kubectl exec -it &lt;pod_name&gt; -c &lt;container_name&gt; -- &lt;commands&gt;</code>.</p>
<pre><code>root@cka001:~# kubectl exec -it multi-container-pod -c container-1-nginx -- /bin/bash
root@multi-container-pod:/# ls
bin  boot  dev  docker-entrypoint.d  docker-entrypoint.sh  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
</code></pre>
<h3 id="usage-of-kubectl">Usage of kubectl</h3>
<h4 id="grant-authorization-to-serviceaccount">Grant Authorization to ServiceAccount</h4>
<p>With Kubernetes 1.23 and lower version, when we create a new namespace, Kubernetes will automatically create a ServiceAccount <code>default</code> and a token <code>default-token-xxxxx</code>.</p>
<p>We can say that the ServiceAccount <code>default</code> is an account under the namespace.</p>
<p>Here is an example of new namespace <code>jh-namespace</code> I created.</p>
<ul>
<li>ServiceAcccount: <code>default</code></li>
<li>Token: <code>default-token-8vrsc</code></li>
</ul>
<pre><code>root@cka001:~# kubectl get sa -n jh-namespace
NAME      SECRETS   AGE
default   1         26h

root@cka001:~# kubectl get secrets -n jh-namespace
NAME                  TYPE                                  DATA   AGE
default-token-8vrsc   kubernetes.io/service-account-token   3      26h
</code></pre>
<p>There is a cluster rule <code>admin</code>, and no related rolebinding.</p>
<pre><code>root@cka001:~# kubectl get clusterrole admin -n jh-namespace
NAME    CREATED AT
admin   2022-06-25T06:24:44Z

root@cka001:~# kubectl get role -n jh-namespace
No resources found in jh-namespace namespace.

root@cka001:~# kubectl get rolebinding -n jh-namespace
No resources found in jh-namespace namespace.
</code></pre>
<p>Get token of the service account <code>default</code>.</p>
<pre><code>TOKEN=$(kubectl describe secret $(kubectl get secrets | grep default | cut -f1 -d ' ') | grep -E '^token' | cut -f2 -d':' | tr -d ' ')
echo $TOKEN
</code></pre>
<p>Get API Service address.</p>
<pre><code>APISERVER=$(kubectl config view | grep https | cut -f 2- -d &quot;:&quot; | tr -d &quot; &quot;)
echo $APISERVER
</code></pre>
<p>Get pod resources in namespace <code>jh-namespace</code> via API server with JSON layout.</p>
<pre><code>curl $APISERVER/api/v1/namespaces/jh-namespace/pods --header &quot;Authorization: Bearer $TOKEN&quot; --insecure
</code></pre>
<p>We will receive below error message. The serviceaccount <code>default</code> does not have authorization to access pod.</p>
<pre><code>&quot;message&quot;: &quot;pods is forbidden: User \&quot;system:serviceaccount:jh-namespace:default\&quot; cannot list resource \&quot;pods\&quot; in API group \&quot;\&quot; in the namespace \&quot;jh-namespace\&quot;&quot;,
</code></pre>
<p>Let's create a rolebinding <code>rolebinding-admin</code> to bind cluster role <code>admin</code> to service account <code>default</code> in namespapce <code>jh-namespace</code>.
Hence service account <code>default</code> is granted adminstrator authorization in namespace <code>jh-namespace</code>.</p>
<pre><code># Usage:
kubectl create rolebinding &lt;rule&gt; --clusterrole=&lt;clusterrule&gt; --serviceaccount=&lt;namespace&gt;:&lt;name&gt; --namespace=&lt;namespace&gt;

# Crate rolebinding:
kubectl create rolebinding rolebinding-admin --clusterrole=admin --serviceaccount=jh-namespace:default --namespace=jh-namespace
</code></pre>
<p>Result looks like below.</p>
<pre><code>root@cka001:~# kubectl get rolebinding -n jh-namespace
NAME                ROLE                AGE
rolebinding-admin   ClusterRole/admin   39s
</code></pre>
<p>Try again, get pod resources in namespace <code>jh-namespace</code> via API server with JSON layout.</p>
<pre><code>curl $APISERVER/api/v1/namespaces/jh-namespace/pods --header &quot;Authorization: Bearer $TOKEN&quot; --insecure
</code></pre>
<h4 id="label-node">Label Node</h4>
<p>Get current label of nodes.</p>
<pre><code>root@cka001:~# kubectl get node --show-labels
NAME     STATUS   ROLES                  AGE   VERSION   LABELS
cka001   Ready    control-plane,master   4d    v1.23.8   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=cka001,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node.kubernetes.io/exclude-from-external-load-balancers=
cka002   Ready    &lt;none&gt;                 4d    v1.23.8   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=cka002,kubernetes.io/os=linux
cka003   Ready    &lt;none&gt;                 4d    v1.23.8   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=cka003,kubernetes.io/os=linux
</code></pre>
<p>Label a node <code>cka003</code>.</p>
<pre><code>root@cka001:~# kubectl label node cka003 node=demonode
</code></pre>
<h4 id="deployment">Deployment</h4>
<p>Create a deployment <code>myapp</code>. <code>--port=8080</code> means the port that this container exposes.</p>
<pre><code>kubectl create deployment myapp --image=docker.io/jocatalin/kubernetes-bootcamp:v1 --replicas=1 --port=8080
</code></pre>
<p>Get the status of the deployment.</p>
<pre><code>root@cka001:~# kubectl get deployment -o wide
NAME    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS            IMAGES                                       SELECTOR
myapp   0/1     1            0           19s   kubernetes-bootcamp   docker.io/jocatalin/kubernetes-bootcamp:v1   app=myapp
</code></pre>
<p>Get the details of deployment.</p>
<pre><code>root@cka001:~# kubectl describe deployment myapp
</code></pre>
<p>Get the status of the Pod.</p>
<pre><code>root@cka001:~# kubectl get pod -o wide
NAME                    READY   STATUS    RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
myapp-b5d775f5d-6jtgs   1/1     Running   0          2m36s   10.244.2.12   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Get the details of the Pod.</p>
<pre><code>root@cka001:~# kubectl describe pod myapp-b5d775f5d-6jtgs
</code></pre>
<h4 id="namespace">Namespace</h4>
<p>Get current available namespaces.</p>
<pre><code>root@cka001:~# kubectl get namespace
NAME              STATUS   AGE
default           Active   4d1h
jh-namespace      Active   2d19h
kube-node-lease   Active   4d1h
kube-public       Active   4d1h
kube-system       Active   4d1h
</code></pre>
<p>Get Pod under a specific namespace.</p>
<pre><code>root@cka001:~# kubectl get pod -n kube-system
NAME                             READY   STATUS    RESTARTS   AGE
coredns-6d8c4cb4d-9khd8          1/1     Running   0          4d1h
coredns-6d8c4cb4d-qcp2l          1/1     Running   0          4d1h
etcd-cka001                      1/1     Running   0          4d1h
kube-apiserver-cka001            1/1     Running   0          4d1h
kube-controller-manager-cka001   1/1     Running   0          4d1h
kube-flannel-ds-hfvf7            1/1     Running   0          4d
kube-flannel-ds-m5mdl            1/1     Running   0          4d
kube-flannel-ds-rf54c            1/1     Running   0          4d
kube-proxy-bj75j                 1/1     Running   0          4d
kube-proxy-gxjj4                 1/1     Running   0          4d
kube-proxy-v7rsr                 1/1     Running   0          4d1h
kube-scheduler-cka001            1/1     Running   0          4d1h
</code></pre>
<p>Get Pods in all namespaces.</p>
<pre><code>root@cka001:~# kubectl get pod --all-namespaces
root@cka001:~# kubectl get pod -A
</code></pre>
<h4 id="expose-service">Expose Service</h4>
<p>Get current running Pod we created just now.</p>
<pre><code>root@cka001:~# kubectl get deployment myapp -o wide
NAME    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS            IMAGES                                       SELECTOR
myapp   1/1     1            1           44m   kubernetes-bootcamp   docker.io/jocatalin/kubernetes-bootcamp:v1   app=myapp

root@cka001:~# kubectl get pod -o wide
NAME                    READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
myapp-b5d775f5d-6jtgs   1/1     Running   0          25m   10.244.2.12   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Send http request to the Pod.</p>
<pre><code>root@cka001:~# curl 10.244.2.12:8080
Hello Kubernetes bootcamp! | Running on: myapp-b5d775f5d-6jtgs | v=1
</code></pre>
<p>To make pod be accessed outside, we need expose port <code>8080</code> to a node port. A related service will be created. </p>
<pre><code>root@cka001:~# kubectl expose deployment myapp --type=NodePort --port=8080
service/myapp exposed
</code></pre>
<p>Get details of service <code>myapp</code>.</p>
<pre><code>root@cka001:~# kubectl get svc myapp -o wide
NAME    TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE     SELECTOR
myapp   NodePort   10.108.93.159   &lt;none&gt;        8080:30520/TCP   5h14m   app=myapp

root@cka001:~# kubectl get svc myapp -o yaml
root@cka001:~# kubectl describe svc myapp
</code></pre>
<p>Get details of related endpoint <code>myapp</code>.</p>
<pre><code>root@cka001:~# kubectl get endpoints myapp -o wide
NAME    ENDPOINTS          AGE
myapp   10.244.2.12:8080   5h21m

root@cka001:~# kubectl describe ep myapp
Name:         myapp
Namespace:    jh-namespace
Labels:       app=myapp
Annotations:  endpoints.kubernetes.io/last-change-trigger-time: 2022-06-29T08:03:17Z
Subsets:
  Addresses:          10.244.2.12
  NotReadyAddresses:  &lt;none&gt;
  Ports:
    Name     Port  Protocol
    ----     ----  --------
    &lt;unset&gt;  8080  TCP

Events:  &lt;none&gt;
</code></pre>
<p>Get details of Pod of <code>myapp</code>.</p>
<pre><code>root@cka001:~# kubectl get pod -owide
NAME                    READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
myapp-b5d775f5d-6jtgs   1/1     Running   0          70m   10.244.2.12   cka003   &lt;none&gt;           &lt;none&gt;

root@cka001:~# kubectl get node -o wide
NAME     STATUS   ROLES                  AGE    VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
cka001   Ready    control-plane,master   4d2h   v1.23.8   172.16.18.161   &lt;none&gt;        Ubuntu 20.04.4 LTS   5.4.0-113-generic   containerd://1.5.9
cka002   Ready    &lt;none&gt;                 4d1h   v1.23.8   172.16.18.160   &lt;none&gt;        Ubuntu 20.04.4 LTS   5.4.0-113-generic   containerd://1.5.9
cka003   Ready    &lt;none&gt;                 4d1h   v1.23.8   172.16.18.159   &lt;none&gt;        Ubuntu 20.04.4 LTS   5.4.0-113-generic   containerd://1.5.9
</code></pre>
<p>Send http request to the service and node sucessfully. Pod port <code>8080</code> is mapped to node port <code>30520</code>.</p>
<pre><code>root@cka001:~# curl http://10.108.93.159:8080
Hello Kubernetes bootcamp! | Running on: myapp-b5d775f5d-6jtgs | v=1

root@cka001:~# curl http://172.16.18.159:30520
Hello Kubernetes bootcamp! | Running on: myapp-b5d775f5d-6jtgs | v=1
</code></pre>
<h4 id="scalling">Scalling</h4>
<p>Deployment <code>myapp</code> is now having 1 replica.</p>
<pre><code>root@cka001:~# kubectl get deployment myapp -o wide
NAME    READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS            IMAGES                                       SELECTOR
myapp   1/1     1            1           6h12m   kubernetes-bootcamp   docker.io/jocatalin/kubernetes-bootcamp:v1   app=myapp
</code></pre>
<p>Scale to 2 replicas.</p>
<pre><code>root@cka001:~# kubectl scale deployment myapp --replicas=2
deployment.apps/myapp scaled

root@cka001:~# kubectl get deployment myapp -o wide
NAME    READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS            IMAGES                                       SELECTOR
myapp   2/2     2            2           6h14m   kubernetes-bootcamp   docker.io/jocatalin/kubernetes-bootcamp:v1   app=myapp
</code></pre>
<p>Scale to 1 replicas. We can see interim phase that one Pos is been terminating.</p>
<pre><code>root@cka001:~# kubectl get deployment myapp -o wide
NAME    READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS            IMAGES                                       SELECTOR
myapp   1/1     1            1           6h17m   kubernetes-bootcamp   docker.io/jocatalin/kubernetes-bootcamp:v1   app=myapp

root@cka001:~# kubectl get pod
NAME                    READY   STATUS        RESTARTS   AGE
myapp-b5d775f5d-6jtgs   1/1     Running       0          6h17m
myapp-b5d775f5d-mpshb   1/1     Terminating   0          3m28s
</code></pre>
<h4 id="rolling">Rolling</h4>
<p>Get current deployment image version.</p>
<pre><code>root@cka001:~# kubectl get deployment -o wide
NAME    READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS            IMAGES                                       SELECTOR
myapp   1/1     1            1           6h21m   kubernetes-bootcamp   docker.io/jocatalin/kubernetes-bootcamp:v1   app=myapp
</code></pre>
<p>Update image to new versions.</p>
<pre><code>kubectl set image deployment/myapp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v2 --record
kubectl set image deployment/myapp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v3 --record
kubectl set image deployment/myapp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v4 --record
kubectl set image deployment/myapp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v5 --record
</code></pre>
<p>We can observe that Pod's IP is changed to new one, and running on another node.
New Pod is in <code>ImagePullBackOff</code> status due to network issue to access <code>docker.io/jocatalin/kubernetes-bootcamp</code>.</p>
<pre><code>root@cka001:~# kubectl get pod -o wide
NAME                     READY   STATUS             RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
myapp-75ccb85dd6-hzc82   0/1     ImagePullBackOff   0          2m15s   10.244.1.13   cka002   &lt;none&gt;           &lt;none&gt;
myapp-b5d775f5d-6jtgs    1/1     Running            0          6h24m   10.244.2.12   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Let's verify if the service is still available after rolling update. Send http request to the node sucessfully.</p>
<pre><code>root@cka001:~# curl http://172.16.18.160:30520
Hello Kubernetes bootcamp! | Running on: myapp-b5d775f5d-6jtgs | v=1
</code></pre>
<p>Get rolling update history.</p>
<pre><code>root@cka001:~# kubectl rollout history deployment/myapp
deployment.apps/myapp 
REVISION  CHANGE-CAUSE
1         &lt;none&gt;
2         kubectl set image deployment/myapp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v2 --record=true
3         kubectl set image deployment/myapp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v3 --record=true
4         kubectl set image deployment/myapp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v4 --record=true
5         kubectl set image deployment/myapp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v5 --record=true
</code></pre>
<p>Reverse to revision 3. Copied revision <code>3</code> to <code>6</code> as current revision.</p>
<pre><code>root@cka001:~# kubectl rollout undo deployment/myapp --to-revision=3
deployment.apps/myapp rolled back

root@cka001:~# kubectl rollout history deployment/myapp
deployment.apps/myapp 
REVISION  CHANGE-CAUSE
1         &lt;none&gt;
2         kubectl set image deployment/myapp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v2 --record=true
4         kubectl set image deployment/myapp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v4 --record=true
5         kubectl set image deployment/myapp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v5 --record=true
6         kubectl set image deployment/myapp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v3 --record=true
</code></pre>
<h4 id="event">Event</h4>
<p>Get detail event info of related Pod.</p>
<pre><code>root@cka001:~# kubectl describe pod myapp-78bdb65cd8-bnjbj
</code></pre>
<p>Result looks like below.</p>
<pre><code>Events:
  Type     Reason     Age                 From               Message
  ----     ------     ----                ----               -------
  Normal   Scheduled  15m                 default-scheduler  Successfully assigned jh-namespace/myapp-78bdb65cd8-bnjbj to cka002
  Normal   Pulling    14m (x4 over 15m)   kubelet            Pulling image &quot;docker.io/jocatalin/kubernetes-bootcamp:v3&quot;
  Warning  Failed     14m (x4 over 15m)   kubelet            Failed to pull image &quot;docker.io/jocatalin/kubernetes-bootcamp:v3&quot;: rpc error: code = NotFound desc = failed to pull and unpack image &quot;docker.io/jocatalin/kubernetes-bootcamp:v3&quot;: failed to resolve reference &quot;docker.io/jocatalin/kubernetes-bootcamp:v3&quot;: docker.io/jocatalin/kubernetes-bootcamp:v3: not found
  Warning  Failed     14m (x4 over 15m)   kubelet            Error: ErrImagePull
  Warning  Failed     14m (x6 over 15m)   kubelet            Error: ImagePullBackOff
  Normal   BackOff    44s (x65 over 15m)  kubelet            Back-off pulling image &quot;docker.io/jocatalin/kubernetes-bootcamp:v3&quot;
</code></pre>
<p>Get detail event info of entire cluster.</p>
<pre><code>root@cka001:~# kubectl get event
</code></pre>
<h4 id="logging">Logging</h4>
<p>Get log info of Pod.</p>
<pre><code>kubectl logs -f &lt;pod_name&gt;
kubectl logs -f &lt;pod_name&gt; -c &lt;container_name&gt; 
</code></pre>
<pre><code>root@cka001:~# kubectl logs -f myapp-78bdb65cd8-bnjbj
Error from server (BadRequest): container &quot;kubernetes-bootcamp&quot; in pod &quot;myapp-78bdb65cd8-bnjbj&quot; is waiting to start: trying and failing to pull image
</code></pre>
<p>Get log info of K8s components. </p>
<pre><code>root@cka001:~# kubectl logs kube-apiserver-cka001 -n kube-system
root@cka001:~# kubectl logs kube-controller-manager-cka001 -n kube-system
root@cka001:~# kubectl logs kube-scheduler-cka001 -n kube-system
root@cka001:~# kubectl logs etcd-cka001 -n kube-system
root@cka001:~# systemctl status kubelet
root@cka001:~# journalctl -fu kubelet
root@cka001:~# kubectl logs kube-proxy-bj75j -n kube-system
</code></pre>
<h3 id="demo-workload-resources">Demo: Workload Resources</h3>
<h4 id="deployment_1">Deployment</h4>
<p>Create deployment via command <code>kubectl create</code>.</p>
<pre><code>root@cka001:~# kubectl create deployment deploy-http-app1 --image=nginx:1.19
</code></pre>
<p>Create deployment via yaml file and apply it.</p>
<pre><code>root@cka001:~# cat &gt; deploy-http-app2.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deploy-http-app2
  labels:
    app: deploy-http-app2
spec:
  selector:
    matchLabels:
      app: deploy-http-app2
  replicas: 1
  template:
    metadata:
      labels:
        app: deploy-http-app2
    spec:
      containers:
      - name: nginx
        image: nginx:1.19
EOF


root@cka001:~# kubectl apply -f deploy-http-app2.yaml
</code></pre>
<p>Get Deployment Pod created just now.</p>
<pre><code>root@cka001:~# kubectl get pod
NAME                                READY   STATUS    RESTARTS   AGE
deploy-http-app1-7cbc9b645d-zztg9   1/1     Running   0          116s
deploy-http-app2-5f5f7765c9-7hcmt   1/1     Running   0          46s
</code></pre>
<p>Use below commands to check details of deployment pod we creatd just now.</p>
<pre><code># kubectl describe deployment
# kubectl get deployment -oyaml
# kubectl describe pod
# kubectl get pod -oyaml
</code></pre>
<h4 id="statefulset">StatefulSet</h4>
<p>Create StatefulSet with yaml file and apply it.</p>
<pre><code>root@cka001:~# cat &gt; stateufulset-web.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: &quot;nginx&quot;
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
          name: web
EOF

root@cka001:~# kubectl apply -f stateufulset-web.yaml
</code></pre>
<p>Get details of StatefulSet Pod created just now.</p>
<pre><code>root@cka001:~# kubectl get pod | grep web
NAME                                READY   STATUS    RESTARTS   AGE
web-0                               1/1     Running   0          2m1s
web-1                               1/1     Running   0          117s

root@cka001:~# kubectl get sts -o wide
NAME   READY   AGE   CONTAINERS   IMAGES
web    2/2     88s   nginx        nginx
</code></pre>
<p>Use command <code>kubectl edit sts web</code> to update an existing StatefulSet.
ONLY these fields can be updated: <code>replicas</code>、<code>image</code>、<code>rolling updates</code>、<code>labels</code>、<code>resource request/limit</code> and <code>annotations</code>.</p>
<p>Note: 
Copy of StatefulSet Pod will not be created automatically in other node when it's dead in current node.  </p>
<h4 id="daemonset">DaemonSet</h4>
<p>Create DaemonSet with yaml file and apply it.</p>
<pre><code>root@cka001:~# cat &gt; daemonset-busybox.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: daemonset-busybox
  labels:
    app: daemonset-busybox
spec:
  selector:
    matchLabels:
      app: daemonset-busybox
  template:
    metadata:
      labels:
        app: daemonset-busybox
    spec:
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: busybox
        image: busybox:1.28
        args:
        - sleep
        - &quot;10000&quot;
EOF


root@cka001:~# kubectl apply -f daemonset-busybox.yaml
</code></pre>
<p>Get status of DaemonSet Pod. Note, it's deployed on each node.</p>
<pre><code>root@cka001:~# kubectl get daemonset
NAME                DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset-busybox   3         3         3       3            3           &lt;none&gt;          5m33s

root@cka001:~# kubectl get pod -o wide | grep daemonset-busybox
NAME                                READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
daemonset-busybox-kb2kp             1/1     Running   0          75s   10.244.0.6    cka001   &lt;none&gt;           &lt;none&gt;
daemonset-busybox-lnspq             1/1     Running   0          75s   10.244.2.16   cka003   &lt;none&gt;           &lt;none&gt;
daemonset-busybox-r6sc7             1/1     Running   0          75s   10.244.1.17   cka002   &lt;none&gt;           &lt;none&gt;
</code></pre>
<h4 id="job">Job</h4>
<p>Create Job with yaml file and apply it.</p>
<pre><code>root@cka001:~# cat &gt; job-pi.yaml &lt;&lt;EOF
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    spec:
      containers:
      - name: pi
        image: perl:5.34
        command: [&quot;perl&quot;,  &quot;-Mbignum=bpi&quot;, &quot;-wle&quot;, &quot;print bpi(2000)&quot;]
      restartPolicy: Never
  backoffLimit: 4
EOF

root@cka001:~# kubectl apply -f job-pi.yaml
</code></pre>
<p>Get details of Job.</p>
<pre><code>root@cka001:~# kubectl get jobs
</code></pre>
<p>Get details of Job Pod. The status <code>Completed</code> means the job was done successfully.</p>
<pre><code>root@cka001:~# kubectl get pod pi-s28pr
</code></pre>
<p>Get log info of the Job Pod.</p>
<pre><code>root@cka001:~# kubectl logs pi-s28pr
3.141592653589793..............
</code></pre>
<h4 id="cronjob">Cronjob</h4>
<p>Create Cronjob with yaml file and apply it.</p>
<pre><code>root@cka001:~# cat &gt; cronjob-hello.yaml &lt;&lt;EOF
apiVersion: batch/v1
kind: CronJob
metadata:
 name: hello
spec:
  schedule: &quot;*/1 * * * *&quot;
  jobTemplate:
   spec:
    template:
     spec:
      containers:
      - name: hello
        image: busybox
        args:
        - /bin/sh
        - -c
        - date ; echo Hello from the kubernetes cluster
      restartPolicy: OnFailure
EOF


root@cka001:~# kubectl apply -f cronjob-hello.yaml
</code></pre>
<p>Get detail of Cronjob</p>
<pre><code>root@cka001:~# kubectl get cronjobs
</code></pre>
<p>Monitor Jobs. Every 1 minute a new job will be created. </p>
<pre><code>root@cka001:~# kubectl get jobs -w
</code></pre>
<h2 id="6label-and-annotation">6.Label and Annotation</h2>
<h3 id="label-and-annotation">Label and Annotation</h3>
<h4 id="label">Label</h4>
<p>Set Label <code>disktype=ssd</code> for node <code>cka003</code>.</p>
<pre><code>root@cka001:~# kubectl label node cka002 disktype=ssd
</code></pre>
<p>Get Label info</p>
<pre><code>root@cka001:~# kubectl get node --show-labels
root@cka001:~# kubectl describe node cka003
root@cka001:~# kubectl get node cka003 -oyaml
</code></pre>
<p>Overwrite Label with <code>disktype=hdd</code> for node <code>cka003</code>.</p>
<pre><code>root@cka001:~# kubectl label node cka003 disktype=hdd --overwrite
</code></pre>
<p>Remove Label for node <code>cka003</code></p>
<pre><code>root@cka001:~# kubectl label node cka003 disktype-
</code></pre>
<h4 id="annotation">Annotation</h4>
<p>Create Nginx deployment</p>
<pre><code>root@cka001:~# kubectl create deploy nginx --image=nginx:mainline
</code></pre>
<p>Get Annotation info.</p>
<pre><code>root@cka001:~# kubectl describe deployment/nginx

Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=nginx
</code></pre>
<p>Add new Annotation.</p>
<pre><code>root@cka001:~# kubectl annotate deployment nginx owner=jh

Annotations:            deployment.kubernetes.io/revision: 1
                        owner: jh
Selector:               app=nginx
</code></pre>
<p>Update/Overwrite Annotation.</p>
<pre><code>root@cka001:~# kubectl annotate deployment/nginx owner=qwer --overwrite

Annotations:            deployment.kubernetes.io/revision: 1
                        owner: qwer
Selector:               app=nginx
</code></pre>
<p>Remove Annotation</p>
<pre><code>root@cka001:~# kubectl annotate deployment/nginx owner-

Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=nginx
</code></pre>
<h2 id="7health-check">7.Health Check</h2>
<h3 id="status-of-pod-and-container">Status of Pod and Container</h3>
<p>Create a yaml file <code>multi-pods.yaml</code>. </p>
<pre><code>cat &gt; multi-pods.yaml  &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: multi-pods
  name: multi-pods
spec:
  containers:
  - image: nginx
    name: nginx
  - image: busybox
    name: busybox
  dnsPolicy: ClusterFirst
  restartPolicy: Always
EOF
</code></pre>
<p>Apply the yaml file to create a Pod <code>multi-pods</code> with two containers <code>nginx</code> and <code>busybox</code>. </p>
<pre><code>kubectl apply -f multi-pods.yaml
</code></pre>
<p>Minotor the status with option <code>--watch</code>. The status of Pod was changed from <code>ContainerCreating</code> to <code>NotReady</code> to <code>CrashLoopBackOff</code>.</p>
<pre><code>root@cka001:~# kubectl get pod multi-pods --watch
NAME         READY   STATUS              RESTARTS   AGE
multi-pods   0/2     ContainerCreating   0          49s
multi-pods   1/2     NotReady            1          99s
multi-pods   1/2     CrashLoopBackOff    2          110s
</code></pre>
<p>Get details of the Pod <code>multi-pods</code>, focus on Container's state under segment <code>Containers</code> and Conditions of Pod under segment <code>Conditions</code>.</p>
<pre><code>root@cka001:~# kubectl describe pod multi-pods
......
Containers:
  nginx:
    ......
    State:          Running
      Started:      Sun, 03 Jul 2022 12:52:42 +0800
    Ready:          True
    Restart Count:  0
    ......
  busybox:
    ......
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sun, 03 Jul 2022 12:58:43 +0800
      Finished:     Sun, 03 Jul 2022 12:58:43 +0800
    Ready:          False
    Restart Count:  6
    ......
Conditions:
  Type              Status
  Initialized       True     # Set to True when initCounter completed successfully.
  Ready             False    # Set to True when ContainersReady is True.
  ContainersReady   False    # Set to True when all containers are ready.
  PodScheduled      True     # Set to True when Pos schedule completed successfully.
...... 
</code></pre>
<h3 id="livenessprobe">LivenessProbe</h3>
<p>Detail description of the demo can be found on the <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">Kubernetes document</a>.</p>
<p>Create a yaml file <code>liveness.yaml</code> with <code>livenessProbe</code> setting and apply it.</p>
<pre><code>root@cka001:~# cat &gt; liveness.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - name: liveness
    image: busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
EOF

root@cka001:~# kubectl apply -f liveness.yaml
</code></pre>
<p>Let's see what happened in the Pod <code>liveness-exec</code>.</p>
<ul>
<li>Create a folder <code>/tmp/healthy</code>.</li>
<li>Execute the the command <code>cat /tmp/healthy</code> and return successful code.</li>
<li>After <code>35</code> seconds, execute command <code>rm -rf /tmp/healthy</code> to delete the folder. The probe <code>livenessProbe</code> detects the failure and return error message.</li>
<li>The kubelet kills the container and restarts it. The folder is created again <code>touch /tmp/healthy</code>.</li>
</ul>
<p>By command <code>kubectl describe pod liveness-exec</code>, wec can see below event message. Once failure detected, image will be pulled again and the folder <code>/tmp/healthy</code> is in place again.</p>
<pre><code>Events:
  Type     Reason     Age                   From               Message
  ----     ------     ----                  ----               -------
  Normal   Scheduled  4m21s                 default-scheduler  Successfully assigned jh-namespace/liveness-exec to cka002
  Normal   Pulled     4m19s                 kubelet            Successfully pulled image &quot;busybox&quot; in 1.906981795s
  Normal   Pulled     3m4s                  kubelet            Successfully pulled image &quot;busybox&quot; in 1.967545593s
  Normal   Created    109s (x3 over 4m19s)  kubelet            Created container liveness
  Normal   Started    109s (x3 over 4m19s)  kubelet            Started container liveness
  Normal   Pulled     109s                  kubelet            Successfully pulled image &quot;busybox&quot; in 2.051565102s
  Warning  Unhealthy  66s (x9 over 3m46s)   kubelet            Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory
  Normal   Killing    66s (x3 over 3m36s)   kubelet            Container liveness failed liveness probe, will be restarted
  Normal   Pulling    36s (x4 over 4m21s)   kubelet            Pulling image &quot;busybox&quot;
</code></pre>
<h3 id="readinessprobe">ReadinessProbe</h3>
<p>Readiness probes are configured similarly to liveness probes. The only difference is that you use the readinessProbe field instead of the livenessProbe field.</p>
<p>Create a yaml file <code>readiness.yaml</code> with <code>readinessProbe</code> setting and apply it.</p>
<pre><code>cat &gt; readiness.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: readiness
spec:
    containers:
    - name: readiness
      image: busybox
      args:
      - /bin/sh
      - -c
      - touch /tmp/healthy; sleep 5;rm -rf /tmp/healthy; sleep 600
      readinessProbe:
        exec:
          command:
          - cat
          - /tmp/healthy
        initialDelaySeconds: 10
        periodSeconds: 5
EOF

kubectl apply -f readiness.yaml
</code></pre>
<p>The ready status of the Pod is 0/1, that is, the Pod is not up successfully.</p>
<pre><code>root@cka001:~# kubectl get pod readiness --watch
NAME        READY   STATUS    RESTARTS   AGE
readiness   0/1     Running   0          15s
</code></pre>
<p>Execute command <code>kubectl describe pod readiness</code> to check status of Pod. We will see failure message <code>Readiness probe failed</code>.</p>
<pre><code>Events:
  Type     Reason     Age               From               Message
  ----     ------     ----              ----               -------
  Normal   Scheduled  35s               default-scheduler  Successfully assigned jh-namespace/readiness to cka003
  Normal   Pulling    35s               kubelet            Pulling image &quot;busybox&quot;
  Normal   Pulled     32s               kubelet            Successfully pulled image &quot;busybox&quot; in 2.420171698s
  Normal   Created    32s               kubelet            Created container readiness
  Normal   Started    32s               kubelet            Started container readiness
  Warning  Unhealthy  5s (x4 over 20s)  kubelet            Readiness probe failed: cat: can't open '/tmp/healthy': No such file or directory
</code></pre>
<p>Liveness probes do not wait for readiness probes to succeed. If you want to wait before executing a liveness probe you should use initialDelaySeconds or a startupProbe.</p>
<h3 id="demo-of-health-check">Demo of Health Check</h3>
<p>Set up yaml file of health check for Nginx based Deployment + Service and apply it.</p>
<pre><code>cat &gt; nginx-healthcheck.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-healthcheck
spec:
  replicas: 2
  selector:
    matchLabels:
      name: nginx-healthcheck
  template:
    metadata:
      labels:
        name: nginx-healthcheck
    spec:
      containers:
        - name: nginx-healthcheck
          image: nginx:latest
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 80  
          livenessProbe:
            initialDelaySeconds: 5
            periodSeconds: 5
            tcpSocket:
              port: 80
            timeoutSeconds: 5   
          readinessProbe:
            httpGet:
              path: /
              port: 80
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-healthcheck
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
  type: NodePort
  selector:
    name: nginx-healthcheck
EOF


kubectl apply -f nginx-healthcheck.yaml
</code></pre>
<p>Check nginx-healthcheck Pod.</p>
<pre><code>kubectl get pod -owide
</code></pre>
<p>Get below result.</p>
<pre><code>NAME                                 READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
nginx-healthcheck-79fc55d944-9jbvj   1/1     Running   0          50s   10.244.2.82   cka003   &lt;none&gt;           &lt;none&gt;
nginx-healthcheck-79fc55d944-rwx7n   1/1     Running   0          50s   10.244.1.11   cka002   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Access Pod IP via <code>curl</code> command, e.g., above example.</p>
<pre><code>curl 10.244.2.82
curl 10.244.1.11
</code></pre>
<p>We will see a successful <code>index.html</code> conten of Nginx below with above example.</p>
<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>Check details of Service craeted in above example.</p>
<pre><code>kubectl describe svc nginx-healthcheck
</code></pre>
<p>We will see below output. There are two Pods information listed in <code>Endpoints</code>.</p>
<pre><code>Name:                     nginx-healthcheck
Namespace:                jh-namespace
Labels:                   &lt;none&gt;
Annotations:              &lt;none&gt;
Selector:                 name=nginx-healthcheck
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.98.196.231
IPs:                      10.98.196.231
Port:                     &lt;unset&gt;  80/TCP
TargetPort:               80/TCP
NodePort:                 &lt;unset&gt;  31505/TCP
Endpoints:                10.244.1.11:80,10.244.2.82:80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   &lt;none&gt;
</code></pre>
<p>We can also get information of Endpoints.</p>
<pre><code>kubectl get endpoints nginx-healthcheck
</code></pre>
<p>Get below result.</p>
<pre><code>NAME                ENDPOINTS                       AGE
nginx-healthcheck   10.244.1.11:80,10.244.2.82:80   8m35s
</code></pre>
<p>Till now, two <code>nginx-healthcheck</code> Pods are working and providing service as expected. </p>
<p>Let's simulate an error by deleting and <code>index.html</code> file in on of <code>nginx-healthcheck</code> Pod and see what's readinessProbe will do.</p>
<p>First, execute <code>kubectl exec -it &lt;your_pod_name&gt; -- bash</code> to log into <code>nginx-healthcheck</code> Pod, and delete the <code>index.html</code> file.</p>
<pre><code>kubectl exec -it nginx-healthcheck-79fc55d944-9jbvj -- bash
cd /usr/share/nginx/html/
rm -rf index.html
exit
</code></pre>
<p>After that, let's check the status of above Pod that <code>index.html</code> file was deleted.</p>
<pre><code>kubectl describe pod nginx-healthcheck-79fc55d944-9jbvj
</code></pre>
<p>We can now see <code>Readiness probe failed</code> error event message.</p>
<pre><code>Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  29m                default-scheduler  Successfully assigned jh-namespace/nginx-healthcheck-79fc55d944-9jbvj to cka003
  Normal   Pulled     29m                kubelet            Container image &quot;nginx:latest&quot; already present on machine
  Normal   Created    29m                kubelet            Created container nginx-healthcheck
  Normal   Started    29m                kubelet            Started container nginx-healthcheck
  Warning  Unhealthy  1s (x16 over 71s)  kubelet            Readiness probe failed: HTTP probe failed with statuscode: 403
</code></pre>
<p>Let's check another Pod. </p>
<pre><code>kubectl describe pod nginx-healthcheck-79fc55d944-rwx7n
</code></pre>
<p>There is no error info.</p>
<pre><code>Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  31m   default-scheduler  Successfully assigned jh-namespace/nginx-healthcheck-79fc55d944-rwx7n to cka002
  Normal  Pulled     31m   kubelet            Container image &quot;nginx:latest&quot; already present on machine
  Normal  Created    31m   kubelet            Created container nginx-healthcheck
  Normal  Started    31m   kubelet            Started container nginx-healthcheck
</code></pre>
<p>Now, access Pod IP via <code>curl</code> command and see what the result of each Pod.</p>
<pre><code>curl 10.244.2.82
curl 10.244.1.11
</code></pre>
<p>We will receive error while access the first Pod <code>curl 10.244.2.82</code>. The second Pos works well <code>curl 10.244.1.11</code>. </p>
<pre><code>&lt;html&gt;
&lt;head&gt;&lt;title&gt;403 Forbidden&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;&lt;h1&gt;403 Forbidden&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx/1.23.0&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>Let's check current status of Nginx Service after one of Pods runs into failure. </p>
<pre><code>kubectl describe svc nginx-healthcheck
</code></pre>
<p>In below output, there is only one Pod information listed in Endpoint.</p>
<pre><code>Name:                     nginx-healthcheck
Namespace:                jh-namespace
Labels:                   &lt;none&gt;
Annotations:              &lt;none&gt;
Selector:                 name=nginx-healthcheck
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.98.196.231
IPs:                      10.98.196.231
Port:                     &lt;unset&gt;  80/TCP
TargetPort:               80/TCP
NodePort:                 &lt;unset&gt;  31505/TCP
Endpoints:                10.244.1.11:80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   &lt;none&gt;
</code></pre>
<p>Same result we can get by checking information of Endpoints, which is only Pod is running.</p>
<pre><code>kubectl get endpoints nginx-healthcheck 
</code></pre>
<p>Output:</p>
<pre><code>NAME                ENDPOINTS        AGE
nginx-healthcheck   10.244.1.11:80   40m
</code></pre>
<p>Conclusion: 
By delete the index.html file, the Pod is in unhealth status and is removed from endpoint list. 
One one health Pod can provide normal service.</p>
<p>Let's re-create the <code>index.html</code> file again in the Pod. </p>
<pre><code>kubectl exec -it nginx-healthcheck-79fc55d944-9jbvj -- bash

cd /usr/share/nginx/html/

cat &gt; index.html &lt;&lt; EOF 
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
EOF

exit
</code></pre>
<p>We now can see that two Pods are back to Endpoints to provide service now.</p>
<pre><code>kubectl describe svc nginx-healthcheck

kubectl get endpoints nginx-healthcheck
</code></pre>
<p>Re-access Pod IP via <code>curl</code> command and we can see both are back to normal status.</p>
<pre><code>curl 10.244.2.82
curl 10.244.1.11
</code></pre>
<p>Verify the Pod status again. </p>
<pre><code>kubectl describe pod nginx-healthcheck-79fc55d944-9jbvj
</code></pre>
<h2 id="8namespace">8.Namespace</h2>
<p>Get list of Namespace</p>
<pre><code>kubectl get namespace
</code></pre>
<p>Get list of Namespace with Label information.</p>
<pre><code>kubectl get ns --show-labels
</code></pre>
<p>Create a Namespace</p>
<pre><code>kubectl create namespace cka
</code></pre>
<p>Label the new created Namespace <code>cka</code>.</p>
<pre><code>kubectl label ns cka cka=true
</code></pre>
<p>Create Nginx Deployment in Namespace <code>cka</code>. </p>
<pre><code>kubectl create deploy nginx --image=nginx --namespace cka
</code></pre>
<p>Check Deployments and Pods running in namespace <code>cka</code>.</p>
<pre><code>kubectl get deploy,pod -n cka
</code></pre>
<p>Result is below.</p>
<pre><code>NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx   1/1     1            1           2m14s

NAME                         READY   STATUS    RESTARTS   AGE
pod/nginx-85b98978db-bmkhf   1/1     Running   0          2m14s
</code></pre>
<p>Delete namespace <code>cka</code>. All resources in the namespaces will be gone.</p>
<pre><code>kubectl delete ns cka
</code></pre>
<h2 id="9horizontal-pod-autoscaling-hpa">9.Horizontal Pod Autoscaling (HPA)</h2>
<ul>
<li>Install Metrics Server component</li>
</ul>
<p>Download yaml file for Metrics Server component</p>
<pre><code>wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
</code></pre>
<p>Replace Google image by Aliyun image <code>image: registry.aliyuncs.com/google_containers/metrics-server:v0.6.1</code>.</p>
<pre><code>sed -i 's/k8s\.gcr\.io\/metrics-server\/metrics-server\:v0\.6\.1/registry\.aliyuncs\.com\/google_containers\/metrics-server\:v0\.6\.1/g' components.yaml
</code></pre>
<p>Change <code>arg</code> of <code>metrics-server</code> by adding <code>--kubelet-insecure-tls</code> to disable tls certificate validation. </p>
<pre><code>vi components.yaml
</code></pre>
<p>Updated <code>arg</code> of <code>metrics-server</code> is below.</p>
<pre><code>  template:
    metadata:
      labels:
        k8s-app: metrics-server
    spec:
      containers:
      - args:
        - --cert-dir=/tmp
        - --secure-port=4443
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --kubelet-use-node-status-port
        - --metric-resolution=15s
        - --kubelet-insecure-tls
        image: registry.aliyuncs.com/google_containers/metrics-server:v0.6.1

</code></pre>
<p>Appy the yaml file <code>components.yaml</code> to deploy <code>metrics-server</code>.</p>
<pre><code>kubectl apply -f components.yaml
</code></pre>
<p>Below resources were crested. </p>
<pre><code>serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
</code></pre>
<p>Verify if <code>metrics-server</code> Pod is running as expected.</p>
<pre><code>kubectl get pod -n kube-system -owide | grep metrics-server
</code></pre>
<p>Get current usage of CPU, memory of each node.</p>
<pre><code>kubectl top node
</code></pre>
<p>Result:</p>
<pre><code>NAME     CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
cka001   148m         7%     1746Mi          45%
cka002   41m          2%     1326Mi          34%       
cka003   39m          1%     1383Mi          36%
</code></pre>
<h3 id="deploy-a-service-podinfo">Deploy a Service <code>podinfo</code></h3>
<p>Create and apply the yaml file <code>podinfo.yaml</code> to deploy Deployment and Service <code>podinfo</code> for further stress testing.</p>
<pre><code>cat &gt; podinfo.yaml &lt;&lt; EOF
apiVersion: v1
kind: Service
metadata:
  name: podinfo
  labels:
    app: podinfo
spec:
  type: NodePort
  ports:
    - port: 9898
      targetPort: 9898
      nodePort: 31198
      protocol: TCP
  selector:
    app: podinfo
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: podinfo
  labels:
    app: podinfo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: podinfo
  template:
    metadata:
      labels:
        app: podinfo
    spec:
      containers:
      - name: podinfod
        image: stefanprodan/podinfo:0.0.1
        imagePullPolicy: Always
        command:
          - ./podinfo
          - -port=9898
          - -logtostderr=true
          - -v=2
        ports:
        - containerPort: 9898
          protocol: TCP
        resources:
          requests:
            memory: &quot;32Mi&quot;
            cpu: &quot;10m&quot;
          limits:
            memory: &quot;256Mi&quot;
            cpu: &quot;100m&quot;
EOF


kubectl apply -f podinfo.yaml
</code></pre>
<h3 id="config-hpa">Config HPA</h3>
<p>Create and apply yaml file <code>hpa.yaml</code> for HPA by setting CPU threshold <code>50%</code> to trigger auto-scalling with minimal <code>2</code> and maximal <code>10</code> Replicas.</p>
<pre><code>cat &gt; hpa.yaml &lt;&lt;EOF
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: nginx
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: podinfo
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 50
EOF


kubectl apply -f hpa.yaml
</code></pre>
<p>Get status of HPA.</p>
<pre><code>kubectl get hpa
</code></pre>
<p>Result:</p>
<pre><code>NAME    REFERENCE            TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
nginx   Deployment/podinfo   10%/50%   2         10        2          26s
</code></pre>
<h3 id="stress-testing">Stress Testing</h3>
<p>Here we will use <code>ab</code> tool to simulate 1000 concurrency.</p>
<p>The <code>ab</code> command is a command line load testing and benchmarking tool for web servers that allows you to simulate high traffic to a website. </p>
<p>The short definition form apache.org is: The acronym <code>ab</code> stands for Apache Bench where bench is short for benchmarking.</p>
<h4 id="install-ab">Install ab</h4>
<p>Execute below command to install <code>ab</code> tool.</p>
<pre><code>apt install apache2-utils -y
</code></pre>
<p>Most common options of <code>ab</code> are <code>-n</code> and <code>-c</code>：</p>
<pre><code>-n requests     Number of requests to perform
-c concurrency  Number of multiple requests to make at a time
-t timelimit    Seconds to max. to spend on benchmarking. This implies -n 50000
-p postfile     File containing data to POST. Remember also to set -T      
-T content-type Content-type header to use for POST/PUT data, eg. 'application/x-www-form-urlencoded'. Default is 'text/plain'
-k              Use HTTP KeepAlive feature
</code></pre>
<p>Example: </p>
<pre><code>ab -n 1000 -c 100 http://www.baidu.com/
</code></pre>
<h4 id="concurrency-stres-test">Concurrency Stres Test</h4>
<p>Simulate 1000 concurrency request to current node running command <code>ab</code>.</p>
<pre><code>ab -c 1000 -t 60 http://127.0.0.1:31198/
</code></pre>
<p>By command <code>kubectl get hpa -w</code> we can see that CPU workload has been increasing.</p>
<pre><code>NAME    REFERENCE            TARGETS    MINPODS   MAXPODS   REPLICAS   AGE
nginx   Deployment/podinfo   388%/50%   2         10        10         32m
</code></pre>
<p>And see auto-scalling automically triggered via commands <code>kubectl get pod</code> and <code>kubectl get deployment</code>.</p>
<pre><code>NAME                                 READY   STATUS    RESTARTS   AGE
nginx-healthcheck-79fc55d944-9jbvj   1/1     Running   0          153m
nginx-healthcheck-79fc55d944-rwx7n   1/1     Running   0          153m
podinfo-668b5b9b5b-4rxwr             1/1     Running   0          51m
podinfo-668b5b9b5b-6vm5k             1/1     Running   0          6m
podinfo-668b5b9b5b-7p74p             1/1     Running   0          5m45s
podinfo-668b5b9b5b-8929m             1/1     Running   0          5m45s
podinfo-668b5b9b5b-9fr28             1/1     Running   0          51m
podinfo-668b5b9b5b-dz74z             1/1     Running   0          6m
podinfo-668b5b9b5b-fzszt             1/1     Running   0          5m30s
podinfo-668b5b9b5b-gb2qq             1/1     Running   0          5m45s
podinfo-668b5b9b5b-tbdvj             1/1     Running   0          5m30s
podinfo-668b5b9b5b-z6dlh             1/1     Running   0          5m45s
</code></pre>
<p>Please be noted the scale up is a phased process rather than a sudden event to scale to max. 
And it'll be scaled down to a balanced status when CPU workload is down.</p>
<pre><code>kubectl get hpa -w
</code></pre>
<p>After several hours, we can see below result with above command.</p>
<pre><code>NAME    REFERENCE            TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
nginx   Deployment/podinfo   0%/50%    2         10        2          8h 
</code></pre>
<h2 id="10service">10.Service</h2>
<h3 id="clusterip">ClusterIP</h3>
<h4 id="create-service">Create Service</h4>
<p>Create a Deployment <code>http-app</code>.
Create a Service with same name and link with Development by Label Selector. 
Service type is <code>ClusterIP</code>, which is default type and accessable internally. </p>
<p>Create yaml file <code>svc-clusterip.yaml</code> and apply it to create Deployment and Service <code>http-app</code>.</p>
<pre><code>cat &gt; svc-clusterip.yaml &lt;&lt;EOF
apiVersion: v1
kind: Service
metadata:
  name: httpd-app
spec:
  type: ClusterIP
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  selector:
    app: httpd
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd-app
spec:
  selector:
    matchLabels:
      app: httpd
  replicas: 2
  template:
    metadata:
      labels:
        app: httpd
    spec:
      containers:
      - name: httpd
        image: httpd
        ports:
        - containerPort: 80
EOF

kubectl apply -f svc-clusterip.yaml
</code></pre>
<p>Execute command <code>kubectl get deployment,service,pod -o wide</code> to check resources we created. </p>
<pre><code>NAME                        READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES   SELECTOR
deployment.apps/httpd-app   2/2     2            2           3m1s   httpd        httpd    app=httpd

NAME                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE    SELECTOR
service/httpd-app   ClusterIP   10.100.67.181   &lt;none&gt;        80/TCP    3m1s   app=httpd

NAME                             READY   STATUS    RESTARTS   AGE    IP            NODE     NOMINATED NODE   READINESS GATES
pod/httpd-app-6496d888c9-mg2jt   1/1     Running   0          3m1s   10.244.2.97   cka003   &lt;none&gt;           &lt;none&gt;
pod/httpd-app-6496d888c9-pdgq8   1/1     Running   0          3m1s   10.244.1.19   cka002   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Verify the access from node <code>cka001</code> to Pod IPs.</p>
<pre><code>curl 10.244.2.97
curl 10.244.1.19
</code></pre>
<p>And receive below successful information.</p>
<pre><code>&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;
</code></pre>
<p>Verify the access via ClusterIP with Port.</p>
<pre><code>curl 10.100.67.181:80
</code></pre>
<p>And receive below successful information.</p>
<pre><code>&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;
</code></pre>
<h4 id="expose-service_1">Expose Service</h4>
<p>Create and attach to a temporary Pod <code>Busybox</code> and use <code>nslookup</code> to verify DNS resolution. The option <code>--rm</code> means delete the Pod after exit.</p>
<pre><code>kubectl run -it nslookup --rm --image=busybox:1.28
</code></pre>
<p>After attach to the Pod, run command <code>nslookup httpd-app</code>. The IP address <code>10.100.67.181</code> of name <code>httpd-app</code> we received is the ClusterIP of Service <code>httpd-app</code>.</p>
<pre><code>/ # nslookup httpd-app
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      httpd-app
Address 1: 10.100.67.181 httpd-app.jh-namespace.svc.cluster.local
</code></pre>
<p>We can check the IP of temporary Pod <code>Busybox</code> in a new terminal by executing command <code>kubectl get pod -o wide</code>. The Pod <code>Busybox</code> has different IP <code>10.244.2.98</code>.</p>
<pre><code>root@cka001:~# kubectl get pod nslookup
NAME                         READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
nslookup                     1/1     Running   0          12m   10.244.2.98   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<h3 id="nodeport">NodePort</h3>
<p>Create and apply yaml file <code>svc-nodeport.yaml</code> to create a Service <code>httpd-app</code>.</p>
<pre><code>cat &gt; svc-nodeport.yaml &lt;&lt;EOF
apiVersion: v1
kind: Service
metadata:
  name: httpd-app
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 80
    nodePort: 30080
  selector:
     app: httpd
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd-app
spec:
  selector:
    matchLabels:
      app: httpd
  replicas: 2
  template:
    metadata:
      labels:
        app: httpd
    spec:
      containers:
      - name: httpd
        image: httpd
        ports:
        - containerPort: 80
EOF


kubectl apply -f svc-nodeport.yaml
</code></pre>
<p>We will receive below output. The command <code>kubectl apply -f &lt;yaml_file&gt;</code> will update configuration to existing resources.
Here the Service <code>httpd-app</code> is changed from <code>ClusterIP</code> to <code>NodePort</code> type. No change to the Deployment <code>httpd-app</code>.</p>
<pre><code>service/httpd-app configured
deployment.apps/httpd-app unchanged
</code></pre>
<p>Check the Service <code>httpd-app</code> via <code>kubectl get svc</code>. 
IP is the same.
Type is changed to NodePort.
Port numbers is changed from <code>80/TCP</code> to <code>80:30080/TCP</code>.</p>
<pre><code>NAME        TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
httpd-app   NodePort   10.100.67.181   &lt;none&gt;        80:30080/TCP   78m
</code></pre>
<p>Test the connection to the Service <code>httpd-app</code> via command <code>curl &lt;your_node_ip&gt;:30080</code>. It's node IP, not cluster IP, nor Pod IP.
We will receive below successful information.</p>
<pre><code>&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;
</code></pre>
<h3 id="special-service">Special Service</h3>
<h4 id="headless-service">Headless Service</h4>
<p>Create and apply yaml file <code>svc-headless.yaml</code> to create a <code>Headless Service</code>.</p>
<pre><code>cat &gt; svc-headless.yaml &lt;&lt;EOF
apiVersion: v1
kind: Service
metadata:
  name: web
  labels:
    app: web
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: web
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: &quot;web&quot;
  replicas: 2
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: web
        image: nginx
        ports:
        - containerPort: 80
          name: web
EOF

kubectl apply -f svc-headless.yaml
</code></pre>
<p>Check Pos by command <code>kubectl get pod -owide -l app=web</code>.</p>
<pre><code>NAME    READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
web-0   1/1     Running   0          85s   10.244.2.99   cka003   &lt;none&gt;           &lt;none&gt;
web-1   1/1     Running   0          82s   10.244.1.20   cka002   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Get details of the Service by command <code>kubectl describe svc -l app=web</code>.</p>
<pre><code>Name:              web
Namespace:         jh-namespace
Labels:            app=web
Annotations:       &lt;none&gt;
Selector:          app=web
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                None
IPs:               None
Port:              web  80/TCP
TargetPort:        80/TCP
Endpoints:         10.244.1.20:80,10.244.2.99:80
Session Affinity:  None
Events:            &lt;none&gt;
</code></pre>
<p>启动一个Busybox Pod，使用 nslookup 来 测试 DNS 解析</p>
<p>Attach to the temporary Pod <code>Busybox</code> and use <code>nslookup</code> to verify DNS resolution.</p>
<pre><code>kubectl run -it nslookup --rm --image=busybox:1.28
</code></pre>
<p>With <code>nslookup</code> command for Headless Service <code>web</code>, we received two IP of Pods, not ClusterIP due to Headless Service. </p>
<pre><code>/ # nslookup web
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web
Address 1: 10.244.2.99 web-0.web.jh-namespace.svc.cluster.local
Address 2: 10.244.1.20 web-1.web.jh-namespace.svc.cluster.local
</code></pre>
<p>We can also use <code>nslookup</code> for <code>web-0.web</code> and <code>web-0.web</code>. Every Pod of Headless Service has own Service Name for DNS lookup.</p>
<pre><code>/ # nslookup web-0.web
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-0.web
Address 1: 10.244.2.99 web-0.web.jh-namespace.svc.cluster.local
</code></pre>
<p>Clean up all resources created before.</p>
<h2 id="11ingress">11.Ingress</h2>
<h3 id="deploy-ingress-controller">Deploy Ingress Controller</h3>
<p>Get Ingress Controller yaml file.</p>
<pre><code>wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.2.1/deploy/static/provider/cloud/deploy.yaml
</code></pre>
<p>Replace grc.io to Aliyun.</p>
<ul>
<li><code>k8s.gcr.io/ingress-nginx/kube-webhook-certgen</code> to <code>registry.aliyuncs.com/google_containers/kube-webhook-certgen</code>.</li>
<li><code>k8s.gcr.io/ingress-nginx/controller</code> to <code>registry.aliyuncs.com/google_containers/nginx-ingress-controller</code>.</li>
</ul>
<pre><code>sed -i 's/k8s.gcr.io\/ingress-nginx\/kube-webhook-certgen/registry.aliyuncs.com\/google\_containers\/kube-webhook-certgen/g' deploy.yaml
sed -i 's/k8s.gcr.io\/ingress-nginx\/controller/registry.aliyuncs.com\/google\_containers\/nginx-ingress-controller/g' deploy.yaml
</code></pre>
<p>Apply the yaml file <code>deploy.yaml</code> to create Ingress Nginx.</p>
<pre><code>kubectl apply -f deploy.yaml
</code></pre>
<p>Check the status of Pod.
Please be noted that a new namespace <code>ingress-nginx</code> was created and Ingress Nginx resources are running under the new namespace.</p>
<pre><code>kubectl get pod -n ingress-nginx
</code></pre>
<p>The result is below.</p>
<pre><code>NAME                                        READY   STATUS      RESTARTS   AGE
ingress-nginx-admission-create-dcsww        0/1     Completed   0          3m32s
ingress-nginx-admission-patch-hslwf         0/1     Completed   0          3m32s
ingress-nginx-controller-556fbd6d6f-trl9r   1/1     Running     0          3m32s
</code></pre>
<h3 id="create-deployments">Create Deployments</h3>
<p>Create two deployment <code>nginx-app-1</code> and <code>nginx-app-2</code>.</p>
<pre><code>cat &gt; nginx-app.yaml &lt;&lt; EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-app-1
spec:
  selector:
    matchLabels:
      app: nginx-app-1
  replicas: 1 
  template:
    metadata:
      labels:
        app: nginx-app-1
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
          - name: html
            mountPath: /usr/share/nginx/html
      volumes:
       - name: html
         hostPath:
           path: /root/html-1
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-app-2
spec:
  selector:
    matchLabels:
      app: nginx-app-2
  replicas: 1 
  template:
    metadata:
      labels:
        app: nginx-app-2
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
          - name: html
            mountPath: /usr/share/nginx/html
      volumes:
       - name: html
         hostPath:
           path: /root/html-2
EOF


kubectl apply -f nginx-app.yaml
</code></pre>
<p>Get status of Pods by executing <code>kubectl get pod -o wide</code>. Two Pods are running on node <code>cka003</code> with two different Pod IPs.</p>
<pre><code>NAME                           READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
nginx-app-1-695b7b647d-l76bh   1/1     Running   0          34s   10.244.2.104   cka003   &lt;none&gt;           &lt;none&gt;
nginx-app-2-7f6bf6f4d4-lvbz8   1/1     Running   0          34s   10.244.2.105   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Access to two Pod via curl. We get <code>403</code> error.</p>
<pre><code>curl 10.244.2.104
</code></pre>
<p>Log onto node <code>cka003</code> and create <code>index.html</code> file in path <code>/root/html-1/</code>. The directory <code>/root/html-1/</code> is already in place after <code>nginx-app-1</code> and <code>nginx-app-2</code> created.</p>
<pre><code>echo 'This is test 1 !!' &gt; /root/html-1/index.html
echo 'This is test 2 !!' &gt; /root/html-2/index.html
</code></pre>
<p>Check Pods status again by executing <code>kubectl get pod -o wide</code>.</p>
<pre><code>NAME                           READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
nginx-app-1-695b7b647d-l76bh   1/1     Running   0          6m11s   10.244.2.104   cka003   &lt;none&gt;           &lt;none&gt;
nginx-app-2-7f6bf6f4d4-lvbz8   1/1     Running   0          6m11s   10.244.2.105   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Access to two Pod via curl. </p>
<pre><code>curl 10.244.2.104
curl 10.244.2.105
</code></pre>
<p>We get correct information now.</p>
<pre><code>This is test 1 !!
This is test 2 !!
</code></pre>
<h3 id="create-service_1">Create Service</h3>
<p>Create Service <code>nginx-app-1</code> and <code>nginx-app-2</code> and map to related deployment <code>nginx-app-1</code> and <code>nginx-app-2</code>.</p>
<pre><code>cat &gt; nginx-svc.yaml &lt;&lt; EOF
apiVersion: v1
kind: Service
metadata:
 name: nginx-app-1
spec:
 ports:
 - protocol: TCP
   port: 80
   targetPort: 80
 selector:
   app: nginx-app-1
---
kind: Service
apiVersion: v1
metadata:
 name: nginx-app-2
spec:
 ports:
 - protocol: TCP
   port: 80
   targetPort: 80
 selector:
   app: nginx-app-2
EOF


kubectl apply -f nginx-svc.yaml
</code></pre>
<p>Check the status by executing <code>kubectl get svc -o wide</code>.</p>
<pre><code>NAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE   SELECTOR
nginx-app-1   ClusterIP   10.111.95.99   &lt;none&gt;        80/TCP    22s   app=nginx-app-1
nginx-app-2   ClusterIP   10.96.15.218   &lt;none&gt;        80/TCP    22s   app=nginx-app-2
</code></pre>
<p>Access to two Service via curl. </p>
<pre><code>curl 10.111.95.99
curl 10.96.15.218
</code></pre>
<p>We get correct information.</p>
<pre><code>This is test 1 !!
This is test 2 !!
</code></pre>
<h3 id="create-ingress">Create Ingress</h3>
<p>Create Ingress resource via file <code>nginx-app-ingress.yaml</code>. 
Map to two Services <code>nginx-app-1</code> and <code>nginx-app-1</code> we created..
Change the namespace if needed.</p>
<pre><code>cat &gt; nginx-app-ingress.yaml &lt;&lt; EOF
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-app
  namespace: jh-namespace
spec:
  ingressClassName: &quot;nginx&quot;
  rules:
  - host: app1.com
    http:
      paths:
      - pathType: Prefix
        path: &quot;/&quot;
        backend:
          service:
            name: nginx-app-1
            port: 
              number: 80
  - host: app2.com
    http:
      paths:
      - pathType: Prefix
        path: &quot;/&quot;
        backend:
          service:
            name: nginx-app-2
            port:
              number: 80
EOF


kubectl apply -f nginx-app-ingress.yaml
</code></pre>
<p>Get Ingress status by executing command <code>kubectl get ingress</code>.</p>
<pre><code>NAME        CLASS   HOSTS               ADDRESS   PORTS   AGE
nginx-app   nginx   app1.com,app2.com             80      64s
</code></pre>
<h3 id="test-accessiblity">Test Accessiblity</h3>
<p>By executing <code>kubectl get pod -n ingress-nginx -o wide</code>, we know Ingress Controllers are running on node <code>cka002</code>.</p>
<pre><code>NAME                                        READY   STATUS      RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
ingress-nginx-admission-create-dcsww        0/1     Completed   0          33m   10.244.2.102   cka003   &lt;none&gt;           &lt;none&gt;
ingress-nginx-admission-patch-hslwf         0/1     Completed   0          33m   10.244.2.103   cka003   &lt;none&gt;           &lt;none&gt;
ingress-nginx-controller-556fbd6d6f-trl9r   1/1     Running     0          33m   10.244.1.22    cka002   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>By executing <code>kubectl get node -o wide</code>, we know node <code>cka002</code>'s IP is <code>172.16.18.159</code>.</p>
<pre><code>NAME     STATUS   ROLES                  AGE   VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
cka001   Ready    control-plane,master   13d   v1.23.8   172.16.18.161   &lt;none&gt;        Ubuntu 20.04.4 LTS   5.4.0-113-generic   containerd://1.5.9
cka002   Ready    &lt;none&gt;                 13d   v1.23.8   172.16.18.160   &lt;none&gt;        Ubuntu 20.04.4 LTS   5.4.0-113-generic   containerd://1.5.9
cka003   Ready    &lt;none&gt;                 13d   v1.23.8   172.16.18.159   &lt;none&gt;        Ubuntu 20.04.4 LTS   5.4.0-113-generic   containerd://1.5.9
</code></pre>
<p>Add below into <code>/etc/hosts file</code> in one of node. Put node IP below. In above example, IP of node <code>cka002</code> is <code>172.16.18.160</code>, which is running <code>ingress-nginx-controller</code>.</p>
<pre><code>cat &gt;&gt; /etc/hosts &lt;&lt; EOF
172.16.18.160   app1.com
172.16.18.160   app2.com
EOF
</code></pre>
<p>By executing <code>kubectl -n ingress-nginx get svc</code> to get NodePort of Ingress Controller. </p>
<pre><code>NAME                                 TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx-controller             LoadBalancer   10.110.117.253   &lt;pending&gt;     80:31640/TCP,443:31338/TCP   40m
ingress-nginx-controller-admission   ClusterIP      10.107.32.104    &lt;none&gt;        443/TCP                      40m
</code></pre>
<p>Two files <code>index.html</code> are in two Pods, the web services are exposed to outside via node IP. 
Hence we can see below result. The <code>ingress-nginx-controller</code> plays a central entry point for outside access, and provide two ports for different backend services from Pods.</p>
<pre><code>curl app1.com:31640
This is test 1 !!
</code></pre>
<pre><code>curl app2.com:31338
This is test 2 !!
</code></pre>
<h2 id="12storage">12.Storage</h2>
<h3 id="emptydir">emptyDir</h3>
<p>Create a Pod with <code>emptyDir</code> type Volume.</p>
<pre><code>cat &gt; pod-emptydir.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
 name: hello-producer
spec:
 containers:
 - image: busybox
   name: producer
   volumeMounts:
   - mountPath: /producer_dir
     name: shared-volume
   args:
   - /bin/sh
   - -c
   - echo &quot;hello world&quot; &gt; /producer_dir/hello; sleep 30000
 volumes:
 - name: shared-volume
   emptyDir: {}
EOF


kubectl apply -f pod-emptydir.yaml
</code></pre>
<p>Check which node the Pod <code>hello-producer</code> is running. </p>
<pre><code>kubectl get pod hello-producer -owide
</code></pre>
<p>The Pod is running on node <code>cka003</code>.</p>
<pre><code>NAME             READY   STATUS    RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES
hello-producer   1/1     Running   0          106s   10.244.2.106   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Log onto <code>cka003</code> because the Pod <code>hello-producer</code> is running on the node.</p>
<p>Set up the environment <code>CONTAINER_RUNTIME_ENDPOINT</code> for command <code>crictl</code>. Suggest to do the same for all nodes.</p>
<pre><code>export CONTAINER_RUNTIME_ENDPOINT=unix:///run/containerd/containerd.sock
</code></pre>
<p>Run command <code>crictl ps</code> to get the container ID of Pod <code>hello-producer</code>.</p>
<pre><code>crictl ps |grep hello-producer
</code></pre>
<p>The ID of container <code>producer</code> is <code>05f5e1bb6a1bb</code>.</p>
<pre><code>CONTAINER           IMAGE               CREATED             STATE               NAME                ATTEMPT             POD ID              POD
05f5e1bb6a1bb       62aedd01bd852       15 minutes ago      Running             producer            0                   995cbc23eb763       hello-producer
</code></pre>
<p>Run command <code>crictl inspect</code> to get the path of mounted <code>shared-volume</code>, which is <code>emptyDir</code>.</p>
<pre><code>crictl inspect 05f5e1bb6a1bb | grep source | grep empty
</code></pre>
<p>The result is below.</p>
<pre><code>&quot;source&quot;: &quot;/var/lib/kubelet/pods/272ba5fa-e213-4c79-ab57-d4c91f4371ba/volumes/kubernetes.io~empty-dir/shared-volume&quot;,
</code></pre>
<p>Change the path to the path of mounted <code>shared-volume</code> we get above.</p>
<pre><code>cd /var/lib/kubelet/pods/272ba5fa-e213-4c79-ab57-d4c91f4371ba/volumes/kubernetes.io~empty-dir/shared-volume
cat hello
</code></pre>
<p>We will see the content of file <code>hello</code>. </p>
<p>The path <code>/producer_dir</code> inside the Pod is mounted to the local host path. 
The file <code>/producer_dir/hello</code> we created inside the Pod is actually in the host local path.</p>
<p>Let's delete the container <code>producer</code>, the file <code>hello</code> is still there.</p>
<pre><code>crictl ps
crictl stop &lt;your_container_id&gt;
crictl rm &lt;your_container_id&gt;
ls /var/lib/kubelet/pods/272ba5fa-e213-4c79-ab57-d4c91f4371ba/volumes/kubernetes.io~empty-dir/shared-volume
</code></pre>
<p>Let's delete the Pod <code>hello-producer</code>, the file <code>hello</code> was gone with error <code>No such file or directory</code>.</p>
<pre><code>kubectl delete pod hello-producer
ls /var/lib/kubelet/pods/272ba5fa-e213-4c79-ab57-d4c91f4371ba/volumes/kubernetes.io~empty-dir/shared-volume
</code></pre>
<h3 id="hostpath">hostPath</h3>
<p>Apply below yaml file to create a MySQL Pod and mount a <code>hostPath</code>.
It'll mount host directory <code>/tmp/mysql</code> to Pod directory <code>/var/lib/mysql</code>.
Check locally if directory <code>/tmp/mysql</code> is in place. If not, create it using <code>mkdir /tmp/mysql</code>.</p>
<pre><code>cat &gt; mysql-hostpath.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
 name: mysql
spec:
 selector:
   matchLabels:
     app: mysql
 template:
   metadata:
     labels:
       app: mysql
   spec:
     containers:
     - image: mysql:8.0
       name: mysql
       env:
       - name: MYSQL_ROOT_PASSWORD
         value: password
       ports:
       - containerPort: 3306
         name: mysql
       volumeMounts:
       - name: mysql-vol
         mountPath: /var/lib/mysql
     volumes:
     - hostPath:
            path: /tmp/mysql
       name: mysql-vol
EOF

kubectl apply -f mysql-hostpath.yaml
</code></pre>
<h4 id="verify-mysql-availability">Verify MySQL Availability</h4>
<p>Check status of MySQL Pod. Need document the Pod name and node it's running on.</p>
<pre><code>kubectl get pod -l app=mysql -o wide
</code></pre>
<p>Attach into the MySQL Pod on the running node.</p>
<pre><code>kubectl exec -it &lt;your_pod_name&gt; -- bash
</code></pre>
<p>Within the Pod, go to directory <code>/var/lib/mysql</code>, all files in the directory are same with all files in host directory <code>/tmp/mysql</code>.</p>
<p>Connect to the database in the Pod.</p>
<pre><code>mysql -h 127.0.0.1 -uroot -ppassword
</code></pre>
<p>Operate the database.</p>
<pre><code>mysql&gt; show databases;
mysql&gt; connect mysql;
mysql&gt; show tables;
</code></pre>
<h3 id="pv-and-pvc">PV and PVC</h3>
<p>Here we will use NFS as backend storage to demo how to deploy PV and PVC.</p>
<h4 id="set-up-nfs-server">Set up NFS Server</h4>
<h5 id="install-nfs-kernel-server">Install nfs-kernel-server</h5>
<p>Log onto <code>cka002</code>.</p>
<p>Choose one Worker <code>cka002</code> to build NFS server. </p>
<pre><code>sudo apt-get install -y nfs-kernel-server
</code></pre>
<h5 id="configure-share-folder">Configure Share Folder</h5>
<p>Create share folder.  </p>
<pre><code>mkdir /nfsdata
</code></pre>
<p>Append one line in file <code>/etc/exports</code>.</p>
<pre><code>cat &gt;&gt; /etc/exports &lt;&lt; EOF
/nfsdata *(rw,sync,no_root_squash)
EOF
</code></pre>
<p>There are many different NFS sharing options, including these:</p>
<ul>
<li><code>*</code>: accessable to all IPs, or specific IPs.</li>
<li><code>rw</code>: Share as read-write. Keep in mind that normal Linux permissions still apply. (Note that this is a default option.)</li>
<li><code>ro</code>: Share as read-only.</li>
<li><code>sync</code>: File data changes are made to disk immediately, which has an impact on performance, but is less likely to result in data loss. On som* `distributions this is the default.</li>
<li><code>async</code>: The opposite of sync; file data changes are made initially to memory. This speeds up performance but is more likely to result in data loss. O* `some distributions this is the default.</li>
<li><code>root_squash</code>: Map the root user and group account from the NFS client to the anonymous accounts, typically either the nobody account or the nfsnobod* `account. See the next section, “User ID Mapping,” for more details. (Note that this is a default option.)</li>
<li><code>no_root_squash</code>: Map the root user and group account from the NFS client to the local root and group accounts.</li>
</ul>
<p>We will use password-free remote mount based on <code>nfs</code> and <code>rpcbind</code> services between Linux servers, not based on <code>smb</code> service. 
The two servers must first grant credit, install and set up nfs and rpcbind services on the server side, set the common directory, start the service, and mount it on the client</p>
<p>Start <code>rpcbind</code> service.</p>
<pre><code>sudo systemctl enable rpcbind
sudo systemctl restart rpcbind
</code></pre>
<p>Start <code>nfs</code> service.</p>
<pre><code>sudo systemctl enable nfs-server
sudo systemctl start nfs-server
</code></pre>
<p>Once <code>/etc/exports</code> is changed, we need run below command to make change effected.</p>
<pre><code>exportfs -ra
</code></pre>
<p>Check whether sharefolder is configured. </p>
<pre><code>showmount -e
</code></pre>
<p>And see below output.</p>
<pre><code>Export list for cka002:
/nfsdata *
</code></pre>
<h5 id="install-nfs-client">Install NFS Client</h5>
<p>Install NFS client on all nodes.</p>
<pre><code>sudo apt-get install -y nfs-common
</code></pre>
<h5 id="verify-nfs-server">Verify NFS Server</h5>
<p>Log onto any nodes to verify NFS service and sharefolder list.</p>
<p>Log onto <code>cka001</code> and check sharefolder status on <code>cka002</code>.</p>
<pre><code>showmount -e cka002
</code></pre>
<p>Below result will be shown if no issues.</p>
<pre><code>Export list for cka002:
/nfsdata *
</code></pre>
<h5 id="mount-nfs">Mount NFS</h5>
<p>Execute below command to mount remote NFS folder on any other non-NFS-server node, e.g., <code>cka001</code> or <code>cka003</code>.</p>
<pre><code>mkdir /remote-nfs-dir
mount -t nfs cka002:/nfsdata /remote-nfs-dir/
</code></pre>
<p>Use command <code>df -h</code> to verify mount point. Below is the sample output.</p>
<pre><code>Filesystem       Size  Used Avail Use% Mounted on
cka002:/nfsdata   40G  6.8G   31G  18% /remote-nfs-dir
</code></pre>
<h4 id="create-pv">Create PV</h4>
<p>Create a PV with below yaml file <code>mysql-pv.yaml</code>. Replace the NFS Server IP with actual IP that NFS server is running on.</p>
<pre><code>cat &gt; mysql-pv.yaml &lt;&lt;EOF
apiVersion: v1
kind: PersistentVolume
metadata:
 name: mysql-pv
spec:
 accessModes:
     - ReadWriteOnce
 capacity:
   storage: 1Gi
 persistentVolumeReclaimPolicy: Retain
 storageClassName: nfs
 nfs:
   path: /nfsdata/
   server: 172.16.18.160
EOF

kubectl apply -f mysql-pv.yaml
</code></pre>
<p>Check the PV.</p>
<pre><code>kubectl get pv
</code></pre>
<p>The result:</p>
<pre><code>NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
mysql-pv   1Gi        RWO            Retain           Available           nfs                     9s
</code></pre>
<h4 id="create-pvc">Create PVC</h4>
<p>Create a PVC with below yaml file <code>mysql-pvc.yaml</code>, specify storage size, access mode, and storage class. 
The PVC will be binded with PV automatically viw storage class name. </p>
<pre><code>cat &gt; mysql-pvc.yaml &lt;&lt;EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: nfs
EOF

kubectl apply -f mysql-pvc.yaml
</code></pre>
<h4 id="consume-pvc">Consume PVC</h4>
<p>Create a Deployment to consume the PVC created.</p>
<pre><code>cat &gt; mysql.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
 name: mysql
spec:
 selector:
   matchLabels:
     app: mysql
 template:
   metadata:
     labels:
       app: mysql
   spec:
     containers:
     - image: mysql:8.0
       name: mysql
       env:
       - name: MYSQL_ROOT_PASSWORD
         value: password
       ports:
       - containerPort: 3306
         name: mysql
       volumeMounts:
       - name: mysql-persistent-storage
         mountPath: /var/lib/mysql
         subPath: mysqldata
     volumes:
     - name: mysql-persistent-storage
       persistentVolumeClaim:
        claimName: mysql-pvc
EOF


kubectl apply -f mysql.yaml
</code></pre>
<p>Now we can see MySQL files were moved to directory <code>/nfsdata</code> on <code>cka002</code></p>
<h3 id="storageclass">StorageClass</h3>
<h4 id="configure-rbac-authorization">Configure RBAC Authorization</h4>
<p>RBAC authorization uses the rbac.authorization.k8s.io API group to drive authorization decisions, allowing you to dynamically configure policies through the Kubernetes API.</p>
<ul>
<li>ServiceAccount: <code>nfs-client-provisioner</code></li>
<li>
<p>namespace: <code>jh-namespace</code></p>
</li>
<li>
<p>ClusterRole: <code>nfs-client-provisioner-runner</code></p>
</li>
<li>
<p>ClusterRoleBinding: <code>run-nfs-client-provisioner</code>, roleRefer: above ClusterRole, link to above ServiceAccount.</p>
</li>
<li>
<p>Role: <code>leader-locking-nfs-client-provisioner</code></p>
</li>
<li>RoleBinding: <code>leader-locking-nfs-client-provisioner</code>, roleRefer: above Role, link to above ServiceAccount.</li>
</ul>
<p>Create RBAC Authorization.</p>
<pre><code>cat &gt; nfs-provisioner-rbac.yaml &lt;&lt;EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: jh-namespace
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [&quot;&quot;]
    resources: [&quot;nodes&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;persistentvolumes&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;persistentvolumeclaims&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]
  - apiGroups: [&quot;storage.k8s.io&quot;]
    resources: [&quot;storageclasses&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;events&quot;]
    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: jh-namespace
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: jh-namespace
rules:
  - apiGroups: [&quot;&quot;]
    resources: [&quot;endpoints&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: jh-namespace
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: jh-namespace
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io
EOF


kubectl apply -f nfs-provisioner-rbac.yaml
</code></pre>
<h4 id="install-nfs-provisioner">Install <code>nfs-provisioner</code></h4>
<p>Create NFS Provisioner with below yaml file. </p>
<pre><code>cat &gt; nfs-provisioner-deployment.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nfs-client-provisioner
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: liyinlin/nfs-subdir-external-provisioner:v4.0.2
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: k8s-sigs.io/nfs-subdir-external-provisioner
            - name: NFS_SERVER
              value: 172.16.18.160
            - name: NFS_PATH
              value: /nfsdata
      volumes:
        - name: nfs-client-root
          nfs:
            server: 172.16.18.160
            path: /nfsdata
EOF

kubectl apply -f nfs-provisioner-deployment.yaml
</code></pre>
<h4 id="create-nfs-storageclass">Create NFS StorageClass</h4>
<p>Create yaml file <code>nfs-storageclass.yaml</code>.</p>
<pre><code>vi nfs-storageclass.yaml
</code></pre>
<p>And add below info to create NFS StorageClass.</p>
<pre><code>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-client
  annotations:
    storageclass.kubernetes.io/is-default-class: &quot;true&quot;
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
parameters:
  pathPattern: &quot;${.PVC.namespace}/${.PVC.annotations.nfs.io/storage-path}&quot;
  onDelete: delete
</code></pre>
<p>Apply the yaml file.</p>
<pre><code>kubectl apply -f nfs-storageclass.yaml
</code></pre>
<h4 id="verify">Verify</h4>
<h5 id="create-pvc_1">Create PVC</h5>
<p>Create PVC </p>
<pre><code>cat &gt; storageclass-pvc.yaml &lt;&lt;EOF
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: nfs-pvc-from-sc
spec:
  storageClassName: nfs-client
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
EOF

kubectl apply -f storageclass-pvc.yaml
</code></pre>
<p>Check the PVC status we ceated.</p>
<pre><code>kubectl get pvc nfs-pvc-from-sc
</code></pre>
<pre><code>NAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
nfs-pvc-from-sc   Bound    pvc-25eaa043-911e-46c7-b17e-f65256f52725   1Gi        RWX            nfs-client     39h
</code></pre>
<h5 id="consume-pvc_1">Consume PVC</h5>
<p>Create Pod to consume the PVC&gt;</p>
<pre><code>cat &gt; mysql-with-sc-pvc.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-with-sc-pvc
spec:
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - image: mysql:8.0
        name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: password
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: mysql-pv
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-pv
        persistentVolumeClaim:
          claimName: nfs-pvc-from-sc
EOF


kubectl apply -f mysql-with-sc-pvc.yaml
</code></pre>
<p>Check the Deployment status.</p>
<pre><code>kubectl get deployment mysql-with-sc-pvc -o wide
</code></pre>
<pre><code>NAME                READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES      SELECTOR
mysql-with-sc-pvc   1/1     1            1           39h   mysql        mysql:8.0   app=mysql
</code></pre>
<p>Check related Pods status. Be noted that the Pod <code>mysql-with-sc-pvc-7c97d875f8-dwfkc</code> is running on <code>cka003</code>.</p>
<pre><code>kubectl get pod -o wide -l app=mysql
</code></pre>
<pre><code>NAME                                 READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
mysql-774db46945-sztrp               1/1     Running   0          40h   10.244.1.23    cka002   &lt;none&gt;           &lt;none&gt;
mysql-with-sc-pvc-7c97d875f8-dwfkc   1/1     Running   0          38h   10.244.2.110   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Let's check directory <code>/nfsdata/</code> on NFS server. </p>
<pre><code>ll /nfsdata/
</code></pre>
<p>Two folders were created. Same content of <code>/remote-nfs-dir/</code> on other nodes.</p>
<pre><code>drwxrwxrwx  6 systemd-coredump root 4096 Jul 10 23:08 jh-namespace/
drwxr-xr-x  6 systemd-coredump root 4096 Jul 10 21:23 mysqldata/
</code></pre>
<p>One sub-folder's name is namespace under directory <code>/nfsdata/</code> and it is mounted to Pod.
By default, namespace name will be used at mount point. 
If we want to use customized folder for that purpose, we need claim an annotation <code>nfs.io/storage-path</code>, e.g., <code>test-path</code> in below example.</p>
<pre><code>cat &gt; test-claim.yaml &lt;&lt;EOF
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-claim
  namespace: kube-system
  annotations:
    nfs.io/storage-path: &quot;test-path&quot;
spec:
  storageClassName: nfs-client
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
EOF

kubectl apply -f test-claim.yaml
</code></pre>
<p>In above case, the PVC was created in <code>kube-system</code> Namespace, hence we can see directory <code>test-path</code> is under directory<code>kube-system</code>. 
Overall directory structure of folder <code>/nfsdata/</code> looks like below.</p>
<pre><code>tree /nfsdata/
</code></pre>
<pre><code>/nfsdata/
├── jh-namespace
│   ├── mysql.sock -&gt; /var/run/mysqld/mysqld.sock
│   ├── sys
│   │   └── sys_config.ibd
│   ├── undo_001
│   └── undo_002
├── kube-system
│   └── test-path
└── mysqldata
</code></pre>
<p>Please be noted that above rule is following <code>nfs-subdir-external-provisioner</code> implementation. It's may be different with other <code>provisioner</code>.</p>
<p>Detail about <code>nfs-subdir-external-provisioner</code> project is <a href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner">here</a></p>
<hr />
<h3 id="configuration">Configuration</h3>
<h4 id="configmap">ConfigMap</h4>
<p>Create a yaml file <code>configmap.yaml</code> for ConfigMap.</p>
<pre><code>vi configmap.yaml
</code></pre>
<p>Paste below content.</p>
<pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    cattle.io/creator: norman
  name: nginx
  namespace: jh-namespace
data:
  nginx.conf: |-
    user  nginx;
    worker_processes  2;

    error_log  /var/log/nginx/error.log warn;
    pid        /var/run/nginx.pid;


    events {
        worker_connections  1024;
    }


    http {
        include       /etc/nginx/mime.types;
        default_type  application/octet-stream;

        log_format  main  '$remote_addr - $remote_user [$time_local] &quot;$request&quot; '
                          '$status $body_bytes_sent &quot;$http_referer&quot; '
                          '&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;';

        access_log  /var/log/nginx/access.log  main;

        sendfile        on;
        #tcp_nopush     on;

        keepalive_timeout  65;

        #gzip  on;

        include /etc/nginx/conf.d/*.conf;
    }
</code></pre>
<p>Apply the ConfigMap.</p>
<pre><code>kubectl apply -f configmap.yaml
</code></pre>
<p>Create Pod <code>nginx-with-cm</code>.</p>
<pre><code>cat &gt; nginx-with-cm.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: nginx-with-cm
spec:
 containers:
 - name: nginx
   image: nginx
   volumeMounts:
   - name: foo
     mountPath: &quot;/etc/nginx/nginx.conf&quot;
     subPath:  nginx.conf
 volumes:
 - name: foo
   configMap:
     name: nginx
EOF

kubectl apply -f nginx-with-cm.yaml
</code></pre>
<p>Be noted:</p>
<ul>
<li>By default to mount ConfigMap, Kubernetes will overwrite all content of the mount point. We can use <code>volumeMounts.subPath</code> to specify that only overwrite the file <code>nginx.conf</code>.</li>
<li>Is we use <code>volumeMounts.subPath</code> to mount a Container Volume, Kubernetes won't do hot update to reflect real-time update.</li>
</ul>
<p>Verify if the <code>nginx.conf</code> mounted from outside is in the Container by comparing with above file.</p>
<pre><code>kubectl exec -it nginx-with-cm -- sh 
cat /etc/nginx/nginx.conf
</code></pre>
<h4 id="secret">Secret</h4>
<p>Encode password with base64  </p>
<pre><code>echo -n admin | base64  
YWRtaW4=

echo -n 123456 | base64
MTIzNDU2
</code></pre>
<p>Create Secret.</p>
<pre><code>cat &gt; secret.yaml &lt;&lt;EOF
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
data:
  username: YWRtaW4=
  password: MTIzNDU2
EOF

kubectl apply -f secret.yaml
</code></pre>
<p>Using Volume to mount (injection) Secret to a Pod.</p>
<pre><code>cat &gt; busybox-with-secret.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: busybox-with-secret
spec:
 containers:
 - name: mypod
   image: busybox
   args:
    - /bin/sh
    - -c
    - sleep 1000000;
   volumeMounts:
   - name: foo
     mountPath: &quot;/tmp/secret&quot;
 volumes:
 - name: foo
   secret:
    secretName: mysecret
EOF

kubectl apply -f busybox-with-secret.yaml
</code></pre>
<p>Let's attach to the Pod <code>busybox-with-secret</code> to verify if two data elements of <code>mysecret</code> are successfully mounted to the path <code>/tmp/secret</code> within the Pod.</p>
<pre><code>kubectl exec -it busybox-with-secret -- sh
</code></pre>
<p>By executing below command, we can see two data elements are in the directory <code>/tmp/secret</code> as file type. </p>
<pre><code>/ # ls -l /tmp/secret/
lrwxrwxrwx    1 root     root            15 Jul 12 16:13 password -&gt; ..data/password
lrwxrwxrwx    1 root     root            15 Jul 12 16:13 username -&gt; ..data/username
</code></pre>
<p>And we can get the content of each element, which are predefined before.</p>
<pre><code>/ # cat /tmp/secret/username
admin

/ # cat /tmp/secret/password
123456
</code></pre>
<h4 id="additional-keys">Additional Keys</h4>
<h5 id="various-way-to-create-configmap">Various way to create ConfigMap</h5>
<p>ConfigMap can be created by file, directory, or value. </p>
<p>Let's create a ConfigMap <code>colors</code> includes:</p>
<ul>
<li>Four files with four color names.</li>
<li>One file with favorite color name.</li>
</ul>
<pre><code>mkdir configmap
cd configmap
mkdir primary

echo c &gt; primary/cyan
echo m &gt; primary/magenta
echo y &gt; primary/yellow
echo k &gt; primary/black
echo &quot;known as key&quot; &gt;&gt; primary/black
echo blue &gt; favorite
</code></pre>
<p>Final structure looks like below via command <code>tree configmap</code>.</p>
<pre><code>configmap
├── favorite
└── primary
    ├── black
    ├── cyan
    ├── magenta
    └── yellow
</code></pre>
<p>Create ConfigMap referring above files we created. Make sure we are now in the path <code>~/configmap</code>.</p>
<pre><code>kubectl create configmap colors \
--from-literal=text=black  \
--from-file=./favorite  \
--from-file=./primary/
</code></pre>
<p>Check content of the ConfigMap <code>colors</code>.</p>
<pre><code>kubectl get configmap colors -o yaml
</code></pre>
<pre><code>apiVersion: v1
data:
  black: |
    k
    known as key
  cyan: |
    c
  favorite: |
    blue
  magenta: |
    m
  text: black
  yellow: |
    y
kind: ConfigMap
metadata:
  creationTimestamp: &quot;2022-07-12T16:38:27Z&quot;
  name: colors
  namespace: jh-namespace
  resourceVersion: &quot;2377258&quot;
  uid: d5ab133f-5e4d-41d4-bc9e-2bbb22a872a1
</code></pre>
<h5 id="set-environment-variable-via-configmap">Set environment variable via ConfigMap</h5>
<p>Here we will create a Pod <code>pod-configmap-env</code> and set the environment variable <code>ilike</code> and assign value of <code>favorite</code> from ConfigMap <code>colors</code>.</p>
<pre><code>cat &gt; pod-configmap-env.yaml &lt;&lt; EOF
apiVersion: v1
kind: Pod
metadata:
  name: pod-configmap-env
spec:
  containers:
  - name: nginx
    image: nginx
    env:
    - name: ilike
      valueFrom:
        configMapKeyRef:
          name: colors
          key: favorite
EOF

kubectl apply -f pod-configmap-env.yaml
</code></pre>
<p>Attach to the Pod <code>pod-configmap-env</code>.</p>
<pre><code>kubectl exec -it pod-configmap-env -- bash
</code></pre>
<p>Verify the value of env variable <code>ilike</code> is <code>blue</code>, which is the value of <code>favorite</code> of ConfigMap <code>colors</code>.</p>
<pre><code>root@pod-configmap-env:/# echo $ilike
blue
</code></pre>
<p>We can also use all key-value of ConfigMap to set up environment variables of Pod.</p>
<pre><code>cat &gt; pod-configmap-env-2.yaml &lt;&lt; EOF
apiVersion: v1
kind: Pod
metadata:
 name: pod-configmap-env-2
spec:
 containers:
 - name: nginx
   image: nginx
   envFrom:
    - configMapRef:
        name: colors
EOF

kubectl apply -f pod-configmap-env-2.yaml
</code></pre>
<p>Attach to the Pod <code>pod-configmap-env-2</code>.</p>
<pre><code>kubectl exec -it pod-configmap-env-2 -- bash
</code></pre>
<p>Verify the value of env variables based on key-values we defined in ConfigMap <code>colors</code>.</p>
<pre><code>root@pod-configmap-env-2:/# echo $black
k known as key
root@pod-configmap-env-2:/# echo $cyan
c
root@pod-configmap-env-2:/# echo $favorite
blue
</code></pre>
<h2 id="13scheduling">13.Scheduling</h2>
<h3 id="nodeselector">nodeSelector</h3>
<p>Let's assume the scenario below.</p>
<ul>
<li>We have a group of high performance servers.</li>
<li>Some applications require high performance computing.</li>
<li>These applicaiton need to be scheduled and running on those high performance servers.</li>
</ul>
<p>We can leverage Kubernetes attributes node <code>label</code> and <code>nodeSelector</code> to group resources as a whole for scheduling to meet above requirement.</p>
<h4 id="label-node_1">Label Node</h4>
<p>Here I will label <code>cka002</code> with <code>Configuration=hight</code>.</p>
<pre><code>kubectl label node cka002 configuration=hight
</code></pre>
<p>Verify. We wil see the label <code>configuration=hight</code> on <code>cka002</code>.</p>
<pre><code>kubectl get node --show-labels
</code></pre>
<h4 id="configure-nodeselector-for-pod">Configure nodeSelector for Pod</h4>
<p>Create a Pod and use <code>nodeSelector</code> to schedule the Pod running on specified node.</p>
<pre><code>cat &gt; mysql-nodeselector.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
 name: mysql-nodeselector
spec:
 selector:
   matchLabels:
     app: mysql
 template:
   metadata:
     labels:
       app: mysql
   spec:
     containers:
     - image: mysql:8.0
       name: mysql
       env:
       - name: MYSQL_ROOT_PASSWORD
         value: password
       ports:
       - containerPort: 3306
         name: mysql
     nodeSelector:
       configuration: hight
EOF

kubectl apply -f mysql-nodeselector.yaml
</code></pre>
<p>Let's check with node the Pod <code>mysql-nodeselector</code> is running.</p>
<pre><code>kubectl get pod -l app=mysql -o wide |  grep mysql-nodeselector
</code></pre>
<p>With below result, Pod <code>mysql-nodeselector</code> is running on <code>cka002</code> node.</p>
<pre><code>NAME                                  READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
mysql-nodeselector-6b7d9c875d-227t6   1/1     Running   0          50s     10.244.1.26    cka002   &lt;none&gt;           &lt;none&gt;
</code></pre>
<h3 id="nodename">nodeName</h3>
<p>Be noted, <code>nodeName</code> has hightest priority as it's not scheduled by <code>Scheduler</code>.</p>
<p>Create a Pod <code>nginx-nodename</code> with <code>nodeName=cka003</code>.</p>
<pre><code>cat &gt; nginx-nodename.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: nginx-nodename
spec:
  containers:
  - name: nginx
    image: nginx
  nodeName: cka003
EOF

kubectl apply -f nginx-nodename.yaml
</code></pre>
<p>Verify if Pod <code>nginx-nodename</code> is running on `ckc003 node.</p>
<pre><code>kubectl get pod -owide |grep nginx-nodename
</code></pre>
<pre><code>NAME                                      READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
nginx-nodename                            1/1     Running   0          6s      10.244.2.113   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<h3 id="affinity">Affinity</h3>
<p>In Kubernetes cluster, some Pods have frequent interaction with other Pods. With that situation, it's suggested to schedule these Pods running on same node. 
For example, Two Pods Nginx and Mysql, we need deploy them on one node if they frequently communicate.</p>
<p>We can use <code>podAffinity</code> to select Pods based on their relationship. </p>
<p>There are two scheduling type of <code>podAffinity</code>.</p>
<ul>
<li><code>requiredDuringSchedulingIgnoredDuringExecution</code>(硬亲和)</li>
<li><code>preferredDuringSchedulingIgnoredDuringExecution</code>(软亲和)</li>
</ul>
<p><code>topologyKey</code> could be set by below types:</p>
<ul>
<li><code>kubernetes.io/hostname</code> ＃NodeName</li>
<li><code>failure-domain.beta.kubernetes.io/zone</code> ＃Zone </li>
<li><code>failure-domain.beta.kubernetes.io/region</code> # Region </li>
</ul>
<p>We can set node Label to classify Name/Zone/Region of node, which can be used by <code>podAffinity</code>.</p>
<p>Create a Pod Nginx.</p>
<pre><code>cat &gt; pod_nginx.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx
EOF


kubectl apply -f pod_nginx.yaml
</code></pre>
<p>Create a Pod MySql.</p>
<pre><code>cat &gt; pod_mysql.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  containers:
  - name: mysql
    image: mysql
    env:
     - name: &quot;MYSQL_ROOT_PASSWORD&quot;
       value: &quot;123456&quot;
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - nginx
        topologyKey: kubernetes.io/hostname
EOF

kubectl apply -f pod_mysql.yaml
</code></pre>
<p>As we configured <code>podAffinity</code>, so Pod <code>mysql</code> will be scheduled to the same node with Pod <code>nginx</code> by Label <code>app:nginx</code>.</p>
<p>Via the command <code>kubectl get pod -o wide</code> we can get two Pods are running on node <code>cka002</code>.</p>
<pre><code>NAME                                      READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
mysql                                     1/1     Running   0          2m48s   10.244.1.28    cka002   &lt;none&gt;           &lt;none&gt;
nginx                                     1/1     Running   0          9m53s   10.244.1.27    cka002   &lt;none&gt;           &lt;none&gt;
</code></pre>
<h3 id="taints-tolerations">Taints &amp; Tolerations</h3>
<h4 id="concept">Concept</h4>
<p>Node affinity is a property of Pods that attracts them to a set of nodes (either as a preference or a hard requirement). 
Taints are the opposite -- they allow a node to repel a set of pods.</p>
<p>Tolerations are applied to pods. 
Tolerations allow the scheduler to schedule pods with matching taints. 
Tolerations allow scheduling but don't guarantee scheduling: the scheduler also evaluates other parameters as part of its function.</p>
<p>Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. 
One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints.</p>
<p>In the production environment, we generally configure Taints for Control Plane nodes (in fact, most K8s installation tools automatically add Taints to Control Plane nodes), because Control Plane only runs Kubernetes system components. 
If it is used to run application Pods, it is easy to consume resources. In the end, the Control Plane node will crash. 
If we need to add additional system components to the Control Plane node later, we can configure Tolerations for the corresponding system component Pod to tolerate taints.</p>
<h4 id="set-taints">Set Taints</h4>
<p>Set <code>cka003</code> node to taint node. Set status to <code>NoSchedule</code>, which won't impact existing Pods running on <code>cka003</code>.</p>
<pre><code>kubectl taint nodes cka003 key=value:NoSchedule
</code></pre>
<h4 id="set-tolerations">Set Tolerations</h4>
<p>We can use Tolerations to let Pods schedule to a taint node. </p>
<pre><code>cat &gt; mysql-tolerations.yaml &lt;&lt;EOF
apiVersion: apps/v1
kind: Deployment
metadata:
 name: mysql-tolerations
spec:
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - image: mysql:8.0
        name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: password
        ports:
        - containerPort: 3306
          name: mysql
      tolerations:
      - key: &quot;key&quot;
        operator: &quot;Equal&quot;
        value: &quot;value&quot;
        effect: &quot;NoSchedule&quot;
EOF

kubectl apply -f mysql-tolerations.yaml
</code></pre>
<p>The Pod of Deployment <code>mysql-tolerations</code> is scheduled and running on node <code>cka003</code> with <code>tolerations</code> setting, which is a taint node.</p>
<pre><code>kubectl get pod -o wide | grep mysql-tolerations
</code></pre>
<pre><code>NAME                                      READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
mysql-tolerations-5c5986944b-cg9bs        1/1     Running   0          57s     10.244.2.114   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<h4 id="remove-taints">Remove Taints</h4>
<pre><code>kubectl taint nodes cka003 key-
</code></pre>
<h2 id="14resourcequota">14.ResourceQuota</h2>
<h3 id="create-namespace">Create Namespace</h3>
<p>Ceate a Namespace</p>
<pre><code>kubectl create ns quota-object-example
</code></pre>
<h3 id="create-resourcequota">Create ResourceQuota</h3>
<p>Create a Namespace ResourceQuota and apply to namespace <code>quota-object-example</code>.
Within the namespace, we can only create 1 PVC, 1 LoadBalancer Service, can not create NodePort Service.</p>
<pre><code>cat &gt; resourcequota.yaml &lt;&lt;EOF
apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-quota-demo
  namespace: quota-object-example
spec:
  hard:
    persistentvolumeclaims: &quot;1&quot;
    services.loadbalancers: &quot;2&quot;
    services.nodeports: &quot;0&quot;
EOF


kubectl apply -f resourcequota.yaml
</code></pre>
<h3 id="verify-test">Verify &amp; Test</h3>
<p>Check Quota status</p>
<pre><code>kubectl get resourcequota object-quota-demo --namespace=quota-object-example --output=yaml
</code></pre>
<p>Key information is below. </p>
<pre><code>spec:
  hard:
    persistentvolumeclaims: &quot;1&quot;
    services.loadbalancers: &quot;2&quot;
    services.nodeports: &quot;0&quot;
status:
  hard:
    persistentvolumeclaims: &quot;1&quot;
    services.loadbalancers: &quot;2&quot;
    services.nodeports: &quot;0&quot;
  used:
    persistentvolumeclaims: &quot;0&quot;
    services.loadbalancers: &quot;0&quot;
    services.nodeports: &quot;0&quot;
</code></pre>
<h4 id="test-nodeport">Test NodePort</h4>
<p>Create a Deployment <code>ns-quota-test</code> on namespace <code>quota-object-example</code>.</p>
<pre><code>kubectl create deployment ns-quota-test --image nginx --namespace=quota-object-example
</code></pre>
<p>Expose the Deployment via NodePort    </p>
<pre><code>kubectl expose deployment ns-quota-test --port=80 --type=NodePort --namespace=quota-object-example
</code></pre>
<p>We receive below error, which is expected because we set Quota <code>services.nodeports: 0</code>.</p>
<pre><code class="language-shell">Error from server (Forbidden): services &quot;ns-quota-test&quot; is forbidden: exceeded quota: object-quota-demo, requested: services.nodeports=1, used: services.nodeports=0, limited: services.nodeports=0
</code></pre>
<h4 id="test-pvc">Test PVC</h4>
<p>Create a PVC <code>pvc-quota-demo</code> on namespace <code>quota-object-example</code>.</p>
<pre><code>cat &gt; test-pvc-quota.yaml &lt;&lt; EOF
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc-quota-demo
  namespace: quota-object-example
spec:
  storageClassName: nfs-client
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
EOF


kubectl apply -f test-pvc-quota.yaml
</code></pre>
<p>Check the Quota status.</p>
<pre><code>kubectl get resourcequota object-quota-demo --namespace=quota-object-example --output=yaml
</code></pre>
<p>Here <code>persistentvolumeclaims</code> is used <code>1</code>, and the quota is also <code>1</code>. If we create PVC again, will receive 403 error.</p>
<pre><code>spec:
  hard:
    persistentvolumeclaims: &quot;1&quot;
    services.loadbalancers: &quot;2&quot;
    services.nodeports: &quot;0&quot;
status:
  hard:
    persistentvolumeclaims: &quot;1&quot;
    services.loadbalancers: &quot;2&quot;
    services.nodeports: &quot;0&quot;
  used:
    persistentvolumeclaims: &quot;1&quot;
    services.loadbalancers: &quot;0&quot;
    services.nodeports: &quot;0&quot;
</code></pre>
<h2 id="15limitrange">15.LimitRange</h2>
<p>A <em>LimitRange</em> provides constraints that can:</p>
<ul>
<li>Enforce minimum and maximum compute resources usage per Pod or Container in a namespace.</li>
<li>Enforce minimum and maximum storage request per PersistentVolumeClaim in a namespace.</li>
<li>Enforce a ratio between request and limit for a resource in a namespace.</li>
<li>Set default request/limit for compute resources in a namespace and automatically inject them to Containers at runtime.</li>
</ul>
<h3 id="create-namespace_1">Create Namespace</h3>
<p>Create a Namespace <code>default-cpu-example</code> for demo.</p>
<pre><code>kubectl create namespace default-cpu-example
</code></pre>
<h3 id="set-limitrange">Set LimitRange</h3>
<p>Create LimitRange by below yaml file to define range of CPU Request and CPU Limit for a Container.
After apply LimitRange resource, the CPU limitation will affect all new created Pods.</p>
<pre><code>cat &gt; cpu-limitrange.yaml &lt;&lt; EOF
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
  namespace: default-cpu-example
spec:
  limits:
  - default:
      cpu: 1
    defaultRequest:
      cpu: 0.5
    type: Container
EOF


kubectl apply -f cpu-limitrange.yaml
</code></pre>
<h3 id="test-via-pod">Test via Pod</h3>
<h4 id="scenario-1-pod-without-specified-limits">Scenario 1: Pod without specified limits</h4>
<p>Create a Pod without any specified limits.</p>
<pre><code>cat &gt; default-cpu-demo-pod.yaml &lt;&lt; EOF
apiVersion: v1
kind: Pod
metadata:
  name: default-cpu-demo
  namespace: default-cpu-example
spec:
  containers:
  - name: default-cpu-demo-ctr
    image: nginx
EOF


kubectl apply -f default-cpu-demo-pod.yaml
</code></pre>
<p>Verify details of the Pod we created. The Pod inherits the both CPU Limits and CPU Requests from namespace as its default.</p>
<pre><code>kubectl get pod default-cpu-demo --output=yaml --namespace=default-cpu-example
</code></pre>
<pre><code>spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: default-cpu-demo-ctr
    resources:
      limits:
        cpu: &quot;1&quot;
      requests:
        cpu: 500m
</code></pre>
<h4 id="scenario-2-pod-with-cpu-limit-without-cpu-request">Scenario 2: Pod with CPU limit, without CPU Request</h4>
<p>Create Pod with specified CPU limits only.  </p>
<pre><code>cat &gt; default-cpu-demo-limit.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: default-cpu-demo-limit
  namespace: default-cpu-example
spec:
  containers:
  - name: default-cpu-demo-limit-ctr
    image: nginx
    resources:
      limits:
        cpu: &quot;1&quot;
EOF

kubectl apply -f default-cpu-demo-limit.yaml
</code></pre>
<p>Verify details of the Pod we created. The Pod inherits the CPU Request from namespace as its default and specifies own CPU Limits.</p>
<pre><code>kubectl get pod default-cpu-demo-limit --output=yaml --namespace=default-cpu-example
</code></pre>
<pre><code>spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: default-cpu-demo-limit-ctr
    resources:
      limits:
        cpu: &quot;1&quot;
      requests:
        cpu: &quot;1&quot;
</code></pre>
<h4 id="scenario-3-pod-with-cpu-request-onlyl-without-cpu-limits">Scenario 3: Pod with CPU Request onlyl, without CPU Limits</h4>
<p>Create Pod with specified CPU Request only. </p>
<pre><code>cat &gt; default-cpu-demo-request.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: default-cpu-demo-request
  namespace: default-cpu-example
spec:
  containers:
  - name: default-cpu-demo-request-ctr
    image: nginx
    resources:
      requests:
        cpu: &quot;0.75&quot;
EOF

kubectl apply -f default-cpu-demo-request.yaml
</code></pre>
<p>Verify details of the Pod we created. The Pod inherits the CPU Limits from namespace as its default and specifies own CPU Requests.</p>
<pre><code>kubectl get pod default-cpu-demo-request --output=yaml --namespace=default-cpu-example
</code></pre>
<pre><code>spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: default-cpu-demo-request-ctr
    resources:
      limits:
        cpu: &quot;1&quot;
      requests:
        cpu: 750m
</code></pre>
<h2 id="16troubleshooting">16.Troubleshooting</h2>
<h3 id="event_1">Event</h3>
<p>Usage:</p>
<pre><code>kubectl describe &lt;resource_type&gt; &lt;resource_name&gt; --namespace=&lt;namespace_name&gt;
</code></pre>
<p>Get event information of a Pod</p>
<p>Create a Tomcat Pod.</p>
<pre><code>kubectl run tomcat --image=tomcat
</code></pre>
<p>Check event of above deplyment.</p>
<pre><code>kubectl describe pod/tomcat
</code></pre>
<p>Get below event information.</p>
<pre><code>Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  55s   default-scheduler  Successfully assigned jh-namespace/tomcat to cka002
  Normal  Pulling    54s   kubelet            Pulling image &quot;tomcat&quot;
  Normal  Pulled     21s   kubelet            Successfully pulled image &quot;tomcat&quot; in 33.134162692s
  Normal  Created    19s   kubelet            Created container tomcat
  Normal  Started    19s   kubelet            Started container tomcat
</code></pre>
<p>Get event information for a Namespace.</p>
<pre><code>kubectl get events -n &lt;your_namespace_name&gt;
</code></pre>
<p>Get current default namespace event information.</p>
<pre><code>LAST SEEN   TYPE      REASON           OBJECT                          MESSAGE
70s         Warning   FailedGetScale   horizontalpodautoscaler/nginx   deployments/scale.apps &quot;podinfo&quot; not found
2m16s       Normal    Scheduled        pod/tomcat                      Successfully assigned jh-namespace/tomcat to cka002
2m15s       Normal    Pulling          pod/tomcat                      Pulling image &quot;tomcat&quot;
102s        Normal    Pulled           pod/tomcat                      Successfully pulled image &quot;tomcat&quot; in 33.134162692s
100s        Normal    Created          pod/tomcat                      Created container tomcat
100s        Normal    Started          pod/tomcat                      Started container tomcat
</code></pre>
<p>Get event information for all Namespace.</p>
<pre><code>kubectl get events -A
</code></pre>
<h3 id="logs">Logs</h3>
<p>Usage:</p>
<pre><code>kubectl logs &lt;pod_name&gt; -n &lt;namespace_name&gt;
</code></pre>
<p>Options:</p>
<ul>
<li><code>--tail &lt;n&gt;</code>: display only the most recent <code>&lt;n&gt;</code> lines of output</li>
<li><code>-f</code>: streaming the output</li>
</ul>
<p>Get the most recent 100 lines of output.</p>
<pre><code>kubectl logs -f tomcat --tail 100
</code></pre>
<p>If it's multipPod, use <code>-c</code> to specify Container.</p>
<pre><code>kubectl logs -f tomcat --tail 100 -c tomcat
</code></pre>
<h3 id="monitoring-indicators">Monitoring Indicators</h3>
<h4 id="nodes">Nodes</h4>
<p>Get node monitoring information</p>
<pre><code>kubectl top node
</code></pre>
<p>Output:</p>
<pre><code>NAME     CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
cka001   147m         7%     1940Mi          50%
cka002   62m          3%     2151Mi          56%
cka003   63m          3%     1825Mi          47%
</code></pre>
<p>Get Pod monitoring information</p>
<pre><code>kubectl top pod
</code></pre>
<p>Output:</p>
<pre><code>root@cka001:~# kubectl top pod
NAME                                      CPU(cores)   MEMORY(bytes)   
busybox-with-secret                       0m           0Mi
mysql                                     2m           366Mi
mysql-774db46945-sztrp                    2m           349Mi
mysql-nodeselector-6b7d9c875d-227t6       2m           365Mi
mysql-tolerations-5c5986944b-cg9bs        2m           366Mi
mysql-with-sc-pvc-7c97d875f8-dwfkc        2m           349Mi
nfs-client-provisioner-699db7fd58-bccqs   2m           7Mi
nginx                                     0m           3Mi
nginx-app-1-695b7b647d-l76bh              0m           3Mi
nginx-app-2-7f6bf6f4d4-lvbz8              0m           3Mi
nginx-nodename                            0m           3Mi
nginx-with-cm                             0m           3Mi
pod-configmap-env                         0m           3Mi
pod-configmap-env-2                       0m           3Mi
tomcat                                    1m           58Mi
</code></pre>
<p>Sort output by CPU or Memory using option <code>--sort-by</code>.</p>
<pre><code>kubectl top pod --sort-by=cpu
</code></pre>
<p>Output:</p>
<pre><code>NAME                                      CPU(cores)   MEMORY(bytes)   
nfs-client-provisioner-699db7fd58-bccqs   2m           7Mi
mysql                                     2m           366Mi
mysql-774db46945-sztrp                    2m           349Mi
mysql-nodeselector-6b7d9c875d-227t6       2m           365Mi
mysql-tolerations-5c5986944b-cg9bs        2m           366Mi
mysql-with-sc-pvc-7c97d875f8-dwfkc        2m           349Mi
tomcat                                    1m           58Mi
nginx                                     0m           3Mi
nginx-app-1-695b7b647d-l76bh              0m           3Mi
nginx-app-2-7f6bf6f4d4-lvbz8              0m           3Mi
nginx-nodename                            0m           3Mi
nginx-with-cm                             0m           3Mi
pod-configmap-env                         0m           3Mi
pod-configmap-env-2                       0m           3Mi
busybox-with-secret                       0m           0Mi
</code></pre>
<h3 id="node-eviction">Node Eviction</h3>
<p>Disable scheduling for a Node.</p>
<pre><code>kubectl cordon &lt;node_name&gt;
</code></pre>
<p>Example:</p>
<pre><code>kubectl cordon cka003
</code></pre>
<p>Node status:</p>
<pre><code>NAME     STATUS                     ROLES                  AGE   VERSION
cka001   Ready                      control-plane,master   18d   v1.23.8
cka002   Ready                      &lt;none&gt;                 18d   v1.23.8
cka003   Ready,SchedulingDisabled   &lt;none&gt;                 18d   v1.23.8
</code></pre>
<p>Enable scheduling for a Node.</p>
<pre><code>kubectl uncordon &lt;node_name&gt;
</code></pre>
<p>Example:</p>
<pre><code>kubectl uncordon cka003
</code></pre>
<p>Node status:</p>
<pre><code>NAME     STATUS   ROLES                  AGE   VERSION
cka001   Ready    control-plane,master   18d   v1.23.8
cka002   Ready    &lt;none&gt;                 18d   v1.23.8
cka003   Ready    &lt;none&gt;                 18d   v1.23.8
</code></pre>
<p>Evict Pods on a Node.</p>
<pre><code>kubectl drain &lt;node_name&gt;
kubectl drain &lt;node_name&gt; --ignore-daemonsets
kubectl drain &lt;node_name&gt; --ignore-daemonsets --delete-emptydir-data
</code></pre>
<h2 id="17rbac">17.RBAC</h2>
<p>Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within the organization.</p>
<p>When using client certificate authentication, we can generate certificates manually through <code>easyrsa</code>, <code>openssl</code> or <code>cfssl</code>.</p>
<p>Tasks in this section:</p>
<ol>
<li>Create differnet profiles for one cluster.</li>
<li>Use <code>cfssl</code> generate certificates for each profile.</li>
<li>Create new kubeconfig file with all profiles and associated users.</li>
<li>Merge old and new kubeconfig files into new kubeconfig file. We can switch different context for further demo.</li>
</ol>
<p>Best pracice: </p>
<p>The purpose of kubeconfig is to grant different authorizations to different users for different clusters. 
Different contexts will link to different clusters.</p>
<p>It's not recommended to put multiple users' contexts for one cluster in one kubeconfig. 
It's recommended to use one kubeconfig file for one user.</p>
<h3 id="install-cfssl">Install cfssl</h3>
<p>Install <code>cfssl</code> tool</p>
<pre><code>apt install golang-cfssl
</code></pre>
<h3 id="set-multiple-contexts">Set Multiple Contexts</h3>
<h4 id="current-context">Current Context</h4>
<p>Execute command <code>kubectl config</code> to get current contenxt.</p>
<pre><code>kubectl config get-contexts
</code></pre>
<p>We get below key information of the cluster.</p>
<ul>
<li>Cluster Name: kubernetes</li>
<li>System account: kubenetes-admin</li>
<li>Current context name: kubernetes-admin@kubernetes (format: <code>&lt;system_account&gt;@&lt;cluster_name&gt;</code>)</li>
</ul>
<pre><code>CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
*         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin 
</code></pre>
<h4 id="create-ca-config-file">Create CA Config File</h4>
<p>Get overview of directory <code>/etc/kubernetes/pki</code>.</p>
<pre><code>tree /etc/kubernetes/pki
</code></pre>
<pre><code>/etc/kubernetes/pki
├── apiserver.crt
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
├── apiserver.key
├── apiserver-kubelet-client.crt
├── apiserver-kubelet-client.key
├── ca.crt
├── ca.key
├── etcd
│   ├── ca.crt
│   ├── ca.key
│   ├── healthcheck-client.crt
│   ├── healthcheck-client.key
│   ├── peer.crt
│   ├── peer.key
│   ├── server.crt
│   └── server.key
├── front-proxy-ca.crt
├── front-proxy-ca.key
├── front-proxy-client.crt
├── front-proxy-client.key
├── sa.key
└── sa.pub
</code></pre>
<p>Change to directory <code>/etc/kubernetes/pki</code>.</p>
<pre><code>cd /etc/kubernetes/pki
</code></pre>
<p>Check if file <code>ca-config.json</code> is in place in current directory.</p>
<pre><code>ll ca-config.json
</code></pre>
<p>If not, create it.</p>
<ul>
<li>We can add multiple profiles to specify different expiry date, scenario, parameters, etc.. </li>
<li>Profile will be used to sign certificate.</li>
<li><code>87600</code> hours are about 10 years.</li>
</ul>
<p>Here we will create 1 additional profile <code>dev</code>.</p>
<pre><code>cat &gt; ca-config.json &lt;&lt;EOF
{
  &quot;signing&quot;: {
    &quot;default&quot;: {
      &quot;expiry&quot;: &quot;87600h&quot;
    },
    &quot;profiles&quot;: {
      &quot;dev&quot;: {
        &quot;usages&quot;: [
            &quot;signing&quot;,
            &quot;key encipherment&quot;,
            &quot;server auth&quot;,
            &quot;client auth&quot;
        ],
        &quot;expiry&quot;: &quot;87600h&quot;
      }
    }
  }
}
EOF
</code></pre>
<h4 id="create-csr-file-for-signature">Create CSR file for signature</h4>
<p>A CertificateSigningRequest (CSR) resource is used to request that a certificate be signed by a denoted signer, after which the request may be approved or denied before finally being signed.</p>
<p>It is important to set <code>CN</code> and <code>O</code> attribute of the CSR. </p>
<ul>
<li>The <code>CN</code> is the <em>name of the user</em> to request CSR.</li>
<li>The <code>O</code> is the <em>group</em> that this user will belong to. We can refer to RBAC for standard groups.</li>
</ul>
<p>Stay in the directory <code>/etc/kubernetes/pki</code>.</p>
<p>Create csr file <code>cka-dev-csr.json</code>. 
<code>CN</code> is <code>cka-dev</code>.
<code>O</code> is <code>k8s</code>.</p>
<pre><code>cat &gt; cka-dev-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;cka-dev&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;Shanghai&quot;,
      &quot;L&quot;: &quot;Shanghai&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;System&quot;
    }
  ]
}
EOF
</code></pre>
<p>Generate certifcate and key for the profile we defined above.
<code>cfssljson -bare cka-dev</code> will generate two files, <code>cka-dev.pem</code> as public key and <code>cka-dev-key.pem</code> as private key.</p>
<pre><code>cfssl gencert -ca=ca.crt -ca-key=ca.key -config=ca-config.json -profile=dev cka-dev-csr.json | cfssljson -bare cka-dev
</code></pre>
<p>Get below files.</p>
<pre><code>ll -tr | grep cka-dev
</code></pre>
<pre><code>-rw-r--r-- 1 root root  222 Jul 18 20:36 cka-dev-csr.json
-rw-r--r-- 1 root root 1281 Jul 18 20:49 cka-dev.pem
-rw------- 1 root root 1679 Jul 18 20:49 cka-dev-key.pem
-rw-r--r-- 1 root root 1001 Jul 18 20:49 cka-dev.csr
</code></pre>
<h4 id="create-file-kubeconfig">Create file kubeconfig</h4>
<p>Get the IP of Control Plane (e.g., <code>172.16.18.161</code> here) to composite evn variable <code>KUBE_APISERVER</code> (<code>https://&lt;control_plane_ip&gt;:&lt;port&gt;</code>).</p>
<pre><code>kubectl get node -owide
</code></pre>
<pre><code>NAME     STATUS   ROLES                  AGE   VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
cka001   Ready    control-plane,master   18d   v1.23.8   172.16.18.161   &lt;none&gt;        Ubuntu 20.04.4 LTS   5.4.0-113-generic   containerd://1.5.9
cka002   Ready    &lt;none&gt;                 18d   v1.23.8   172.16.18.160   &lt;none&gt;        Ubuntu 20.04.4 LTS   5.4.0-113-generic   containerd://1.5.9
cka003   Ready    &lt;none&gt;                 18d   v1.23.8   172.16.18.159   &lt;none&gt;        Ubuntu 20.04.4 LTS   5.4.0-113-generic   containerd://1.5.9
</code></pre>
<p>Export env <code>KUBE_APISERVER</code>.</p>
<pre><code>echo &quot;export KUBE_APISERVER=\&quot;https://172.16.18.161:6443\&quot;&quot; &gt;&gt; ~/.bashrc
source ~/.bashrc
</code></pre>
<p>Verify the setting.</p>
<pre><code>echo $KUBE_APISERVER
</code></pre>
<p>Output:</p>
<pre><code>https://172.16.18.161:6443
</code></pre>
<h5 id="set-up-cluster">Set up cluster</h5>
<p>Stay in the directory <code>/etc/kubernetes/pki</code>.</p>
<p>Generate kubeconfig file.</p>
<pre><code>kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/pki/ca.crt \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=cka-dev.kubeconfig
</code></pre>
<p>Now we get the new config file <code>cka-dev.kubeconfig</code></p>
<pre><code>ll -tr | grep cka-dev
</code></pre>
<p>Output:</p>
<pre><code>-rw-r--r-- 1 root root  222 Jul 18 20:36 cka-dev-csr.json
-rw-r--r-- 1 root root 1281 Jul 18 20:49 cka-dev.pem
-rw------- 1 root root 1679 Jul 18 20:49 cka-dev-key.pem
-rw-r--r-- 1 root root 1001 Jul 18 20:49 cka-dev.csr
-rw------- 1 root root 1671 Jul 18 20:50 cka-dev.kubeconfig
</code></pre>
<p>Get content of file <code>cka-dev.kubeconfig</code>.</p>
<pre><code>cat cka-dev.kubeconfig
</code></pre>
<pre><code>apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: &lt;your_key&gt;
    server: https://172.16.18.161:6443
  name: kubernetes
contexts: null
current-context: &quot;&quot;
kind: Config
preferences: {}
users: null
</code></pre>
<h5 id="set-up-user">Set up user</h5>
<p>In file <code>cka-dev.kubeconfig</code>, user info is null. </p>
<p>Set up user <code>cka-dev</code>.</p>
<pre><code>kubectl config set-credentials cka-dev \
  --client-certificate=/etc/kubernetes/pki/cka-dev.pem \
  --client-key=/etc/kubernetes/pki/cka-dev-key.pem \
  --embed-certs=true \
  --kubeconfig=cka-dev.kubeconfig
</code></pre>
<p>Now file <code>cka-dev.kubeconfig</code> was updated and user information was added.</p>
<pre><code>cat cka-dev.kubeconfig
</code></pre>
<pre><code>apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: &lt;your_key&gt;
    server: https://172.16.18.161:6443
  name: kubernetes
contexts: null
current-context: &quot;&quot;
kind: Config
preferences: {}
users:
- name: cka-dev
  user:
    client-certificate-data: &lt;your_key&gt;
    client-key-data: &lt;your_key&gt;
</code></pre>
<p>Now we have a complete kubeconfig file <code>cka-dev.kubeconfig</code>.
When we use it to get node information, receive error below because we did not set up current-context in kubeconfig file.</p>
<pre><code>kubectl --kubeconfig=cka-dev.kubeconfig get nodes
</code></pre>
<pre><code>The connection to the server localhost:8080 was refused - did you specify the right host or port?
</code></pre>
<p>Current contents is empty.</p>
<pre><code>kubectl --kubeconfig=cka-dev.kubeconfig config get-contexts
</code></pre>
<pre><code>CURRENT   NAME   CLUSTER   AUTHINFO   NAMESPACE
</code></pre>
<h5 id="set-up-context">Set up Context</h5>
<p>Set up context. </p>
<pre><code>kubectl config set-context dev --cluster=kubernetes --user=cka-dev  --kubeconfig=cka-dev.kubeconfig
</code></pre>
<p>Now we have context now but the <code>CURRENT</code> flag is empty.</p>
<pre><code>kubectl --kubeconfig=cka-dev.kubeconfig config get-contexts
</code></pre>
<p>Output:</p>
<pre><code>CURRENT   NAME   CLUSTER      AUTHINFO   NAMESPACE
          dev    kubernetes   cka-dev 
</code></pre>
<p>Set up default context. The context will link clusters and users for multiple clusters environment and we can switch to different cluster.</p>
<pre><code>kubectl --kubeconfig=cka-dev.kubeconfig config use-context dev
</code></pre>
<h5 id="verify_1">Verify</h5>
<p>Now <code>CURRENT</code> is marked with <code>*</code>, that is, current-context is set up.</p>
<pre><code>kubectl --kubeconfig=cka-dev.kubeconfig config get-contexts
</code></pre>
<pre><code>CURRENT   NAME   CLUSTER      AUTHINFO   NAMESPACE
*         dev    kubernetes   cka-dev      
</code></pre>
<p>Because user <code>cka-dev</code> does not have authorization in the cluster, we will receive <code>forbidden</code> error when we try to get information of Pods or Nodes.</p>
<pre><code>kubectl --kubeconfig=/etc/kubernetes/pki/cka-dev.kubeconfig get pod 
kubectl --kubeconfig=/etc/kubernetes/pki/cka-dev.kubeconfig get node
</code></pre>
<h4 id="merge-kubeconfig-files">Merge kubeconfig files</h4>
<p>Make a copy of your existing config</p>
<pre><code>cp ~/.kube/config ~/.kube/config.old 
</code></pre>
<p>Merge the two config files together into a new config file <code>/tmp/config</code>.</p>
<pre><code>KUBECONFIG=~/.kube/config:/etc/kubernetes/pki/cka-dev.kubeconfig  kubectl config view --flatten &gt; /tmp/config
</code></pre>
<p>Replace the old config with the new merged config</p>
<pre><code>mv /tmp/config ~/.kube/config
</code></pre>
<p>Now the new <code>~/.kube/config</code> looks like below.</p>
<pre><code>apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: &lt;your_key&gt;
    server: https://172.16.18.161:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: cka-dev
  name: cka-dev@kubernetes
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: cka-dev
  user:
    client-certificate-data: &lt;your_key&gt;
    client-key-data: &lt;your_key&gt;
- name: kubernetes-admin
  user:
    client-certificate-data: &lt;your_key&gt;
    client-key-data: &lt;your_key&gt;
</code></pre>
<p>Verify contexts after kubeconfig merged.</p>
<pre><code>kubectl config get-contexts
</code></pre>
<p>Current context is the system default <code>kubernetes-admin@kubernetes</code>.</p>
<pre><code>CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
          cka-dev@kubernetes            kubernetes   cka-dev            
*         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   
</code></pre>
<h3 id="namespaces-contexts">Namespaces &amp; Contexts</h3>
<p>Get list of Namespace with Label information.</p>
<pre><code>kubectl get ns --show-labels
</code></pre>
<p>Create Namespace <code>dev</code>.</p>
<pre><code>kubectl create namespace dev
</code></pre>
<p>Use below command to set a context with new update, e.g, update default namespace, etc..</p>
<pre><code>kubectl config set-context &lt;context name&gt; --cluster=&lt;cluster name&gt; --namespace=&lt;namespace name&gt; --user=&lt;user name&gt; 
</code></pre>
<p>Let's set default namespace to each context.</p>
<pre><code>kubectl config set-context kubernetes-admin@kubernetes --cluster=kubernetes --namespace=default --user=kubernetes-admin
kubectl config set-context dev@kubernetes --cluster=kubernetes --namespace=dev --user=cka-dev
</code></pre>
<p>Let's check current context information.</p>
<pre><code>kubectl config get-contexts
</code></pre>
<p>Output:</p>
<pre><code>CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
          cka-dev@kubernetes            kubernetes   cka-dev            dev
*         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   default
</code></pre>
<p>To switch to a new context, use below command.</p>
<pre><code>kubectl config use-contexts &lt;context name&gt;
</code></pre>
<p>For example.</p>
<pre><code>kubectl config use-context dev
</code></pre>
<p>Verify if it's changed as expected.</p>
<pre><code>kubectl config get-contexts
</code></pre>
<pre><code>CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
*         cka-dev@kubernetes            kubernetes   cka-dev            dev
          kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   default
</code></pre>
<p>Be noted, four users beginning with <code>cka-dev</code> created don't have any authorizations, e.g., access namespaces, get pods, etc..
Referring RBAC to grant their authorizations. </p>
<h3 id="role-rolebinding">Role &amp; RoleBinding</h3>
<p>Switch to context <code>kubernetes-admin@kubernetes</code>.</p>
<pre><code>kubectl config use-context kubernetes-admin@kubernetes
</code></pre>
<p>Use <code>kubectl create role</code> command  with option <code>--dry-run=client</code> and <code>-o yaml</code> to generate yaml template for customizing. </p>
<pre><code>kubectl create role admin-dev --resource=pods --verb=get --verb=list --verb=watch --dry-run=client -o yaml
</code></pre>
<p>Create role with yaml file.</p>
<pre><code>cat &gt; role-cka-dev.yaml &lt;&lt; EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: dev
  name: admin-dev
rules:
- apiGroups:
  - &quot;&quot;
  resources:
  - pods
  verbs:
  - get
  - watch
  - list
EOF

kubectl apply -f role-cka-dev.yaml
</code></pre>
<p>Use <code>kubectl create rolebinding</code> command  with option <code>--dry-run=client</code> and <code>-o yaml</code> to generate yaml template for customizing.</p>
<pre><code>kubectl create rolebinding admin --role=admin-dev --user=cka-dev --dry-run=client -o yaml
</code></pre>
<p>Create rolebinding with yaml file.</p>
<pre><code>cat &gt; role-binding-cka-dev.yaml &lt;&lt; EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: admin
  namespace: dev
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: admin-dev
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: cka-dev
EOF

kubectl apply -f role-binding-cka-dev.yaml
</code></pre>
<p>Verify authorzation of user <code>cka-dev</code> on Namespace <code>dev</code>.</p>
<p>Switch to context <code>cka-dev@kubernetes</code>.</p>
<pre><code>kubectl config use-context cka-dev@kubernetes
</code></pre>
<p>Get Pods status in Namespace <code>dev</code>. Success!</p>
<pre><code>kubectl get pod -n dev
</code></pre>
<p>Get Pods status in Namespace <code>kube-system</code>. Failed, because the authorzation is only for Namespace <code>dev</code>.</p>
<pre><code>kubectl get pod -n kube-system
</code></pre>
<p>Get Nodes status. Failed, because the role we defined is only for Pod resource.</p>
<pre><code>kubectl get node
</code></pre>
<p>Create a Pod in Namespace <code>dev</code>. Failed because we only have <code>get</code>,<code>watch</code>,<code>list</code> for Pod, no <code>create</code> authorization.</p>
<pre><code>kubectl run nginx --image=nginx -n dev
</code></pre>
<h3 id="clusterrole-clusterrolebinding">ClusterRole &amp; ClusterRoleBinding</h3>
<p>Switch to context <code>kubernetes-admin@kubernetes</code>.</p>
<pre><code>kubectl config use-context kubernetes-admin@kubernetes
</code></pre>
<p>Create a ClusterRole with authorization <code>get</code>,<code>watch</code>,<code>list</code> for <code>nodes</code> resource.</p>
<pre><code>cat &gt; clusterrole-cka-dev.yaml &lt;&lt;EOF
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nodes-admin
rules:
- apiGroups:
  - &quot;&quot;
  resources: 
  - nodes
  verbs:
  - get
  - watch
  - list
EOF


kubectl apply -f clusterrole-cka-dev.yaml
</code></pre>
<p>Bind ClusterRole <code>nodes-admin</code> to user <code>cka-dev</code>.</p>
<pre><code>cat &gt; clusterrolebinding-cka-dev.yaml &lt;&lt; EOF
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: admin
subjects:
- kind: User
  name: cka-dev
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: nodes-admin
  apiGroup: rbac.authorization.k8s.io
EOF


kubectl apply -f clusterrolebinding-cka-dev.yaml
</code></pre>
<p>Verify Authorization</p>
<p>Switch to context <code>cka-dev@kubernetes</code>.</p>
<pre><code>kubectl config use-context cka-dev@kubernetes
</code></pre>
<p>Get node information. Success!</p>
<pre><code>kubectl get node
</code></pre>
<h2 id="18network-policy">18.Network Policy</h2>
<h3 id="replace-flannel-by-calico">Replace Flannel by Calico</h3>
<p>Delete Flannel</p>
<pre><code>kubectl delete -f https://raw.githubusercontent.com/coreos/flannel/v0.18.1/Documentation/kube-flannel.yml
</code></pre>
<p>or</p>
<pre><code>kubectl delete -f kube-flannel.yml
</code></pre>
<p>Output:</p>
<pre><code>Warning: policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
podsecuritypolicy.policy &quot;psp.flannel.unprivileged&quot; deleted
clusterrole.rbac.authorization.k8s.io &quot;flannel&quot; deleted
clusterrolebinding.rbac.authorization.k8s.io &quot;flannel&quot; deleted
serviceaccount &quot;flannel&quot; deleted
configmap &quot;kube-flannel-cfg&quot; deleted
daemonset.apps &quot;kube-flannel-ds&quot; deleted
</code></pre>
<p>Clean up iptables for all nodes.</p>
<pre><code>rm -rf /var/run/flannel /opt/cni /etc/cni /var/lib/cni
iptables -F &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t mangle -F &amp;&amp; iptables -X
</code></pre>
<p>Log out and log on to host (e.g., cka001) again. Install Calico.</p>
<pre><code>curl https://docs.projectcalico.org/manifests/calico.yaml -O

kubectl apply -f calico.yaml
</code></pre>
<p>Output:</p>
<pre><code>configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
</code></pre>
<p>Verify status of Calico. Make sure all Pods are running</p>
<pre><code>kubectl get pod -n kube-system | grep calico
</code></pre>
<p>Output:</p>
<pre><code>NAME                                       READY   STATUS        RESTARTS   AGE
calico-kube-controllers-7bc6547ffb-tjfcg   1/1     Running       0          30m
calico-node-7x8jm                          1/1     Running       0          30m
calico-node-cwxj5                          1/1     Running       0          30m
calico-node-rq978                          1/1     Running       0          30m
</code></pre>
<p>If facing any error, check log in the Container.</p>
<pre><code># Get Container ID
crictl ps

# Get log info
crictl logs &lt;your_container_id&gt;
</code></pre>
<p>As we change CNI from Flannel to Calico, we need delete all Pods. All of Pods will be created automatically again. </p>
<pre><code>kubectl delete pod -A --all
</code></pre>
<p>Make sure all Pods are up and running successfully.</p>
<pre><code>kubectl get pod -A
</code></pre>
<h3 id="inbound-rules">Inbound Rules</h3>
<h4 id="create-workload-for-test">Create workload for test.</h4>
<p>Create three Deployments <code>pod-netpol-1</code>,<code>pod-netpol-2</code>,<code>pod-netpol-3</code>.</p>
<pre><code>cat &gt; pod-netpol.yaml &lt;&lt; EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: pod-netpol-1
  name: pod-netpol-1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pod-netpol-1
  template:
    metadata:
      labels:
        app: pod-netpol-1
    spec:
      containers:
      - image: busybox
        name: busybox
        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;sleep 1h&quot;]

---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: pod-netpol-2
  name: pod-netpol-2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pod-netpol-2
  template:
    metadata:
      labels:
        app: pod-netpol-2
    spec:
      containers:
      - image: busybox
        name: busybox
        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;sleep 1h&quot;]

---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: pod-netpol-3
  name: pod-netpol-3
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pod-netpol-3
  template:
    metadata:
      labels:
        app: pod-netpol-3
    spec:
      containers:
      - image: busybox
        name: busybox
        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;sleep 1h&quot;]       
EOF

kubectl apply -f pod-netpol.yaml
</code></pre>
<p>Check Pods IP.</p>
<pre><code>kubectl get pod -owide
</code></pre>
<p>Output:</p>
<pre><code>NAME                                      READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
pod-netpol-1-6494f6bf8b-6nwwf             1/1     Running   0          19s   10.244.102.9   cka003   &lt;none&gt;           &lt;none&gt;
pod-netpol-2-77478d77ff-96hgd             1/1     Running   0          19s   10.244.112.9   cka002   &lt;none&gt;           &lt;none&gt;
pod-netpol-3-68977dcb48-j9fkb             1/1     Running   0          19s   10.244.102.8   cka003   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>Attach to Pod <code>pod-netpol-1</code></p>
<pre><code>kubectl exec -it pod-netpol-1-6494f6bf8b-6nwwf -- sh
</code></pre>
<p>Execute command <code>ping</code> to check if pod-netpol-2 and pod-netpol-3 are pingable. </p>
<pre><code>/ # ping 10.244.112.9
3 packets transmitted, 3 packets received, 0% packet loss

/ # ping 10.244.102.8
3 packets transmitted, 3 packets received, 0% packet loss
</code></pre>
<h4 id="deny-for-all-ingress">Deny For All Ingress</h4>
<p>Create deny policy for all ingress.</p>
<pre><code>cat &gt; networkpolicy-default-deny-ingress.yaml &lt;&lt; EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Ingress
EOF


kubectl apply -f networkpolicy-default-deny-ingress.yaml
</code></pre>
<p>Attach to Pod <code>pod-netpol-1</code> again</p>
<pre><code>kubectl exec -it pod-netpol-1-6494f6bf8b-6nwwf -- sh
</code></pre>
<p>Execute command <code>ping</code> to check if pod-netpol-2 and pod-netpol-3 are pingable. Both ping are denied as expected.</p>
<pre><code>/ # ping 10.244.112.9
3 packets transmitted, 0 packets received, 100% packet loss

/ # ping 10.244.102.8
3 packets transmitted, 0 packets received, 100% packet loss
</code></pre>
<h4 id="allow-for-specific-ingress">Allow For Specific Ingress</h4>
<p>Create NetworkPlicy to allow ingress from pod-netpol-1 to pod-netpol-2.</p>
<pre><code>cat &gt; allow-pod-netpol-1-to-pod-netpol-2.yaml &lt;&lt;EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-pod-netpol-1-to-pod-netpol-2
spec:
  podSelector:
    matchLabels:
      app: pod-netpol-2
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: pod-netpol-1
EOF

kubectl apply -f allow-pod-netpol-1-to-pod-netpol-2.yaml
</code></pre>
<h4 id="verify-networkpolicy">Verify NetworkPolicy</h4>
<p>Attach to Pod <code>pod-netpol-1</code> again.</p>
<pre><code>kubectl exec -it pod-netpol-1-6494f6bf8b-6nwwf -- sh
</code></pre>
<p>Execute command <code>ping</code> to check if pod-netpol-2 and pod-netpol-3 are pingable. 
As expected, pod-netpol-2 is reachable and pod-netpol-3 is still unreachable. </p>
<pre><code>/ # ping 10.244.112.9
3 packets transmitted, 3 packets received, 0% packet loss

/ # ping 10.244.102.8
3 packets transmitted, 0 packets received, 100% packet loss
</code></pre>
<h3 id="inbound-across-namespace">Inbound Across Namespace</h3>
<h4 id="create-workload-and-namespace-for-test">Create workload and namespace for test</h4>
<p>Create Namespace <code>ns-netpol</code>.
Create Deployment <code>pod-netpol</code>.</p>
<pre><code>kubectl create ns ns-netpol

cat &gt; pod-netpol.yaml &lt;&lt; EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: pod-netpol
  name: pod-netpol
  namespace: ns-netpol
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pod-netpol
  template:
    metadata:
      labels:
        app: pod-netpol
    spec:
      containers:
      - image: busybox
        name: busybox
        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;sleep 1h&quot;]
EOF


kubectl apply -f pod-netpol.yaml
</code></pre>
<p>Check Pod status on new namespace.</p>
<pre><code>kubectl get pod -n ns-netpol
</code></pre>
<p>Output:</p>
<pre><code>NAME                          READY   STATUS    RESTARTS   AGE
pod-netpol-5b67b6b496-zxppp   1/1     Running   0          10s
</code></pre>
<p>Attach into <code>pod-netpol</code> Pod.</p>
<pre><code>kubectl exec -it pod-netpol-5b67b6b496-zxppp -n ns-netpol -- sh
</code></pre>
<p>Try to ping pod-netpol-2 (<code>10.244.112.9</code>) in Namespace <code>jh-namespace</code>. It's unreachable. </p>
<pre><code>ping 10.244.112.9
3 packets transmitted, 0 packets received, 100% packet loss
</code></pre>
<h4 id="create-allow-ingress">Create Allow Ingress</h4>
<p>Create NetworkPolicy to allow access to pod-netpol-2 in namespace <code>jh-namespace</code> from all Pods in namespace <code>pod-netpol</code>.</p>
<pre><code>cat &gt; allow-ns-netpol-to-pod-netpol-2.yaml &lt;&lt;EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ns-netpol-to-pod-netpol-2
spec:
  podSelector:
    matchLabels:
      app: pod-netpol-2
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          allow: to-pod-netpol-2
EOF

kubectl apply -f allow-ns-netpol-to-pod-netpol-2.yaml
</code></pre>
<h4 id="verify-policy">Verify Policy</h4>
<p>Attach into <code>pod-netpol</code> Pod.</p>
<pre><code>kubectl exec -it pod-netpol-5b67b6b496-zxppp -n ns-netpol -- sh
</code></pre>
<p>Try to ping pod-netpol-2 (<code>10.244.112.9</code>) in Namespace <code>jh-namespace</code>. It's still unreachable. </p>
<pre><code>ping 10.244.112.9
3 packets transmitted, 0 packets received, 100% packet loss
</code></pre>
<p>What we allowed ingress is from namespace with label <code>allow: to-pod-netpol-2</code>, but namespace <code>ns-netpol</code> does not have it and we need label it.</p>
<pre><code>kubectl label ns ns-netpol allow=to-pod-netpol-2
</code></pre>
<p>Attach into <code>pod-netpol</code> Pod.</p>
<pre><code>kubectl exec -it pod-netpol-5b67b6b496-zxppp -n ns-netpol -- sh
</code></pre>
<p>Try to ping pod-netpol-2 (<code>10.244.112.9</code>) in Namespace <code>jh-namespace</code>. It's now reachable. </p>
<pre><code>ping 10.244.112.9
3 packets transmitted, 3 packets received, 0% packet loss
</code></pre>
<p>Be noted that we can use namespace default label as well.</p>
<h2 id="19cluster-management">19.Cluster Management</h2>
<h3 id="etcd-backup-and-restore"><code>etcd</code> Backup and Restore</h3>
<h4 id="install-etcdctl">Install <code>etcdctl</code></h4>
<p>Download <code>etcd</code> package from Github.</p>
<pre><code>wget https://github.com/etcd-io/etcd/releases/download/v3.5.3/etcd-v3.5.3-linux-amd64.tar.gz
</code></pre>
<p>Unzip and grant execute permission.</p>
<pre><code>tar -zxvf etcd-v3.5.3-linux-amd64.tar.gz
cp etcd-v3.5.3-linux-amd64/etcdctl /usr/local/bin/
sudo chmod u+x /usr/local/bin/etcdctl
</code></pre>
<p>Verify</p>
<pre><code>etcdctl --help
</code></pre>
<h4 id="create-deployment-before-backup">Create Deployment Before Backup</h4>
<p>Create Deployment before backup.</p>
<pre><code>kubectl create deployment app-before-backup --image=nginx
</code></pre>
<h4 id="backup-etcd">Backup <code>etcd</code></h4>
<p>Command usage: </p>
<ul>
<li><code>&lt;CONTROL_PLANE_IP_ADDRESS&gt;</code> is the actual IP address of Control Plane.</li>
<li><code>--endpoints</code>: specify where to save backup of etcd, 2379 is etcd port.</li>
<li><code>--cert</code>: sepcify etcd certificate, which was generated by <code>kubeadm</code> and saved in <code>/etc/kubernetes/pki/etcd/</code>.</li>
<li><code>--key</code>: specify etcd certificate key, which was generated by <code>kubeadm</code> and saved in <code>/etc/kubernetes/pki/etcd/</code>.</li>
<li><code>--cacert</code>: specify etcd certificate CA, which was generated by <code>kubeadm</code> and saved in <code>/etc/kubernetes/pki/etcd/</code>.</li>
</ul>
<pre><code>etcdctl --endpoints=https://&lt;CONTROL_PLANE_IP_ADDRESS&gt;:2379 --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt snapshot save snapshot-$(date +&quot;%Y%m%d%H%M%S&quot;).db
</code></pre>
<pre><code>etcdctl --endpoints=https://172.16.18.161:2379 --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt snapshot save snapshot-$(date +&quot;%Y%m%d%H%M%S&quot;).db
</code></pre>
<p>Output:</p>
<pre><code>{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:&quot;2022-07-14T14:37:39.232+0800&quot;,&quot;caller&quot;:&quot;snapshot/v3_snapshot.go:65&quot;,&quot;msg&quot;:&quot;created temporary db file&quot;,&quot;path&quot;:&quot;snapshot-20220714143739.db.part&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:&quot;2022-07-14T14:37:39.239+0800&quot;,&quot;logger&quot;:&quot;client&quot;,&quot;caller&quot;:&quot;v3/maintenance.go:211&quot;,&quot;msg&quot;:&quot;opened snapshot stream; downloading&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:&quot;2022-07-14T14:37:39.239+0800&quot;,&quot;caller&quot;:&quot;snapshot/v3_snapshot.go:73&quot;,&quot;msg&quot;:&quot;fetching snapshot&quot;,&quot;endpoint&quot;:&quot;https://172.16.18.161:2379&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:&quot;2022-07-14T14:37:39.332+0800&quot;,&quot;logger&quot;:&quot;client&quot;,&quot;caller&quot;:&quot;v3/maintenance.go:219&quot;,&quot;msg&quot;:&quot;completed snapshot read; closing&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:&quot;2022-07-14T14:37:39.359+0800&quot;,&quot;caller&quot;:&quot;snapshot/v3_snapshot.go:88&quot;,&quot;msg&quot;:&quot;fetched snapshot&quot;,&quot;endpoint&quot;:&quot;https://172.16.18.161:2379&quot;,&quot;size&quot;:&quot;5.6 MB&quot;,&quot;took&quot;:&quot;now&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:&quot;2022-07-14T14:37:39.359+0800&quot;,&quot;caller&quot;:&quot;snapshot/v3_snapshot.go:97&quot;,&quot;msg&quot;:&quot;saved&quot;,&quot;path&quot;:&quot;snapshot-20220714143739.db&quot;}
</code></pre>
<p>We can get the backup file in current directory with <code>ls -al</code> command.</p>
<pre><code>-rw------- 1 root root 5632032 Jul 14 14:37 snapshot-20220714143739.db
</code></pre>
<h4 id="create-deployment-after-backup">Create Deployment After Backup</h4>
<p>Create Deployment after backup.</p>
<pre><code>kubectl create deployment app-after-backup --image=nginx
</code></pre>
<p>Delete Deployment we created before backup.</p>
<pre><code>kubectl delete deployment app-before-backup
</code></pre>
<p>Check Deployment status</p>
<pre><code>kubectl get deploy
</code></pre>
<pre><code>NAME                     READY   UP-TO-DATE   AVAILABLE   AGE
app-after-backup         1/1     1            1           108s
</code></pre>
<h4 id="restore-etcd">Restore <code>etcd</code></h4>
<h5 id="stop-services">Stop Services</h5>
<p>Delete <code>etcd</code> directory.</p>
<pre><code>mv /var/lib/etcd/ /var/lib/etcd.bak
</code></pre>
<p>Stop <code>kubelet</code></p>
<pre><code>systemctl stop kubelet
</code></pre>
<p>Stop kube-apiserver</p>
<pre><code>nerdctl -n k8s.io ps -a | grep apiserver
</code></pre>
<pre><code>1eb9a51e0406    registry.aliyuncs.com/google_containers/kube-apiserver:v1.23.8             &quot;kube-apiserver --ad…&quot;    2 weeks ago     Created             k8s://kube-system/kube-apiserver-cka001/kube-apiserver
2c5e1d183fc7    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  2 weeks ago     Created             k8s://kube-system/kube-apiserver-cka001
73d0fdef9c16    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  16 hours ago    Up                  k8s://kube-system/kube-apiserver-cka001
c7e67d4cf78c    registry.aliyuncs.com/google_containers/kube-apiserver:v1.23.8             &quot;kube-apiserver --ad…&quot;    16 hours ago    Up                  k8s://kube-system/kube-apiserver-cka001/kube-apiserver
</code></pre>
<p>Stop those <code>up</code> status containers.</p>
<pre><code>nerdctl -n k8s.io stop &lt;container_id&gt;

nerdctl -n k8s.io stop 73d0fdef9c16
nerdctl -n k8s.io stop c7e67d4cf78c
</code></pre>
<p>No <code>up</code> status <code>kube-apiserver</code> now.</p>
<pre><code>nerdctl -n k8s.io ps -a | grep apiserver
</code></pre>
<pre><code>1eb9a51e0406    registry.aliyuncs.com/google_containers/kube-apiserver:v1.23.8             &quot;kube-apiserver --ad…&quot;    2 weeks ago     Created             k8s://kube-system/kube-apiserver-cka001/kube-apiserver
2c5e1d183fc7    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  2 weeks ago     Created             k8s://kube-system/kube-apiserver-cka001
73d0fdef9c16    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  16 hours ago    Created             k8s://kube-system/kube-apiserver-cka001
c7e67d4cf78c    registry.aliyuncs.com/google_containers/kube-apiserver:v1.23.8             &quot;kube-apiserver --ad…&quot;    16 hours ago    Created             k8s://kube-system/kube-apiserver-cka001/kube-apiserver
</code></pre>
<p>Stop etcd</p>
<pre><code>nerdctl -n k8s.io ps -a | grep etcd
</code></pre>
<pre><code>5812c42bf572    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  2 weeks ago     Created             k8s://kube-system/etcd-cka001
7f4da4416356    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  16 hours ago    Up                  k8s://kube-system/etcd-cka001
897a3e83a512    registry.aliyuncs.com/google_containers/etcd:3.5.1-0                       &quot;etcd --advertise-cl…&quot;    16 hours ago    Up                  k8s://kube-system/etcd-cka001/etcd
ff6626664c43    registry.aliyuncs.com/google_containers/etcd:3.5.1-0                       &quot;etcd --advertise-cl…&quot;    2 weeks ago     Created             k8s://kube-system/etcd-cka001/etcd
</code></pre>
<p>Stop those <code>up</code> status containers.</p>
<pre><code>nerdctl -n k8s.io stop &lt;container_id&gt;
</code></pre>
<pre><code>nerdctl -n k8s.io stop 7f4da4416356
nerdctl -n k8s.io stop 897a3e83a512
</code></pre>
<p>No <code>up</code> status <code>etcd</code> now.</p>
<pre><code>nerdctl -n k8s.io ps -a | grep etcd
</code></pre>
<pre><code>5812c42bf572    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  2 weeks ago     Created             k8s://kube-system/etcd-cka001
7f4da4416356    registry.aliyuncs.com/google_containers/pause:3.6                          &quot;/pause&quot;                  16 hours ago    Created             k8s://kube-system/etcd-cka001
897a3e83a512    registry.aliyuncs.com/google_containers/etcd:3.5.1-0                       &quot;etcd --advertise-cl…&quot;    16 hours ago    Created             k8s://kube-system/etcd-cka001/etcd
ff6626664c43    registry.aliyuncs.com/google_containers/etcd:3.5.1-0                       &quot;etcd --advertise-cl…&quot;    2 weeks ago     Created             k8s://kube-system/etcd-cka001/etcd
</code></pre>
<h5 id="restore-etcd_1">Restore <code>etcd</code></h5>
<p>Execute the restore operation on Control Plane node with actual backup file.</p>
<pre><code>etcdctl snapshot restore snapshot-20220714143739.db \
    --endpoints=172.16.18.161:2379 \
    --cert=/etc/kubernetes/pki/etcd/server.crt \
    --key=/etc/kubernetes/pki/etcd/server.key \
    --cacert=/etc/kubernetes/pki/etcd/ca.crt\
    --data-dir=/var/lib/etcd
</code></pre>
<p>Output:</p>
<pre><code>Deprecated: Use `etcdutl snapshot restore` instead.

2022-07-14T15:19:53+08:00       info    snapshot/v3_snapshot.go:248     restoring snapshot      {&quot;path&quot;: &quot;snapshot-20220714143739.db&quot;, &quot;wal-dir&quot;: &quot;/var/lib/etcd/member/wal&quot;, &quot;data-dir&quot;: &quot;/var/lib/etcd&quot;, &quot;snap-dir&quot;: &quot;/var/lib/etcd/member/snap&quot;, &quot;stack&quot;: &quot;go.etcd.io/etcd/etcdutl/v3/snapshot.(*v3Manager).Restore\n\t/go/src/go.etcd.io/etcd/release/etcd/etcdutl/snapshot/v3_snapshot.go:254\ngo.etcd.io/etcd/etcdutl/v3/etcdutl.SnapshotRestoreCommandFunc\n\t/go/src/go.etcd.io/etcd/release/etcd/etcdutl/etcdutl/snapshot_command.go:147\ngo.etcd.io/etcd/etcdctl/v3/ctlv3/command.snapshotRestoreCommandFunc\n\t/go/src/go.etcd.io/etcd/release/etcd/etcdctl/ctlv3/command/snapshot_command.go:129\ngithub.com/spf13/cobra.(*Command).execute\n\t/go/pkg/mod/github.com/spf13/cobra@v1.1.3/command.go:856\ngithub.com/spf13/cobra.(*Command).ExecuteC\n\t/go/pkg/mod/github.com/spf13/cobra@v1.1.3/command.go:960\ngithub.com/spf13/cobra.(*Command).Execute\n\t/go/pkg/mod/github.com/spf13/cobra@v1.1.3/command.go:897\ngo.etcd.io/etcd/etcdctl/v3/ctlv3.Start\n\t/go/src/go.etcd.io/etcd/release/etcd/etcdctl/ctlv3/ctl.go:107\ngo.etcd.io/etcd/etcdctl/v3/ctlv3.MustStart\n\t/go/src/go.etcd.io/etcd/release/etcd/etcdctl/ctlv3/ctl.go:111\nmain.main\n\t/go/src/go.etcd.io/etcd/release/etcd/etcdctl/main.go:59\nruntime.main\n\t/go/gos/go1.16.15/src/runtime/proc.go:225&quot;}
2022-07-14T15:19:53+08:00       info    membership/store.go:141 Trimming membership information from the backend...
2022-07-14T15:19:53+08:00       info    membership/cluster.go:421       added member    {&quot;cluster-id&quot;: &quot;cdf818194e3a8c32&quot;, &quot;local-member-id&quot;: &quot;0&quot;, &quot;added-peer-id&quot;: &quot;8e9e05c52164694d&quot;, &quot;added-peer-peer-urls&quot;: [&quot;http://localhost:2380&quot;]}
2022-07-14T15:19:53+08:00       info    snapshot/v3_snapshot.go:269     restored snapshot       {&quot;path&quot;: &quot;snapshot-20220714143739.db&quot;, &quot;wal-dir&quot;: &quot;/var/lib/etcd/member/wal&quot;, &quot;data-dir&quot;: &quot;/var/lib/etcd&quot;, &quot;snap-dir&quot;: &quot;/var/lib/etcd/member/snap&quot;}
</code></pre>
<p>Check if <code>etcd</code> folder is back from restore. </p>
<pre><code>tree /var/lib/etcd
</code></pre>
<pre><code>/var/lib/etcd
└── member
    ├── snap
    │   ├── 0000000000000001-0000000000000001.snap
    │   └── db
    └── wal
        └── 0000000000000000-0000000000000000.wal
</code></pre>
<h5 id="start-services">Start Services</h5>
<p>Start <code>kubelet</code>. The <code>kube-apiserver</code> and <code>etcd</code> will be started automatically by <code>kubelet</code>.</p>
<pre><code>systemctl start kubelet
</code></pre>
<p>Execute below comamnds to make sure services are all up.</p>
<pre><code>systemctl status kubelet.service
nerdctl -n k8s.io ps -a | grep etcd
nerdctl -n k8s.io ps -a | grep apiserver
</code></pre>
<h4 id="verify_2">Verify</h4>
<p>Check cluster status, if the Pod <code>app-before-backup</code> is there.</p>
<pre><code>kubectl get deploy
</code></pre>
<pre><code>NAME                     READY   UP-TO-DATE   AVAILABLE   AGE
app-before-backup        1/1     1            1           4h39m
</code></pre>
<h3 id="upgrade-kubeadm">Upgrade <code>kubeadm</code></h3>
<h4 id="upgrade-control-plane">Upgrade <code>Control Plane</code></h4>
<h5 id="preparation_1">Preparation</h5>
<p>首先驱逐节点</p>
<p>Evict Control Plane node.</p>
<pre><code>kubectl drain &lt;control_plane_node_name&gt; --ignore-daemonsets 
</code></pre>
<pre><code>kubectl drain cka001 --ignore-daemonsets 
</code></pre>
<pre><code>node/cka001 cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/calico-node-v7xdm, kube-system/kube-proxy-msw2z
node/cka001 drained
</code></pre>
<p>The Control Plane node is now in <code>SchedulingDisabled</code> status.</p>
<pre><code>NAME     STATUS                     ROLES                  AGE   VERSION
cka001   Ready,SchedulingDisabled   control-plane,master   19d   v1.23.8
cka002   Ready                      &lt;none&gt;                 19d   v1.23.8
cka003   Ready                      &lt;none&gt;                 19d   v1.23.8
</code></pre>
<p>Check current available version of <code>kubeadm</code>.</p>
<pre><code>apt policy kubeadm
</code></pre>
<pre><code>kubeadm:
  Installed: 1.23.8-00
  Candidate: 1.24.2-00
  Version table:
     1.24.2-00 500
        500 https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial/main amd64 Packages
     1.24.1-00 500
        500 https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial/main amd64 Packages
     1.24.0-00 500
        500 https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial/main amd64 Packages
 *** 1.23.8-00 500
        500 https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial/main amd64 Packages
        100 /var/lib/dpkg/status
     1.23.7-00 500
        500 https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial/main amd64 Packages
     1.23.6-00 500
        500 https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial/main amd64 Packages
......
</code></pre>
<p>Upgrade <code>kubeadm</code> to <code>Candidate: 1.24.2-00</code> version.</p>
<pre><code>sudo apt-get -y install kubeadm=1.24.2-00 --allow-downgrades
</code></pre>
<p>Check upgrade plan.</p>
<pre><code>kubeadm upgrade plan
</code></pre>
<p>Get below guideline of upgrade.</p>
<pre><code>Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
COMPONENT   CURRENT       TARGET
kubelet     3 x v1.23.8   v1.23.9

Upgrade to the latest version in the v1.23 series:

COMPONENT                 CURRENT   TARGET
kube-apiserver            v1.23.8   v1.23.9
kube-controller-manager   v1.23.8   v1.23.9
kube-scheduler            v1.23.8   v1.23.9
kube-proxy                v1.23.8   v1.23.9
CoreDNS                   v1.8.6    v1.8.6
etcd                      3.5.1-0   3.5.3-0

You can now apply the upgrade by executing the following command:

        kubeadm upgrade apply v1.23.9

_____________________________________________________________________

Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
COMPONENT   CURRENT       TARGET
kubelet     3 x v1.23.8   v1.24.3

Upgrade to the latest stable version:

COMPONENT                 CURRENT   TARGET
kube-apiserver            v1.23.8   v1.24.3
kube-controller-manager   v1.23.8   v1.24.3
kube-scheduler            v1.23.8   v1.24.3
kube-proxy                v1.23.8   v1.24.3
CoreDNS                   v1.8.6    v1.8.6
etcd                      3.5.1-0   3.5.3-0

You can now apply the upgrade by executing the following command:

        kubeadm upgrade apply v1.24.3

Note: Before you can perform this upgrade, you have to update kubeadm to v1.24.3.

_____________________________________________________________________


The table below shows the current state of component configs as understood by this version of kubeadm.
Configs that have a &quot;yes&quot; mark in the &quot;MANUAL UPGRADE REQUIRED&quot; column require manual config upgrade or
resetting to kubeadm defaults before a successful upgrade can be performed. The version to manually
upgrade to is denoted in the &quot;PREFERRED VERSION&quot; column.

API GROUP                 CURRENT VERSION   PREFERRED VERSION   MANUAL UPGRADE REQUIRED
kubeproxy.config.k8s.io   v1alpha1          v1alpha1            no
kubelet.config.k8s.io     v1beta1           v1beta1             no
_____________________________________________________________________

</code></pre>
<h5 id="upgrade">Upgrade</h5>
<p>Refer to upgrade plan, let's upgrade to v1.24.2 version.</p>
<pre><code>kubeadm upgrade apply v1.24.2
</code></pre>
<p>With option <code>--etcd-upgrade=false</code>, the <code>etcd</code> can be excluded from the upgrade.</p>
<pre><code>kubeadm upgrade apply v1.24.2 --etcd-upgrade=false
</code></pre>
<p>It's successful when receiving below message.</p>
<pre><code>[upgrade/successful] SUCCESS! Your cluster was upgraded to &quot;v1.24.2&quot;. Enjoy!

[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.
</code></pre>
<p>Upgrade <code>kubelet</code> and <code>kubectl</code>.</p>
<pre><code>sudo apt-get -y install kubelet=1.24.2-00 kubectl=1.24.2-00 --allow-downgrades
sudo systemctl daemon-reload
sudo systemctl restart kubelet
</code></pre>
<p>Get current node status.</p>
<pre><code>kubectl get node
</code></pre>
<pre><code>NAME     STATUS                     ROLES           AGE   VERSION
cka001   Ready,SchedulingDisabled   control-plane   19d   v1.24.2
cka002   Ready                      &lt;none&gt;          19d   v1.23.8
cka003   Ready                      &lt;none&gt;          19d   v1.23.8
</code></pre>
<p>等待片刻，升级完成确认节点 Ready 后，取消禁止调度
After verify that each node is in Ready status, enable node scheduling.</p>
<pre><code>kubectl uncordon &lt;control_plane_node_name&gt;
</code></pre>
<pre><code>kubectl uncordon cka001
</code></pre>
<p>Output:</p>
<pre><code>node/cka001 uncordoned
</code></pre>
<p>Check node status again. Make sure all nodes are in Ready status.</p>
<pre><code>kubectl get node
</code></pre>
<p>Output:</p>
<pre><code>NAME     STATUS   ROLES           AGE   VERSION
cka001   Ready    control-plane   19d   v1.24.2
cka002   Ready    &lt;none&gt;          19d   v1.23.8
cka003   Ready    &lt;none&gt;          19d   v1.23.8
</code></pre>
<h4 id="upgrade-worker">Upgrade Worker</h4>
<h5 id="preparation_2">Preparation</h5>
<p>Evict Worker nodes, explicitly specify to remove local storage if needed.</p>
<pre><code>kubectl drain &lt;worker_node_name&gt; --ignore-daemonsets --force
kubectl drain &lt;worker_node_name&gt; --ignore-daemonsets --delete-emptydir-data --force
</code></pre>
<pre><code>kubectl drain cka002 --ignore-daemonsets --ignore-daemonsets --delete-emptydir-data --force
</code></pre>
<pre><code>node/cka002 already cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/calico-node-4qm45, kube-system/kube-proxy-9rrpv
evicting pod jh-namespace/mysql-nodeselector-6b7d9c875d-m862d
evicting pod quota-object-example/ns-quota-test-84c6c557b9-hkbcl
evicting pod ingress-nginx/ingress-nginx-controller-556fbd6d6f-h455s
evicting pod jh-namespace/app-before-backup-66dc9d5cb-6sqcp
evicting pod jh-namespace/pod-netpol-2-77478d77ff-96hgd
evicting pod jh-namespace/mysql-with-sc-pvc-7c97d875f8-xp42f
evicting pod kube-system/coredns-6d8c4cb4d-zdmm5
evicting pod kube-system/metrics-server-7fd564dc66-rjchn
evicting pod jh-namespace/nginx-app-1-695b7b647d-z8chz
pod/app-before-backup-66dc9d5cb-6sqcp evicted
pod/ns-quota-test-84c6c557b9-hkbcl evicted
I0714 17:19:55.890912  869782 request.go:601] Waited for 1.159970307s due to client-side throttling, not priority and fairness, request: GET:https://172.16.18.161:6443/api/v1/namespaces/jh-namespace/pods/nginx-app-1-695b7b647d-z8chz
pod/nginx-app-1-695b7b647d-z8chz evicted
pod/metrics-server-7fd564dc66-rjchn evicted
pod/mysql-nodeselector-6b7d9c875d-m862d evicted
pod/mysql-with-sc-pvc-7c97d875f8-xp42f evicted
pod/coredns-6d8c4cb4d-zdmm5 evicted
pod/ingress-nginx-controller-556fbd6d6f-h455s evicted
pod/pod-netpol-2-77478d77ff-96hgd evicted
node/cka002 drained
</code></pre>
<p>Upgrade kubeadm on <strong>Worker node</strong>.</p>
<p>Log on to <code>cka002</code> and download <code>kubeadm</code> with version <code>v1.24.2</code>.</p>
<pre><code>sudo apt-get -y install kubeadm=1.24.2-00 --allow-downgrades
</code></pre>
<h5 id="upgrade_1">Upgrade</h5>
<p>Perform upgrade on <strong>Worker node</strong>.</p>
<p>Log onto <code>cka002</code>.</p>
<pre><code>sudo kubeadm upgrade node
</code></pre>
<p>Upgrade <code>kubelet</code>.</p>
<pre><code>sudo apt-get -y install kubelet=1.24.2-00 --allow-downgrades
sudo systemctl daemon-reload
sudo systemctl restart kubelet
</code></pre>
<p>Check node status. </p>
<pre><code>kubectl get node
</code></pre>
<pre><code>NAME     STATUS                     ROLES           AGE   VERSION
cka001   Ready                      control-plane   19d   v1.24.2
cka002   Ready,SchedulingDisabled   &lt;none&gt;          19d   v1.24.2
cka003   Ready                      &lt;none&gt;          19d   v1.23.8
</code></pre>
<p>Make sure all nodes are in Ready status, then, enable node scheduling.</p>
<pre><code>kubectl uncordon &lt;worker_node_name&gt;
</code></pre>
<pre><code>kubectl uncordon cka002
</code></pre>
<h4 id="verify_3">Verify</h4>
<pre><code>kubectl get node
</code></pre>
<pre><code>NAME     STATUS   ROLES           AGE   VERSION
cka001   Ready    control-plane   19d   v1.24.2
cka002   Ready    &lt;none&gt;          19d   v1.24.2
cka003   Ready    &lt;none&gt;          19d   v1.23.8
</code></pre>
<p>Repeat the same on node <code>cka003</code>.</p>
<p>Log onto <code>cka001</code> to evict node <code>cka003</code>.</p>
<pre><code>kubectl drain cka003 --ignore-daemonsets --ignore-daemonsets --delete-emptydir-data --force
</code></pre>
<p>Log onto <code>cka003</code> and perform below commands.</p>
<pre><code>sudo apt-get -y install kubeadm=1.24.2-00 --allow-downgrades

sudo kubeadm upgrade node

sudo apt-get -y install kubelet=1.24.2-00 --allow-downgrades
sudo systemctl daemon-reload
sudo systemctl restart kubelet

kubectl get node
kubectl uncordon cka003
</code></pre>
<p>Get final status of all nodes.</p>
<pre><code>kubectl get node
</code></pre>
<pre><code>NAME     STATUS   ROLES           AGE   VERSION
cka001   Ready    control-plane   19d   v1.24.2
cka002   Ready    &lt;none&gt;          19d   v1.24.2
cka003   Ready    &lt;none&gt;          19d   v1.24.2
</code></pre>
<h2 id="20helm-chart">20.Helm Chart</h2>
<h3 id="install-helm">Install Helm</h3>
<p>Install Helm on <code>cka001</code>. </p>
<pre><code># https://github.com/helm/helm/releases
wget https://get.helm.sh/helm-v3.8.2-linux-amd64.tar.gz
tar -zxvf helm-v3.8.2-linux-amd64.tar.gz
cp linux-amd64/helm /usr/bin/
rm -rf linux-amd64 helm-v3.8.2-linux-amd64.tar.gz
</code></pre>
<p>Or manually download the file via link <code>https://get.helm.sh/helm-v3.8.2-linux-amd64.tar.gz</code>, and remote copy to <code>cka001</code>.</p>
<pre><code>scp -i cka-key-pair.pem helm-v3.8.2-linux-amd64.tar.gz root@cka001:/root/
</code></pre>
<pre><code>ssh -i cka-key-pair.pem root@cka001
tar -zxvf helm-v3.8.2-linux-amd64.tar.gz
cp linux-amd64/helm /usr/bin/
rm -rf linux-amd64 helm-v3.8.2-linux-amd64.tar.gz
</code></pre>
<h3 id="usage-of-helm">Usage of Helm</h3>
<p>Check <code>helm</code> version</p>
<pre><code>helm version
</code></pre>
<pre><code>version.BuildInfo{Version:&quot;v3.8.2&quot;, GitCommit:&quot;6e3701edea09e5d55a8ca2aae03a68917630e91b&quot;, GitTreeState:&quot;clean&quot;, GoVersion:&quot;go1.17.5&quot;}
</code></pre>
<p>Get help of <code>helm</code>.</p>
<pre><code>helm help
</code></pre>
<p>Configure auto-completion for <code>helm</code>.</p>
<pre><code>echo &quot;source &lt;(helm completion bash)&quot; &gt;&gt; ~/.bashrc
source &lt;(helm completion bash)
</code></pre>
<h4 id="install-mysql-from-helm">Install MySQL from Helm</h4>
<p>Add bitnami Chartes Repository.</p>
<pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami
</code></pre>
<p>Get current Charts repositories.</p>
<pre><code>helm repo list
</code></pre>
<pre><code>NAME    URL
bitnami https://charts.bitnami.com/bitnami
</code></pre>
<p>Sync up local Charts repositories.</p>
<pre><code>helm repo update
</code></pre>
<p>Search bitnami Charts in repositories.</p>
<pre><code>helm search repo bitnami
</code></pre>
<p>Search bitnami/mysql Charts in repositories.</p>
<pre><code>helm search repo bitnami/mysql
</code></pre>
<p>Install MySQL Chart on namespace <code>jh-namespace</code>：</p>
<pre><code>helm install mysql bitnami/mysql -n jh-namespace
</code></pre>
<pre><code>NAME: mysql
LAST DEPLOYED: Thu Jul 14 18:18:16 2022
NAMESPACE: jh-namespace
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: mysql
CHART VERSION: 9.2.0
APP VERSION: 8.0.29

** Please be patient while the chart is being deployed **

Tip:

  Watch the deployment status using the command: kubectl get pods -w --namespace jh-namespace

Services:

  echo Primary: mysql.jh-namespace.svc.cluster.local:3306

Execute the following to get the administrator credentials:

  echo Username: root
  MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace jh-namespace mysql -o jsonpath=&quot;{.data.mysql-root-password}&quot; | base64 -d)

To connect to your database:

  1. Run a pod that you can use as a client:

      kubectl run mysql-client --rm --tty -i --restart='Never' --image  docker.io/bitnami/mysql:8.0.29-debian-11-r9 --namespace jh-namespace --env MYSQL_ROOT_PASSWORD=$MYSQL_ROOT_PASSWORD --command -- bash

  2. To connect to primary service (read/write):

      mysql -h mysql.jh-namespace.svc.cluster.local -uroot -p&quot;$MYSQL_ROOT_PASSWORD&quot;
</code></pre>
<p>Check installed release：</p>
<pre><code>helm list
</code></pre>
<pre><code>NAME    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION
mysql   jh-namespace    1               2022-07-14 18:18:16.252140606 +0800 CST deployed        mysql-9.2.0     8.0.29
</code></pre>
<p>Check installed mysql release information.</p>
<pre><code>helm status mysql
</code></pre>
<p>Check mysql Pod status.</p>
<pre><code>kubectl get pod
</code></pre>
<pre><code>NAME                                      READY   STATUS    RESTARTS   AGE
mysql-0                                   1/1     Running   0          76s
</code></pre>
<p><strong>思考题</strong></p>
<ol>
<li>查看 MySQL Pod 运行情况，目前 Pod 状态是什么？</li>
<li>为什么当前处于 Pending 状态？</li>
<li>如何排查？( <code>kubectl describe pod</code> )</li>
<li>为什么无法绑定 PVC？</li>
<li>PVC的状态是什么？( <code>kubectl get pvc</code>, <code>kubectl describe pvc data-mysql-0</code> )</li>
<li>为什么在 <code>11 存储</code>实验中部署的 StorageClass 无法满足要求？( <code>accessMode</code> )</li>
<li>如何手动创建满足要求的 PV？</li>
</ol>
<h3 id="develop-a-chart">Develop a Chart</h3>
<p>Below is a demo on how to develop a Chart.</p>
<ol>
<li>Execute <code>helm create</code> to initiate a Chart：</li>
</ol>
<pre><code># Naming conventions of Chart: lowercase a~z and -(minus sign)
helm create cka-demo
</code></pre>
<p>A folder <code>cka-demo</code> was created. Check the folder structure.</p>
<pre><code>cd cka-demo/
tree
</code></pre>
<pre><code>├── charts
├── Chart.yaml
├── templates
│   ├── deployment.yaml
│   ├── _helpers.tpl
│   ├── hpa.yaml
│   ├── ingress.yaml
│   ├── NOTES.txt
│   ├── serviceaccount.yaml
│   ├── service.yaml
│   └── tests
│       └── test-connection.yaml
└── values.yaml
</code></pre>
<p>Delete or empty some files, which will be re-created later.</p>
<pre><code>rm -rf charts
rm -rf templates/tests 
rm -rf templates/*.yaml
echo &quot;&quot; &gt; values.yaml
echo &quot;&quot; &gt; templates/NOTES.txt
echo &quot;&quot; &gt; templates/_helpers.tpl
</code></pre>
<p>Now new structure looks like below.</p>
<pre><code>├── Chart.yaml
├── templates
│   ├── _helpers.tpl
│   └── NOTES.txt
└── values.yaml
</code></pre>
<h4 id="notestxt">NOTES.txt</h4>
<p>NOTES.txt is used to provide summary information to Chart users. 
In the demo, we will use NOTES.txt to privide summary info about whether the user passed CKA certificate or not.</p>
<pre><code>cd cka-demo/
vi templates/NOTES.txt
</code></pre>
<p>Add below info.</p>
<pre><code>{{- if .Values.passExam }}
Congratulations!

You have successfully completed Certified Kubernetes Administrator China Exam (CKA-CN). 

Your CKA score is: {{ .Values.ckaScore }}

Click the link below to view and download your certificate.

https://trainingportal.linuxfoundation.org/learn/dashboard
{{- else }}
Come on! you can do it next time!
{{- end }}
</code></pre>
<h4 id="deployment-template">Deployment Template</h4>
<p>Let's use Busybox service to generate information. 
We use <code>kubectl create deployment --dry-run=client -oyaml</code> to generate Deployment yaml file and write it the yaml file content into file <code>templates/deployment.yaml</code>.</p>
<pre><code>kubectl create deployment cka-demo-busybox --image=busybox:latest --dry-run=client -oyaml &gt; templates/deployment.yaml
</code></pre>
<p>Check content of deployment yaml file <code>templates/deployment.yaml</code>.</p>
<pre><code>cat templates/deployment.yaml
</code></pre>
<pre><code>apiVersion: apps/v1      
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: cka-demo-busybox
  name: cka-demo-busybox 
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cka-demo-busybox
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: cka-demo-busybox
    spec:
      containers:
      - image: busybox:latest
        name: busybox
        resources: {}
status: {}
</code></pre>
<p>Let's replace value of <code>.spec.replicas</code> from <code>1</code> to a variable <code>{{ .Values.replicaCount }}</code>, so we can dynamicly assign replicas number for other Deployment.</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: cka-demo-busybox
  name: cka-demo-busybox
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app: cka-demo-busybox
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: cka-demo-busybox
    spec:
      containers:
      - image: busybox:latest
        name: busybox
        resources: {}
status: {}
</code></pre>
<p>The <code>.spec.replicas</code> will be replaced by actula value of <code>.Values.replicaCount</code> during deployment. </p>
<p>Let's create another file <code>values.yaml</code> and add a variable <code>replicaCount</code> with default value 1 into the file.
Strong recommended to add comments for each value we defined in file <code>values.yaml</code>.</p>
<pre><code>vi values.yaml
</code></pre>
<pre><code># Number of deployment replicas
replicaCount: 1
</code></pre>
<p>Let's add more variables into file <code>templates/deployment.yaml</code>.</p>
<ul>
<li>Replace Release name <code>.metadata.name</code> by <code>{{ .Release.Name }}</code> and filled with variable defined in file <code>values.yaml</code>.</li>
<li>Replace label name <code>.metadata.labels</code> by <code>{{- include "cka-demo.labels" . | nindent 4 }}</code>, and filled with labels name <code>cka-demo.labels</code> defined in file <code>_helpers.tpl</code>.</li>
<li>Replace <code>.spec.replicas</code> by <code>{{ .Values.replicaCount }}</code> and filled with variable defined in file <code>values.yaml</code>.</li>
<li>Replace <code>.spec.selector.matchLabels</code> by <code>{{- include "cka-demo.selectorLabels" . | nindent 6 }}</code> and filled with <code>cka-demo.selectorLabels</code> defined in file <code>_helpers.tpl</code>.</li>
<li>Replace <code>.spec.template.metadata.labels</code> by <code>{{- include "cka-demo.selectorLabels" . | nindent 8 }}</code> and filled with <code>cka-demo.selectorLabels</code> defined in file <code>_helpers.tpl</code>.</li>
<li>Replace <code>.spec.template.spec.containers[0].image</code> by <code>{{ .Values.image.repository }}</code> and <code>{{ .Values.image.tag }}</code> and filled with variables defined in <code>values.yaml</code> for image name and image tag.</li>
<li>Replace <code>.spec.template.spec.containers[0].command</code> and add <code>if-else</code> statement, if <code>.Values.passExam</code> is true, execute commands defined in <code>.Values.passCommand</code>, if false, execute commands defined in <code>.Values.lostCommand</code>.</li>
<li>Use <code>key</code> from <code>ConfigMap</code> from <code>.spec.template.spec.containers[0].env</code> as prefix of ConfigMap name and filled with <code>{{ .Values.studentName }}</code> defined in file <code>values.yaml</code>.</li>
<li>Replace <code>.spec.template.spec.containers[0].resources</code> by <code>{{ .Values.resources }}</code> and filled with variable defined in file <code>values.yaml</code>.</li>
</ul>
<p>The <code>.Release.Name</code> is built-in object, no need to be specified in file <code>values.yaml</code>. It's generated by Release by <code>helm install</code>.</p>
<p>Remove unused lines and final one looks like below.</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}
  labels:
    {{- include &quot;cka-demo.labels&quot; . | nindent 4 }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      {{- include &quot;cka-demo.selectorLabels&quot; . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include &quot;cka-demo.selectorLabels&quot; . | nindent 8 }}
    spec:
      containers:
      - name: id-generator
        image: &quot;{{ .Values.image.repository }}:{{ .Values.image.tag }}&quot;
        {{- if .Values.passExam }}
        {{- with .Values.passCommand }}
        command: {{ range . }}
          - {{ . | quote }}
          {{- end }}
          {{- end }}
        {{- else }}
        {{- with .Values.lostCommand }}
        command: {{ range . }}
          - {{ . | quote }}
          {{- end }}
          {{- end }}
        {{- end }}
        env:
        - name: CKA_SCORE
          valueFrom:
            configMapKeyRef:
              name: {{ .Values.studentName }}-cka-score
              key: cka_score
        {{- with .Values.resources }}
        resources:
            {{- toYaml . | nindent 12 }}
          {{- end}}
      restartPolicy: Always
</code></pre>
<p>Update file <code>values.yaml</code> with variables default values.
Suggestions：add variables one and test one, don't add all at one time.</p>
<pre><code>vi values.yaml
</code></pre>
<pre><code># Number of deployment replicas 
replicaCount: 1

# Image repository and tag
image:
  repository: busybox
  tag: latest

# Container start command
passCommand:
  - '/bin/sh'
  - '-c'
  - &quot;echo Your CKA score is $(CKA_SCORE) and your CKA certificate ID number is $(tr -dc 'A-Za-z0-9' &lt; /dev/urandom | head -c 13; echo) ; sleep 86400&quot;
lostCommand:
  - '/bin/sh'
  - '-c'
  - &quot;echo Your CKA score is $(CKA_SCORE), Come on! you can do it next time! ; sleep 86400&quot;

# Container resources
resources:
  limits:
    cpu: 200m
    memory: 256Mi
  requests:
    cpu: 100m
    memory: 128Mi

# Student Name
studentName: whoareyou

# Student pass CKA exam or not
passExam: true
</code></pre>
<h4 id="configmap-template">ConfigMap Template</h4>
<p>ConfigMap is referred in the Deployment, hence we need define the ConfigMap template.
We will combine name of ConfigMap and <code>cka_score</code> as a variable, like <code>name-cka-score</code>.</p>
<pre><code>vi templates/configmap.yaml
</code></pre>
<pre><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Values.studentName }}-cka-score
  labels:
    {{- include &quot;cka-demo.labels&quot; . | nindent 4 }}
data:
  cka_score: {{ .Values.ckaScore | quote }}
</code></pre>
<p>The <code>studentName</code> was already defined in file <code>values.yaml</code>, we just need add <code>ckaScore</code> with default value.</p>
<pre><code>vi values.yaml
</code></pre>
<pre><code># Student CKA Score
ckaScore: 100
</code></pre>
<h4 id="_helperstpl">_helpers.tpl</h4>
<p>Define a common template <code>_helpers.tpl</code> to add labels and labels of Selector for labels of Deployment and ConfigMap.</p>
<pre><code>vi templates/_helpers.tpl
</code></pre>
<pre><code>{{/*
Common labels
*/}}
{{- define &quot;cka-demo.labels&quot; -}}
{{ include &quot;cka-demo.selectorLabels&quot; . }}
{{- if .Chart.AppVersion }}
app.kubernetes.io/version: {{ .Chart.AppVersion | quote }}
{{- end }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
{{- end -}}


{{/*
Selector labels
*/}}
{{- define &quot;cka-demo.selectorLabels&quot; -}}
app: {{ .Chart.Name }}
release: {{ .Release.Name }}
{{- end -}}
</code></pre>
<h4 id="chartyaml">Chart.yaml</h4>
<p>We use CKA logo as the icon of Chart</p>
<pre><code>wget https://www.cncf.io/wp-content/uploads/2021/09/kubernetes-cka-color.svg
</code></pre>
<p>Edit Chart.yaml file.</p>
<pre><code>vi Chart.yaml
</code></pre>
<p>Append icon info in the file.</p>
<pre><code>icon: file://./kubernetes-cka-color.svg
</code></pre>
<p>Add author info for the Chart</p>
<pre><code>vi Chart.yaml
</code></pre>
<pre><code>maintainers:
  - name: Yinlin.Li
</code></pre>
<p>Final <code>Chart.yaml</code> looks like below.</p>
<pre><code>apiVersion: v2
name: cka-demo
description: A Helm chart for CKA demo.
type: application
version: 0.1.0
appVersion: &quot;v1.24&quot;
maintainers:
  - name: James.H
icon: file://./kubernetes-cka-color.svg
</code></pre>
<h4 id="chart-debug">Chart Debug</h4>
<p>Use <code>helm lint</code> to verify above change.</p>
<pre><code>helm lint
</code></pre>
<pre><code>1 chart(s) linted, 0 chart(s) failed
</code></pre>
<p><code>helm lint</code> only check format of Chart, won't check Manifest file.</p>
<p>We can use <code>helm install --debug --dry-run</code> or <code>helm template</code> to check Manifest output in order to verify all yaml files are correct or not.</p>
<pre><code>helm template cka-demo ./
</code></pre>
<p>Use <code>helm install --debug --dry-run</code> to simulate the installation. We can get expected results from two different options (passed or failed the CKA certificate).</p>
<pre><code>helm install --debug --dry-run cka-demo ./ --create-namespace \
  -n cka \
  --set studentName=kubernetes \
  --set ckaScore=99 \
  --set passExam=true

helm install --debug --dry-run cka-demo ./ --create-namespace \
  -n cka \
  --set studentName=kubernetes \
  --set ckaScore=0 \
  --set passExam=false
</code></pre>
<p>Package Chart to .tgz file, and upload to repository, e.g., Chart Museum or OCI Repo.</p>
<pre><code>cd ../
helm package cka-demo
</code></pre>
<pre><code>Successfully packaged chart and saved it to: /root/cka-demo-0.1.0.tgz
</code></pre>
<p>Till now, we have done our task to develop a Chart. Let's install the Chart.</p>
<pre><code>helm install cka-demo cka-demo-0.1.0.tgz --create-namespace \
  -n cka \
  --set studentName=kubernetes \
  --set ckaScore=0 \
  --set passExam=false
</code></pre>
<pre><code>NAME: cka-demo
LAST DEPLOYED: Fri Jul 15 20:54:52 2022
NAMESPACE: cka
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Come on! you can do it next time!
</code></pre>
<p>If any error, need to unstall <code>cka-demo</code> and reinstall it.</p>
<pre><code>helm list --all-namespace
helm uninstall cka-demo -n &lt;your_namespace&gt;
</code></pre>
<p>Check container log of <code>busybox</code>.</p>
<pre><code>kubectl logs -n cka -l app=cka-demo
</code></pre>
<pre><code>Your CKA score is 0, Come on! you can do it next time!
</code></pre>
<p>Install <code>cka-demo</code> with different options.</p>
<pre><code>helm uninstall cka-demo -n cka

helm install cka-demo cka-demo-0.1.0.tgz --create-namespace \
  -n cka \
  --set studentName=kubernetes \
  --set ckaScore=100 \
  --set passExam=true
</code></pre>
<pre><code>NAME: cka-demo
LAST DEPLOYED: Fri Jul 15 20:58:01 2022
NAMESPACE: cka
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Congratulations!

You have successfully completed Certified Kubernetes Administrator China Exam (CKA-CN).

Your CKA score is: 100

Click the link below to view and download your certificate.

https://trainingportal.linuxfoundation.org/learn/dashboard
</code></pre>
<p>Check container log of <code>busybox</code>.</p>
<pre><code>kubectl logs -n cka -l app=cka-demo
</code></pre>
<pre><code>Your CKA score is 100 and your CKA certificate ID number is fwauD8TJW1OzD
</code></pre>
<p><strong>Built-in Objects</strong></p>
<pre><code>Release.Name                 # 发布名称
Release.Namespace            # 发布Namespace
Release.Service              # 渲染模板的服务，在Helm中默认值为&quot;Helm&quot;
Release.IsUpgrade            # 如果当前是升级或回滚，设置为true
Release.IsInstall            # 如果当前是安装，设置为true
Release.Revision             # 发布版本号
Values                       # 从values.yaml和--set传入，默认为空
Chart                        # 所有Chart.yaml中的内容
Chart.Version                # 例如
Chart.Maintainers            # 例如
Files                        # 在chart中访问非特殊文件
Capabilities                 # 提供关于支持能力的信息（K8s API版本、K8s版本、Helm版本）
Capabilities.KubeVersion     # Kubernetes的版本号
Capabilities.APIVersions.Has &quot;batch/v1&quot; # K8s API版本包含&quot;batch/v1&quot;
Template                     # 当前模板信息
Template.Name                # 当前模板文件路径
Template.BasePath            # 当前模板目录路径
</code></pre>
<h3 id="reference">Reference</h3>
<p><a href="https://helm.sh/">Helm 官网</a></p>
<p><a href="https://helm.sh/zh/docs/topics/version_skew/">Helm 版本支持策略</a></p>
<p><a href="https://github.com/helm/helm/blob/484d43913f97292648c867b56768775a55e4bba6/pkg/releaseutil/kind_sorter.go">Helm Chart 资源对象安装顺序</a></p>
<h2 id="a1discussion">A1.Discussion</h2>
<p><strong>6/26</strong></p>
<ol>
<li>将cka003节点的kubelet服务关闭</li>
<li>在节点上发生了什么，通过nerdctl查看容器发生了什么</li>
<li>在集群层面观察对应节点处于什么状态，本来在节点上运行的Pod发生了什么（kubectl get pod -owide -A -w持续监视Pod变化）</li>
</ol>
<p><strong>6/28</strong></p>
<ol>
<li>用alias给kubectl设置一个别名k，以后操作kubectl就可以只打k啦，比如k get node</li>
</ol>
<p><strong>6/30</strong></p>
<ol>
<li>创建一个具有两个容器的Pod（镜像可以随意选择）</li>
<li>DaemonSet可以设置replicas参数吗？为什么？</li>
<li>kubectl查看Pod日志时如何按关键字过滤</li>
</ol>
<p>https://howtoforge.com/multi-container-pods-in-kubernetes/</p>
<p><strong>7/3</strong></p>
<ol>
<li>
<p>如何基于健康检查实操中的nginx-healthcheck模拟livenessProbe存活探针检查失败的场景？</p>
<ul>
<li>提示1：nginx-healthcheck的livenessProbe探测的是80端口的存活</li>
<li>提示2：容器中可以执行sed</li>
<li>提示3：nginx-healthcheck的默认配置文件位于/etc/nginx/conf.d/下</li>
<li>提示4：Nginx的重新加载配置的命令是nginx -s reload</li>
</ul>
</li>
<li>
<p>HPA计算CPU/内存扩缩容的百分比是如何计算出来的？分子和分母分别是取什么值</p>
</li>
</ol>
<p><strong>7/5</strong></p>
<ol>
<li>通过kubectl create deploy nginx --image=nginx命令创建的Deployment，忘记加容器端口了，如何修改Deployment加上端口</li>
<li>验证Service的internalTrafficPolicy参数</li>
</ol>
<p><strong>7/7</strong></p>
<p>提示，用官网的YAML示例修改：</p>
<ol>
<li>创建一个hostPath类型的PV，目录自定义</li>
<li>按照这个PV，创建一个PVC跟这个PV绑定</li>
<li>创建一个Pod，挂载这个PVC，挂载目录自定义</li>
<li>修改这个Pod，添加一个emptyDir类型的Volume挂载，挂载目录自定义</li>
</ol>
<p><strong>7/10</strong></p>
<ol>
<li>kubectl top命令查看Pod和Node的资源利用率如何按照利用率排序？</li>
</ol>
<p><strong>7/12</strong></p>
<ol>
<li>kubectl命令行方式创建ClusterRole，定义对Deployment的create权限</li>
<li>kubectl命令行方式创建一个Namespace</li>
<li>kubectl命令行方式在Namespace下创建一个ServiceAccount</li>
<li>kubectl命令行方式创建RoleBinding把上面创建的ClusterRole和ServiceAccount绑定起来</li>
</ol>
                
              
              
                


              
            </article>
          </div>
        </div>
        
      </main>
      
        
<footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        
          Made with
          <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
            Material for MkDocs
          </a>
        
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.fcfe8b6d.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.b1047164.min.js"></script>
      
    
  </body>
</html>